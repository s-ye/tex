\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{bookmark}
\usepackage{tikz}
\usepackage{/Users/songye03/Desktop/math_tex/style/quiver}
\usepackage{/Users/songye03/Desktop/math_tex/style/scribe}
\usepackage{fancyhdr}

\usepackage{hyperref}


\usepackage{parskip} % Automatically respects blank lines
\setlength{\parskip}{1em} % Adds more space between paragraphs
\setlength{\parindent}{0pt} % Removes paragraph indentation

\begin{document}


\lhead{Songyu Ye}
\rhead{\today}
\cfoot{\thepage}

\title{Infinite Dimensional Lie Algebras}

\author{Songyu Ye}
\date{\today}
\maketitle


\begin{abstract}
    These are notes for a reading course supervised by Prof. Richard Borcherds in the Fall of 2025 at UC Berkeley. The main references are \cite{kac}. There is an accompanying set of notes on loop groups, following \cite{pressley-segal}.
\end{abstract}
\tableofcontents
Bump \cite{bump} writes that the crown jewel of the theory of semisimple Lie algebras is the Weyl character formula, which gives a closed-form expression for the characters of all irreducible finite-dimensional representations. The proof of this formula uses the Casimir operator, which is constructed using an invariant bilinear form on the Lie algebra. We will generalize these concepts to infinite-dimensional Lie algebras, specifically Kac-Moody algebras.
\section{Semisimple Lie algebras}
We recall the basic structure theory and representation theory of semisimple Lie algebras, culminating in a discussion of the Weyl character formula. Let $\mf g$ be a finite dimensional semisimple Lie algebra over the complex numbers $\mathbb{C}$. We follow \cite{bump}.
\subsection{Preliminaries}
\begin{definition}
    Recall that a Lie algebra is solvable if its derived series eventually becomes zero. The derived series is defined by $L^{(0)} = L$, $L^{(1)} = [L,L]$, and $L^{(n+1)} = [L^{(n)}, L^{(n)}]$.
\end{definition}

\begin{definition}
    A Lie algebra $L$ is \textbf{semisimple} if and only if its radical (the largest solvable ideal) is zero.
\end{definition}

\begin{definition}
    The Killing form of a Lie algebra $L$ is the bilinear form $\kappa(x,y) = \text{tr}(\text{ad}(x)\text{ad}(y))$.
\end{definition}
Recall that there is an big Adjoint representation $\Ad: G \to \GL(\mathfrak{g})$ of a Lie group $G$ on its Lie algebra $\mathfrak{g}$, defined by $\Ad_g(X) = gXg^{-1}$. The differential of this at the identity is the little adjoint representation of the Lie algebra $\mathfrak{g}$ on itself, defined by
\begin{align*}
    \operatorname{ad}(X)Y \;=\; \left.\frac{d}{dt}\right|_{t=0}\!\Ad_{\exp(tX)}(Y).
\end{align*}
In coordinates, $\operatorname{ad}(X) = [X,Y]$.

Now the key idea is to consider the adjoint representation of $\mf h$ on $\mf g$. Since $\mathfrak{h}$ consists of simultaneously diagonalizable endomorphisms, $\operatorname{ad}(H)$ is diagonalizable. Thus we can decompose:
\[
    \mathfrak{g} = \mathfrak{h} \;\oplus\; \bigoplus_{\alpha \in \Phi} \mathfrak{g}_\alpha,
\]
where
\[
    \mathfrak{g}_\alpha = \{ X \in \mathfrak{g} : [H,X] = \alpha(H) X \;\;\forall H\in \mathfrak{h}\}.
\]
The nonzero functionals $\alpha \in \mathfrak{h}^*$ are the roots, and $\Phi$ is the root system. The following proposition is the beginning of the story of the geometry of root systems. The nondegeneracy of the Killing form on the Cartan subalgebra allows us to identify $\mathfrak{h}$ with its dual $\mathfrak{h}^*$. In particular, we get an inner product, lengths, angles, and reflections. However, note that the notion of simple, positive, and integral roots does not come from the Killing form.

\begin{proposition}[Humphreys 8.2]
    The Killing form $\kappa$ is nondegenerate on $\mathfrak{h}$.
\end{proposition}

\begin{proof}

    Recall that a Lie algebra is semisimple if and only if its Killing form is nondegenerate. One shows that the restriction of the Killing form to the centralizer $L_0 = C_L(\mathfrak{h})$ of $\mathfrak{h}$ in $L$ is nondegenerate. Then one shows that in fact $L_0 = \mathfrak{h}$.
\end{proof}

Let $\mathfrak{g}$ be a semisimple Lie algebra, $\mathfrak{h}$ a maximal toral subalgebra, $\Phi \subset \mathfrak{h}^*$ the root system, which we partition into positive and negative roots.

Thus we will associate to $\phi \in \mathfrak{h}^*$ an element $t_\phi \in \mathfrak{h}$ such that
\begin{align*}
    \kappa(t_\phi, h) = \phi(h) \quad \text{for } h \in \mathfrak{h}.
\end{align*}

Then we will define an inner product on $\mathfrak{h}^*$ which we will denote
\begin{align*}
    (\lambda | \mu) = \kappa(t_\lambda, t_\mu).
\end{align*}

If $\alpha \in \Phi \cup \{0\}$ we will denote
\[
    \mathfrak{g}_\alpha = \{x \in \mathfrak{g} \,|\, [h,x] = \alpha(h)x \text{ for } h \in \mathfrak{h} \}.
\]

If $\alpha = 0$, then $\mathfrak{g}_\alpha = \mathfrak{h}$. On the other hand, if $\alpha \in \Phi$, then $\mathfrak{g}_\alpha$ is one-dimensional. We have the decomposition
\begin{align*}
    \mathfrak{g} = \mathfrak{h} \oplus \bigoplus_{\alpha \in \Phi} \mathfrak{g}_\alpha.
\end{align*}

By Humphreys Proposition 8.3(c) we have, for $x \in \mathfrak{g}_\alpha$ and $y \in \mathfrak{g}_{-\alpha}$,
\begin{align*}
    [x,y] = \kappa(x,y)t_\alpha.
\end{align*}

We will denote by
\begin{align*}
    h_\alpha = \frac{2t_\alpha}{(\alpha|\alpha)}.
\end{align*}

Thus if $x_\alpha \in \mathfrak{g}_\alpha$ we have $[h_\alpha, x_\alpha] = 2x_\alpha$. If
\[
    \alpha^\vee = \frac{2\alpha}{(\alpha|\alpha)}
\]
then for $\lambda \in \mathfrak{h}^*$ we have $(\alpha^\vee | \lambda) = \lambda(h_\alpha)$. Either $\alpha^\vee$ or $h_\alpha$ is called a \textbf{coroot}. They are really the same thing if we identify $\mathfrak{h}$ with its double dual $\mathfrak{h}^{**}$. The factor $\tfrac{2}{(\alpha|\alpha)}$ is chosen so that the root vectors $x_\alpha, y_\alpha$ together with $h_\alpha$ form a standard $\mathfrak{sl}_2$-triple, with the eigenvalue of $x_\alpha$ under $h_\alpha$ equal to $2$.

We will denote by $\rho$ half the sum of the positive roots. Humphreys denotes this $\delta$, but the notation $\rho$ is now universally used by everyone. If $\alpha \in \Phi$ we will denote by $r_\alpha$ the reflection
\[
    r_\alpha(x) = x - (x|\alpha^\vee)\alpha.
\]

\begin{remark}
    Recall that for each root $\alpha$, we have an $\mathfrak{sl}_2$-triple $\{x_\alpha, y_\alpha, h_\alpha\}$. When you restrict the adjoint representation of $\mathfrak{g}$ to this $\mathfrak{sl}_2$, every other root space $\mathfrak{g}_\beta$ becomes a finite-dimensional $\mathfrak{sl}_2$-representation.

    For a root $\alpha \in \Phi$, we can find $x_\alpha \in \mathfrak{g}_\alpha$, $y_\alpha \in \mathfrak{g}_{-\alpha}$, and $h_\alpha = [x_\alpha, y_\alpha] \in \mathfrak{h}$ satisfying the relations of $\mathfrak{sl}_2$:
    \begin{align*}
        [h_\alpha, x_\alpha] & = 2x_\alpha,  \\
        [h_\alpha, y_\alpha] & = -2y_\alpha, \\
        [x_\alpha, y_\alpha] & = h_\alpha.
    \end{align*}
    So $\mathfrak{s}_\alpha = \langle x_\alpha, y_\alpha, h_\alpha\rangle \cong \mathfrak{sl}_2$ is a subalgebra of $\mathfrak{g}$. The adjoint representation is $\operatorname{ad}: \mathfrak{g} \to \mathfrak{gl}(\mathfrak{g})$ with $\operatorname{ad}(z)(w) = [z, w]$. If we restrict $\operatorname{ad}$ to the subalgebra $\mathfrak{s}_\alpha$, then $\mathfrak{g}$ becomes an $\mathfrak{sl}_2$-module.

    Take another root $\beta \in \Phi$, $\beta \neq \pm\alpha$. For any $H \in \mathfrak{h}$, $[H, \mathfrak{g}_\beta] \subseteq \mathfrak{g}_\beta$, so $\mathfrak{g}_\beta$ is invariant under $\mathfrak{h}$. In particular, under $h_\alpha$, vectors in $\mathfrak{g}_\beta$ have weight $[h_\alpha, x_\beta] = \beta(h_\alpha)\,x_\beta$ for $x_\beta \in \mathfrak{g}_\beta$.

    $x_\alpha$ and $y_\alpha$ act as “raising” and “lowering” operators:
    \begin{align*}
        [x_\alpha, \mathfrak{g}_\beta] & \subseteq \mathfrak{g}_{\beta+\alpha}, \\
        [y_\alpha, \mathfrak{g}_\beta] & \subseteq \mathfrak{g}_{\beta-\alpha}.
    \end{align*}
    So if you start with a vector in $\mathfrak{g}_\beta$, repeated commutators with $x_\alpha$ and $y_\alpha$ move you up and down the $\alpha$-string through $\beta$: $\beta, \;\beta+\alpha, \;\beta+2\alpha, \;\dots,\;\beta-q\alpha$.

    Now suppose $\mf g$ is finite dimensional. Then there are only finitely many roots, so this process stops in both directions. Thus the span $\bigoplus_{k} \mathfrak{g}_{\beta+k\alpha}$ is a finite-dimensional representation of $\mathfrak{sl}_2$. It decomposes into irreducible $\mathfrak{sl}_2$-modules, with weights given by integers \[\beta(h_\alpha),\;\beta(h_\alpha)-2,\;\dots,\;\beta(h_\alpha)-2m\]

    From this structure one proves that if $\beta$ is a root, then so is $\beta - \langle \beta,\alpha^\vee \rangle \alpha$, which is exactly the reflection of $\beta$ across the hyperplane orthogonal to $\alpha$. The map
    \[
        r_\alpha(x) = x - (x|\alpha^\vee)\alpha
    \]
    is literally a reflection in the Euclidean space $V = \mathbb{R}\Phi \subset \mathfrak{h}^*$.
    One checks that $r_\alpha(\Phi) = \Phi$, i.e. it permutes the set of roots.

    In addition, since all the finite dimensional irreducible representations of $\mathfrak{sl}_2$ are classified and have $1$-dimensional weight spaces, it follows that $\dim(\mathfrak{g}_\alpha) = 1$ for all roots $\alpha$.

    Beware that this argument does not work if $\mathfrak{g}$ is infinite-dimensional. In particular, Kac-Moody algebras have imaginary roots which do not behave like real roots. The reflections $r_\alpha$ for real roots $\alpha$ still exist, but they do not generate a group which permutes all the roots.

\end{remark}

If $\alpha$ is a simple root, we will also use the notation $s_\alpha$ for $r_\alpha$. We have proved that $s_\alpha$ maps $\alpha$ to its negative and permutes the remaining positive roots. Therefore $s_\alpha(\rho) = \rho - \alpha$ and so
\begin{align*}
    (\rho|\alpha^\vee) = 1
\end{align*}
for all simple roots $\alpha$.

An element $\lambda$ of $\mathfrak{h}^*$ is called an \textbf{integral weight} if $(\lambda|\alpha^\vee) \in \mathbb{Z}$ for all $\alpha \in \Phi^+$, or equivalently, for all simple roots $\alpha$. The integral weights form a lattice
\[
    \Lambda = \{x \in V | (\alpha^\vee|x) \in \mathbb{Z} \text{ for } \alpha \in \Phi^+ \},
\]
called the \textbf{weight lattice}. We call $\lambda \in \mathfrak{h}^*$ \textbf{dominant} if $(\lambda|\alpha^\vee) \geq 0$ for all $\alpha \in \Phi^+$ (or equivalently for simple roots $\alpha$). We call $\lambda$ \textbf{strongly dominant} if $(\lambda|\alpha^\vee) > 0$. Thus the special vector $\rho$ is a strongly dominant integral weight.

Here are a couple of important properties of the Weyl group action. Let $V$ be the $\mathbb{R}$-span of $\Phi$ in $\mathfrak{h}^*$. The inner product $(\,|\,)$ makes $V$ into a Euclidean space, and $\mathfrak{h}^* = V + iV$. The set
\[
    C^+ = \{ x \in V | (\alpha^\vee|x) > 0 \text{ for } \alpha \in \Phi^+ \}
\]
is called the \textbf{positive Weyl chamber}. The dominant weights are the ones in $C^+$.

\begin{proposition}[Fundamental domain]\label{prop:fundamental_domain}
    The positive Weyl chamber is a fundamental domain for the action of the Weyl group:
    if $x \in V$ there is a unique element of $C^+$ in the $W$ orbit of $x$.
\end{proposition}

Recall that there is a partial order (known as dominance order) $\succeq$ on $\mathfrak{h}^*$ defined by
\[
    \lambda \succeq \mu \iff \lambda - \mu = \sum_{\alpha \in \Phi^+} n_\alpha \alpha \text{ with } n_\alpha \in \mathbb{Z}_{\geq 0}.
\]
\begin{proposition}
    Let $\lambda$ be a dominant, integral weight and let $w \in W$. Then $\lambda \succeq w\lambda$.
\end{proposition}

\subsection{Highest weight modules}
Let $V$ be a $\mathfrak{g}$-module. For $\lambda \in \mathfrak{h}^*$ we denote the \textbf{weight space}
\[
    V_\lambda = \{ v \in V \mid h \cdot v = \lambda(h)v \ \text{for } h \in \mathfrak{h} \}.
\]
We will say that $V$ is $\mathfrak{h}$-\textbf{diagonalizable} if $V$ is the algebraic direct sum of the $V_\lambda$.

\begin{proposition}
    If $V$ is $\mathfrak{h}$-diagonalizable, then so is any submodule or quotient module.
\end{proposition}

\begin{proof}
    Let $U \subset V$ be a submodule. We must show that an element of $U$ may be expressed as a finite linear sum of $u_\lambda \in U_\lambda$. Since $V$ has a weight space decomposition, we may write $u$ as a sum of $u_\lambda \in V_\lambda$, and the problem is then to show that $u_\lambda \in U$. There exist a finite number of $\lambda_i$ such that
    \[
        u = \sum_{i=1}^m u_{\lambda_i},
    \]
    and we choose $h \in \mathfrak{h}$ such that the values $\lambda_i(h)$ are all distinct. Then for $j=0,\dots,m-1$
    \[
        h^j \cdot u = \sum \lambda_i(h)^j \, u_{\lambda_i} \in \mathfrak{h}.
    \]
    The $m \times m$ matrix $\{ \lambda_i(h)^j \}$ is invertible since its determinant is a Vandermonde determinant. Consider the vectors $u, h \cdot u, h^2 \cdot u, \dots, h^{m-1}\cdot u.$ Each is in $U$, and together they form a linear system:
    \begin{align*}
        \begin{bmatrix}
            1                  & 1                  & \cdots & 1                  \\
            \lambda_1(h)       & \lambda_2(h)       & \cdots & \lambda_m(h)       \\
            \lambda_1(h)^2     & \lambda_2(h)^2     & \cdots & \lambda_m(h)^2     \\
            \vdots             & \vdots             &        & \vdots             \\
            \lambda_1(h)^{m-1} & \lambda_2(h)^{m-1} & \cdots & \lambda_m(h)^{m-1}
        \end{bmatrix}
        \begin{bmatrix}
            u_{\lambda_1} \\ u_{\lambda_2} \\ \vdots \\ u_{\lambda_m}
        \end{bmatrix} = \begin{bmatrix}
                            u \\ h\cdot u \\ \vdots \\ h^{m-1}\cdot u
                        \end{bmatrix}.
    \end{align*}

    This is a Vandermonde system. The matrix is invertible because the $\lambda_i(h)$ are distinct. Applying the inverse to this shows that each $u_{\lambda_i} \in U$, as required.

    This proves that a submodule of a $\mathfrak{h}$-diagonalizable module is diagonalizable. It follows that the same is true for quotient modules, with $(V/U)_\lambda = V_\lambda / U_\lambda$.
\end{proof}

We will work exclusively with diagonalizable modules with $\dim(V_\lambda) < \infty$ for all $\lambda \in \mathfrak{h}^*$. We will define the \textbf{support} $\mathrm{supp}(V) = \{ \lambda \in \mathfrak{h}^* \mid V_\lambda \neq 0 \}$.

Let $U(\mathfrak{g})$ be the universal enveloping algebra. Let $\mathfrak{n}^+$ be the nilpotent subalgebra of $\mathfrak{g}$ generated by the $\mathfrak{g}_\alpha$ ($\alpha \in \Phi^+$), and let $\mathfrak{n}^-$ be the subalgebra generated by the $\mathfrak{g}_\alpha$ with $\alpha \in \Phi^-$. Then clearly we have the \textbf{triangular decomposition}
\[
    \mathfrak{g} = \mathfrak{n}^- \oplus \mathfrak{h} \oplus \mathfrak{n}^+.
\]

\begin{lemma}
    We have $U(\mathfrak{g}) \cong U(\mathfrak{n}^-) \otimes U(\mathfrak{h}) \otimes U(\mathfrak{n}^+)$ in the sense that the multiplication map
    \[
        U(\mathfrak{n}^-) \times U(\mathfrak{h}) \times U(\mathfrak{n}^+) \longrightarrow U(\mathfrak{g})
    \]
    induces a vector space isomorphism $U(\mathfrak{n}^-) \otimes U(\mathfrak{h}) \otimes U(\mathfrak{n}^+) \longrightarrow U(\mathfrak{g})$.
\end{lemma}

\begin{proof}
    This follows from the Poincaré-Birkhoff-Witt theorem (PBW) together with the triangular decomposition. Namely, if $\{x_i\}$ is a basis for $\mathfrak{g}$, then PBW asserts that a basis for $U(\mathfrak{g})$ consists of all elements of the form
    \[
        x_1^{k_1} \cdots x_d^{k_d}, \qquad 0 \leq k_i \in \mathbb{Z}.
    \]
    Now we take the basis in a particular way, where its first $\tfrac{1}{2}|\Phi|$ elements are a basis for $\mathfrak{n}^-$, the next $\ell$ elements are a basis for $\mathfrak{h}$, and the last $\tfrac{1}{2}|\Phi|$ elements are a basis for $\mathfrak{n}^+$. Then the element $x_1^{k_1}\cdots x_d^{k_d}$ factors uniquely as a product $abc$ where $a$ runs through a basis of $U(\mathfrak{n}^-)$, $b$ runs through a basis of $U(\mathfrak{h})$ and $c$ runs through a basis of $U(\mathfrak{n}^+)$.
\end{proof}

We will call a vector $v \in V$ a \textbf{highest weight vector} of weight $\lambda$ if $v \in V_\lambda$ and if $x_\alpha v = 0$ for $\alpha \in \Phi^+$. (Humphreys calls such $v$ a \textbf{maximal vector}.) We will call $V$ a \textbf{highest weight module} of weight $\lambda$ if it is generated by a highest weight vector $v \in V_\lambda$. (Humphreys calls a highest weight module a \textbf{standard cyclic module}.)

\begin{proposition}
    Suppose that $v \in V$ is a highest weight vector. Then the $\mathfrak{g}$-submodule $U(\mathfrak{g})v$ generated by $v$ equals $U(\mathfrak{n}^-)v$. The weight space $V_\mu = 0$ unless $\mu \preceq \lambda$. We have $\dim(V_\lambda) = 1$.
\end{proposition}

\begin{proof}
    We note that any element of $U(\mathfrak{n}^+)$ may be written as a constant times an element of the left ideal $U(\mathfrak{n}^+)\mathfrak{n}^+$, but this ideal annihilates $v$, so $U(\mathfrak{n}^+)v = \mathbb{C}v$. Similarly $U(\mathfrak{h})v = \mathbb{C}v$ since $v \in V_\lambda$. By Lemma 4,
    \[
        U(\mathfrak{g}) = U(\mathfrak{n}^-)U(\mathfrak{h})U(\mathfrak{n}^+)v = U(\mathfrak{n}^-)v.
    \]

    Consider the basis $\{x_{-\alpha}\}$ ($\alpha \in \Phi^+$) of $\mathfrak{n}^-$ with $x_{-\alpha} \in \mathfrak{g}_{-\alpha}$. Using a fixed order on $\Phi^+$, the elements $\prod_{\alpha \in \Phi^+} x_{-\alpha}^{k_\alpha}$ are a PBW basis of $U(\mathfrak{n}^-)$. Since $x_{-\alpha}$ maps $V_\mu$ to $V_{\mu-\alpha}$,
    \[
        \prod_{\alpha \in \Phi^+} x_{-\alpha}^{k_\alpha} v \in V_\mu,
        \qquad \mu = \lambda - \sum_{\alpha \in \Phi^+} k_\alpha \alpha,
    \]
    so $\mu \preceq \lambda$. Unless all $k_\alpha = 0$, $\mu$ is strictly $\prec \lambda$, so $V_\lambda$ is one-dimensional.
\end{proof}

\begin{remark}
    Beware that highest weight modules need not be irreducible. It is true that if a highest weight module is finite dimensional, then it is irreducible and uniquely determined by its highest weight, which must be a dominant integral weight. But infinite-dimensional highest weight modules need not be irreducible, and even if they are irreducible, they need not be uniquely determined by their highest weight.

    In particular, if you consider $\mf g  = \mf{sl}_2(\mathbb{C})$, then basis vectors of $M(\lambda)$ are $\{ f^k v_\lambda : k \ge 0 \}$. We compute the action of $e$:
    \[
        e f^m v_\lambda = m (\lambda - m + 1) f^{m-1} v_\lambda.
    \]
    At $m = \lambda+1$:
    This becomes
    \[
        e f^{\lambda+1} v_\lambda = 0,
    \]
    but the vector $f^{\lambda+1} v_\lambda$ is nonzero in $M(\lambda)$.
    So $f^{\lambda+1} v_\lambda$ is a new singular vector, generating a proper submodule. $L(\lambda)$ is defined as
    \[
        L(\lambda) = M(\lambda) \big/ \langle f^{\lambda+1} v_\lambda \rangle,
    \]
    i.e.\ we quotient out the submodule generated by that new singular vector. In $L(\lambda)$, the vector $f^{\lambda+1} v_\lambda$ is identically zero, so we get no new singular vectors.
\end{remark}

\begin{remark}[Computing the product $e f^m$ in $\mathfrak{sl}_2$]
    Using the relation $[e,f] = h$, we can induct:
    For $m=1$:
    \[
        e f = f e + h.
    \]
    Suppose for $m$:
    \[
        e f^m = f^m e + m f^{m-1}(h - m + 1).
    \]
    Then for $m+1$:
    \begin{align*}
        e f^{m+1} & = e (f^m f) = (e f^m) f                    \\
                  & = \big(f^m e + m f^{m-1}(h - m + 1)\big) f \\
                  & = f^m e f + m f^{m-1}(h - m + 1) f.
    \end{align*}
    Now expand each part:
    \begin{align*}
        f^m e f              & = f^m (f e + h) = f^{m+1} e + f^m h,               \\
        f^{m-1}(h - m + 1) f & = f^{m-1} (f h - 2f - (m-1)f) = f^m h - (m+1) f^m.
    \end{align*}
    where in the second line we used $[h,f] = -2f$. So altogether:
    \begin{align*}
        e f^{m+1} & = f^{m+1} e + f^m h + m(f^m h - (m+1) f^m) \\
                  & = f^{m+1} e + (m+1) f^m h - m(m+1) f^m.
    \end{align*}
\end{remark}


\begin{proposition}
    Let $V$ be a highest weight module with highest weight $\lambda$. A submodule $U$ of $V$ is proper if and only if $U \cap V_\lambda = 0$.
\end{proposition}

\begin{proof}
    Since $\dim(V_\lambda) = 1$, if $U \cap V_\lambda \neq 0$ then $V_\lambda \subseteq U$ and then since $V_\lambda$ generates $V$, it is clear that $U = V$. On the other hand if $U \cap V_\lambda = 0$ then clearly $U$ is proper.
\end{proof}

\begin{proposition}
    Let $V$ be a highest weight module with highest weight $\lambda$. Then $V$ has a unique maximal proper submodule. Moreover $V$ has a unique irreducible quotient.
\end{proposition}

\begin{proof}
    Let $\Sigma$ be the set of proper submodules of $V$, and let
    \[
        W = \sum_{U \in \Sigma} U.
    \]
    By Proposition 3 each $U \in \Sigma$ is diagonalizable, so evidently for $\mu \in \mathfrak{h}^*$
    \[
        W_\mu = \sum_{U \in \Sigma} U_\mu.
    \]

    We apply this with $\mu = \lambda$. Since $U \in \Sigma$ is proper, $U_\lambda = 0$ by the previous proposition, and so $W_\lambda = 0$. This shows that $W$ is proper. We have proved that $W$ is the unique maximal proper submodule of $V$, and consequently $V/W$ is the unique irreducible quotient.
\end{proof}

\begin{theorem}
    Let $\lambda \in V^*$. There is a highest weight module $M = M(\lambda)$ with highest weight vector $m \in M_\lambda$ with the following universal property. If $V$ is another highest weight module with highest weight $\lambda$ and if $v \in V_\lambda$, then there is a unique $\mathfrak{g}$-module homomorphism $M \to V$ mapping $m \mapsto v$. The map $\xi \mapsto \xi \cdot v$ is vector space isomorphism $U(\mathfrak{n}^-) \to M$.
\end{theorem}

\begin{proof}
    Note that since $\mathfrak{h}$ normalizes $\mathfrak{n}^+$, $\mathfrak{b} = \mathfrak{h} \oplus \mathfrak{n}^+$ is a subalgebra of $\mathfrak{g}$, the ``Borel subalgebra.'' As in Lemma 4, $U(\mathfrak{g}) \cong U(\mathfrak{n}^-) \otimes U(\mathfrak{b})$, that is, the multiplication map $U(\mathfrak{n}^-) \times U(\mathfrak{b}) \to U(\mathfrak{g})$ induces a vector space isomorphism $U(\mathfrak{n}^-) \otimes U(\mathfrak{b}) \to U(\mathfrak{g})$. This result is a simple consequence of this fact.

    To elaborate, regarding $\mathbb{C}$ as a one-dimensional abelian Lie algebra, we have a Lie algebra homomorphism $\theta_\lambda : \mathfrak{b} \to \mathbb{C}$ that maps $H \in \mathfrak{h}$ to $\lambda(H)$, and $\mathfrak{n}^+$ to zero. Thus let $H_1, \dots, H_\ell$ be a basis of $\mathfrak{h}$ and $x_\alpha$ ($\alpha \in \Phi^+$) be a basis of $\mathfrak{n}^+$. By the PBW theorem, the elements
    \[
        H_1^{k_1} \cdots H_\ell^{k_\ell} \prod_{\alpha \in \Phi^+} x_\alpha^{k_\alpha}
    \]
    with $k_i$ and $k_\alpha$ nonnegative integers are a basis for $U(\mathfrak{b})$. It is understood that in the product $\prod x_\alpha^{k_\alpha}$ the roots $\alpha \in \Phi^+$ are taken in a fixed definite order. We then have
    \[
        \theta_\lambda \big( H_1^{k_1} \cdots H_\ell^{k_\ell} \prod_{\alpha} x_\alpha^{k_\alpha} \big)
        = \begin{cases}
            \prod \lambda(H_i)^{k_i} & \text{if all } k_\alpha = 0, \\
            0                        & \text{if any } k_\alpha > 0.
        \end{cases}
    \]

    Now let $J_\psi$ be the left ideal generated by $\xi - \theta_\lambda(\xi)$ for $\xi \in \mathfrak{b}$. Let
    \[
        M = M(\lambda) = U(\mathfrak{g})/J_\psi,
    \]
    and let $m$ be the image of $1 \in U(\mathfrak{g})$ in $M(\lambda)$.

    It is clear from the PBW theorem that $H v = \lambda(H)m$ for $H \in \mathfrak{h}$, while $\mathfrak{n}^+ v = 0$, and moreover from $U(\mathfrak{g}) \cong U(\mathfrak{n}^-) \otimes U(\mathfrak{b})$, it is clear that every element of $M(\lambda)$ may be written uniquely as $\eta \cdot v$ for $\eta \in U(\mathfrak{n}^-)$.

    Now let us verify the universal property. Let $V$ be a highest weight module with weight $\lambda$, and let $v_\lambda \in V_\lambda$ be a generator. Then we have a surjective $U(\mathfrak{g})$-module homomorphism
    \[
        U(\mathfrak{g}) \to V, \quad \xi \mapsto \xi \cdot v_\lambda,
    \]
    and since $\beta \cdot v = \theta_\lambda(\beta)v$ for $\beta \in \mathfrak{b}$, $J_\psi$ is in the kernel. Thus the map factors uniquely through $U(\mathfrak{g})/J_\psi = M(\lambda)$.
\end{proof}

\begin{corollary}
    Let $\lambda \in \mathfrak{h}^*$. Up to isomorphism, $\mathfrak{g}$ has a unique irreducible highest weight module $L(\lambda)$ with highest weight $\lambda$.
\end{corollary}

\begin{proof}
    Every highest weight module is a quotient of $M(\lambda)$. Since $M(\lambda)$ has a unique irreducible quotient, there is a unique irreducible highest weight module.
\end{proof}

\begin{remark}
    The irreducible quotient $L(\lambda)$ might be finite or infinite dimensional. Recall that $\lambda$ is called \textbf{integral} if $\langle \alpha^\vee, \lambda \rangle \in \mathbb{Z}$ for all coroots $\alpha^\vee$, and \textbf{dominant} if $\langle \alpha^\vee, \lambda \rangle \geq 0$. If $\lambda$ is a dominant integral weight, then $L(\lambda)$ is finite-dimensional. On the other hand if $\lambda$ is not integral, $L(\lambda)$ will be infinite dimensional, and unless $\langle \alpha^\vee, \lambda \rangle \in \mathbb{Z}$ for some coroot $\alpha^\vee$, we will actually have $M(\lambda)$ irreducible, and $L(\lambda) = M(\lambda)$.
\end{remark}

\begin{remark}[Reducible highest weight modules are not unique] Let $\mathfrak{g}$ have two simple roots $\alpha_1, \alpha_2$ (e.g. $\mathfrak{sl}_3$). Fix a weight $\lambda$ such that both integers
    $\langle \lambda+\rho, \alpha_1^\vee \rangle, \langle \lambda+\rho, \alpha_2^\vee \rangle$
    are positive. Then the Verma module $M(\lambda)$ contains two distinct singular vectors (i.e. highest weight vectors inside $M(\lambda)$ below the top) of weights $s_1\!\cdot\!\lambda$ and $s_2\!\cdot\!\lambda$ (dot action).
    They generate two different submodules $N_1 = U(\mathfrak{g})\,v_{s_1\cdot\lambda}, N_2 = U(\mathfrak{g})\,v_{s_2\cdot\lambda}$. Now the quotients $M(\lambda)/N_1$, $M(\lambda)/N_2$ are both highest weight modules of highest weight $\lambda$, both reducible, and not isomorphic (their composition series differ). Hence reducible highest weight modules with the same top weight are not unique.
\end{remark}

\begin{remark}
    [Importance of the dot action] The BGG theorem tells us: if $\mu$ is a weight such that $\langle \lambda+\rho,\alpha^\vee\rangle \in \mathbb{Z}_{>0}$, then there exists a singular vector in $M(\lambda)$ of weight $s_\alpha\cdot\lambda := s_\alpha(\lambda+\rho)-\rho$.

    Additionally, in category $\mathcal{O}$, irreducible highest weight modules $L(\lambda)$ can only appear as composition factors of Verma modules $M(\mu)$ if $\lambda$ and $\mu$ are in the same dot-orbit under the Weyl group. The dot action partitions the weight lattice into blocks inside which the category decomposes.

    One can also give an interpretation via Harish-Chandra isomorphism. The center $Z(U(\mathfrak{g}))$ acts on a Verma module $M(\lambda)$ by a character. Harish-Chandra's isomorphism says these central characters are Weyl group invariant under the dot action. In other words, two highest weights $\lambda,\mu$ have the same central character iff they're in the same dot orbit.
\end{remark}

More precisely, the BGG theorem states:
\begin{theorem}[BGG theorem]
    If $\lambda \in \mathfrak{h}^*$, $\alpha$ a positive root, and
    $m = \langle \lambda+\rho, \alpha^\vee\rangle \in \mathbb{Z}_{>0}$,
    then there exists a nonzero homomorphism of Verma modules
    $M(s_\alpha \cdot \lambda) \hookrightarrow M(\lambda)$,
    where $s_\alpha$ is the reflection in the Weyl group, and the dot action is
    $w \cdot \lambda = w(\lambda+\rho) - \rho$.


    Concretely: inside $M(\lambda)$, there is a singular vector of weight $s_\alpha \cdot \lambda$, which generates a highest weight submodule isomorphic to $M(s_\alpha \cdot \lambda)$. If $w \leq w'$ in Bruhat order, then $M(w \cdot \lambda) \hookrightarrow M(w' \cdot \lambda)$.
\end{theorem}

\begin{theorem}[Classification of finite-dimensional irreducible modules]\label{thm:fd-classification}
    Let $V$ be a finite dimensional irreducible module. Then
    $V \cong L(\lambda)$ where $\lambda$ is a dominant integral weight.
    Conversely, if $\lambda$ is a dominant integral weight, then $L(\lambda)$
    is finite-dimensional.
\end{theorem}

\begin{proof}
    Assume that $V$ is finite-dimensional. Choose a vector
    $v \in V_\lambda$ where $\lambda$ is a weight of $V$ that is maximal with
    respect to $\succ$. If $\alpha \in \Phi^+$ then
    $x_\alpha v \in V_{\lambda + \alpha}$ so $x_\alpha v = 0$. Therefore $v$
    is a highest weight vector. Then $V = U(\mathfrak{g})v$ since $V$ is
    irreducible. We have proved that $V$ is a highest weight module; it is
    irreducible so $V \cong L(\lambda)$.

    To show that $\lambda$ is a dominant integral weight, let $\alpha$ be a
    simple positive root. The restriction of $V$ to the $\mathfrak{sl}_2$
    spanned by $x_\alpha, x_{-\alpha}$ and $h_\alpha$ is finite-dimensional,
    and $x_\alpha v = 0$. From the classification of finite-dimensional
    $\mathfrak{sl}_2$-modules, this means that
    $(\alpha^\vee \mid \lambda) = \lambda(h_\alpha) \in \mathbb{Z}$ is a
    nonnegative integer. Therefore $\lambda$ is dominant and integral.

    We will omit the slightly tedious proof of the converse, that if $\lambda$ is a dominant integral weight then $L(\lambda)$ is finite-dimensional. For a proof of this see Kac, Lemma 10.1.
\end{proof}

\begin{corollary}[Weyl]
    For $V$ an irreducible finite-dimensional $\mathfrak{g}$-module, the highest
    weight $\lambda$ is a dominant integral weight, and
    \[
        V \longleftrightarrow \lambda
    \]
    is a bijection between the irreducible highest weight modules and the
    dominant integral weights.
\end{corollary}

Now let $\mf g$ be finite dimensional. The Casimir element of the universal enveloping algebra $U(\mathfrak{g})$ may be defined as follows. Let $\{\gamma_i\}$ be a basis of $\mathfrak{g}$ and $\{\gamma^i\}$ the dual basis with respect
to the Killing form, so $\kappa(\gamma_i, \gamma^j) = \delta_{ij}$. Then
\[
    c_{\mathfrak{g}} = \sum_{i=1}^{\dim(\mathfrak{g})} \gamma_i \gamma^i
\]

\begin{proposition}
    $c_{\mathfrak{g}}$ is independent of the choice of basis $\{\gamma_i\}$.
    It lies in the center of $U(\mathfrak{g})$.
\end{proposition}

\begin{proof}
    For any finite-dimensional vector space $V$, there is a canonical iso
    $V \otimes V^* \cong \operatorname{End}(V)$, $v\otimes \phi \mapsto (w \mapsto \phi(w)\,v)$. Under this isomorphism, the element $\sum_i v_i \otimes \phi_i$, where $\{v_i\}$ is a basis and $\{\phi_i\}$ the dual basis, corresponds to the identity map on $V$. This element is independent of the chosen basis (it's just the coordinate expression of the identity endomorphism). Now take $V = \mathfrak{g}$ with the Killing form $\kappa$. So the canonical element $\sum_i \gamma_i \otimes \gamma^i \in \mathfrak{g} \otimes \mathfrak{g}$ corresponds to the identity operator $\operatorname{id}_{\mathfrak{g}}$. Finally push $\Omega$ into $U(\mathfrak{g})$ using multiplication \[c_{\mathfrak{g}} = m(\Omega) = \sum_i \gamma_i \gamma^i\]

    This shows that $c_{\mathfrak{g}}$ is independent of the choice of basis. To see that $c_{\mathfrak{g}}$ is central, let $x \in \mathfrak{g}$. Then

    Write
    \[
        [x, \gamma_i] = \sum_j a_{ij} \gamma_j.
    \]
    Use $\operatorname{ad}$-invariance of $\kappa$:
    \[
        0 = \kappa([x, \gamma_i], \gamma^k) + \kappa(\gamma_i, [x, \gamma^k])
        = a_{ik} + \kappa(\gamma_i, [x, \gamma^k]).
    \]
    If we expand $[x, \gamma^k] = \sum_j b_{kj} \gamma^j$, the relation above gives
    \[
        b_{ki} = -a_{ik},
    \]
    i.e.
    \[
        [x, \gamma^k] = -\sum_i a_{ik} \gamma^i.
    \]
    Now compute in $U(\mathfrak{g})$:
    \begin{align*}
        [x, c_{\mathfrak{g}}]
         & = \sum_i [x, \gamma_i] \gamma^i + \sum_i \gamma_i [x, \gamma^i] \texty{by the Leibniz rule} \\
         & = \sum_{i, j} a_{ij} \gamma_j \gamma^i - \sum_{i, k} a_{ki} \gamma_i \gamma^k.
    \end{align*}
    so $[x, c_{\mathfrak{g}}] = 0$.
\end{proof}

\begin{proposition}
    Let $h_i$ be a basis of $\mathfrak{h}$ and let $h^i$ be the dual basis of $\mathfrak{h}$
    with respect to the Killing form, so $\kappa(h_i, h^j) = \delta_{ij}$.
    Then if $\lambda, \mu \in \mathfrak{h}^*$ we have
    \[
        (\lambda | \mu) = \sum_i \lambda(h^i)\mu(h_i).
    \]
\end{proposition}

\begin{proof}
    Recall that the defining property of $t_\mu$ is that
    \[
        \kappa(t_\mu, h) = \mu(h) \quad \text{for all } h \in \mathfrak{h}.
    \]
    First let us show that
    \begin{equation}\label{eq:tmu}
        t_\mu = \sum_i \mu(h_i) h^i.
    \end{equation}
    To check this, we pair both sides with $h_j$. We have
    \[
        \kappa(t_\mu, h_j) = \mu(h_j) =
        \kappa\!\left( \sum_i \mu(h_i) h^i,\, h_j \right) = \mu(h_j)
    \]
    is exactly the defining property of $t_\mu$. Since the $h_j$ span $\mathfrak{h}$ and $\kappa$ restricted to $\mathfrak{h}$ is nondegenerate, this proves \eqref{eq:tmu}.

    Now \eqref{eq:tmu} implies
    \[
        (\lambda|\mu) = \kappa(t_\lambda, t_\mu)
        = \sum_i \mu(h_i)\kappa(t_\lambda, h^i)
        = \sum_i \mu(h_i)\lambda(h^i).
    \]
    again using the defining property of $t_\mu$.
\end{proof}

\begin{proposition}
    Let $V$ be a highest weight module with highest weight $\lambda$.
    Then the Casimir element $c_{\mathfrak{g}}$ acts by the scalar
    \[
        |\lambda + \rho|^2 - |\rho|^2
    \]
    on $V$.
\end{proposition}

\begin{proof}
    Since $c_{\mathfrak{g}}$ is central in $U(\mathfrak{g})$ it commutes with the action of
    $\mathfrak{g}$ on any module $V$. Because $V$ is generated by a highest weight vector
    $v_\lambda \in V_\lambda$, it is sufficient to show that
    \[
        c_{\mathfrak{g}} v = (|\lambda + \rho|^2 - |\rho|^2)v
    \]

    We need to choose dual bases of $\mathfrak{g}$ with respect to the Killing form.
    For one basis, we choose a basis $h_i$ of $\mathfrak{h} = \mathfrak{g}_0$,
    and vectors $x_\alpha \in \mathfrak{g}_\alpha$.

    Now we describe the dual basis. We know that the Killing form is nondegenerate on $\mathfrak{h}$,
    so we find $h^i$ such that $\kappa(h_i,h^j) = \delta_{ij}$.
    Then we define another set of representatives $y_\alpha \in \mathfrak{g}_\alpha$ so that
    \[
        y_\alpha = \frac{x_\alpha}{\kappa(x_\alpha,x_{-\alpha})}
    \]
    so that
    \[
        \kappa(x_\alpha, y_{-\beta}) = \delta_{\alpha\beta}
    \]

    Thus we have dual bases $\{h_i, x_\alpha\}$ and $\{h^i, y_{-\alpha}\}$.
    Then
    \[
        c_{\mathfrak{g}} = \sum_i h_i h^i + \sum_{\alpha \in \Phi} x_\alpha y_{-\alpha}.
    \]

    We want to rewrite this slightly. We write this as
    \[
        c_{\mathfrak{g}} = \sum_i h_i h^i + \sum_{\alpha \in \Phi^+} x_\alpha y_{-\alpha}
        + \sum_{\alpha \in \Phi^+} x_{-\alpha} y_\alpha.
    \]



    Now observe that $[x_\alpha,y_{-\alpha}] = t_\alpha$. Certainly $[x_\alpha,y_{-\alpha}] \in \mathfrak{h}$ so write it as $t_\alpha$. Then for $h \in \mathfrak{h}$, we check that $\kappa([x_\alpha,y_{-\alpha}],h) = \alpha(h)$.
    \begin{align*}
        \kappa([x_\alpha, y_{-\alpha}], h) = \kappa(x_\alpha, [y_{-\alpha}, h])
    \end{align*}

    But since $h$ acts on the root vector $y_{-\alpha}$ by $[h,y_{-\alpha}] = -\alpha(h)y_{-\alpha}$, we get $[y_{-\alpha},h] = \alpha(h) y_{-\alpha}$. So $\kappa([x_\alpha,y_{-\alpha}], h) = \kappa(x_\alpha, \alpha(h) y_{-\alpha}) = \alpha(h)\kappa(x_\alpha,y_{-\alpha})$.

    And by construction of $y_{-\alpha}$, $\kappa(x_\alpha,y_{-\alpha})=1$.
    Thus
    $\kappa([x_\alpha,y_{-\alpha}],h) = \alpha(h)$.

    That is exactly the defining property of $t_\alpha$. Hence
    $[x_\alpha,y_{-\alpha}] = t_\alpha$.

    So in the enveloping algebra
    \[
        x_\alpha y_{-\alpha} = t_\alpha + y_{-\alpha}x_\alpha.
    \]

    Thus
    \[
        c_{\mathfrak{g}} = \sum_i h_i h^i + \sum_{\alpha \in \Phi^+} t_\alpha
        + \sum_{\alpha \in \Phi^+} (y_{-\alpha}x_\alpha + x_{-\alpha}y_\alpha).
    \]

    Since $v_\lambda$ is a highest weight vector it is annihilated by $x_\alpha$ and $y_\alpha$
    when $\alpha \in \Phi^+$. On the other hand,
    $Hv_\lambda = \lambda(H)v_\lambda$ for $H \in \mathfrak{h}$, and so
    \[
        c_{\mathfrak{g}} v_\lambda
        = \sum_i \lambda(h_i)\lambda(h^i) + \sum_{\alpha \in \Phi^+} \lambda(t_\alpha).
    \]
    The first expression equals $(\lambda|\lambda)$ by the previous proposition, while
    \[
        \sum_{\alpha \in \Phi^+} \lambda(t_\alpha)
        = \sum_{\alpha \in \Phi^+} \langle \lambda, \alpha \rangle
        = 2(\lambda|\rho).
    \]

    Thus
    \[
        c_{\mathfrak{g}} v_\lambda
        = \big( (\lambda|\lambda) + 2(\lambda|\rho)\big) v_\lambda
        = \big( (\lambda+\rho|\lambda+\rho) - (\rho|\rho)\big)v_\lambda,
    \]
    as desired.
\end{proof}

\subsection{Category $\mathcal{O}$ and the Weyl character formula}
We will now prove the Weyl character formula following Kac. It will be useful to work in the following category of representations, Category $\mathcal{O}$, introduced by Bernstein, Gelfand and Gelfand.

\begin{definition}
    A module is in Category $\mathcal{O}$ if it is $\mathfrak{h}$-diagonalizable with finite dimensional weight spaces $V_\lambda$, such that there exists a finite set of weights $\{\lambda_1,\dots,\lambda_N\}$ such that $V_\mu = 0$ unless $\mu \preceq \lambda_i$ for some $i$.
\end{definition}

This category contains all highest weight modules, is closed under finite direct sums, and it contains all submodules and quotient modules of a Category $\mathcal{O}$ module. In particular it is an abelian category with enough projectives and injectives and has a good homological theory. The Verma modules $M(\lambda)$ may or may not be irreducible. We will say a module $V$ is a \textbf{subquotient} of a module $W$ if there are submodules $U \supset Q$ of $W$ such that $U/Q \cong V$. Thus either a submodule or a quotient module is a subquotient.

\begin{proposition}
    Suppose that $V$ is a highest weight module with weight $\mu$ and $V$ is a subquotient of $M(\lambda)$. Then
    \[
        |\lambda + \rho|^2 = |\mu + \rho|^2.
    \]
\end{proposition}

\begin{proof}
    Since $c$ commutes with the action of $\mathfrak{g}$ it must act as a scalar on $M(\lambda)$, and we computed that scalar to be $|\lambda + \rho|^2 - |\rho|^2$. So it acts by the same scalar on any submodule, quotient module or subquotient. Also $c$ acts by the scalar $|\mu + \rho|^2 - |\rho|^2$ on any highest weight module $V$ with highest weight $\lambda$, so
    \[
        |\lambda + \rho|^2 - |\rho|^2 = |\mu + \rho|^2 - |\rho|^2.
    \] as desired.
\end{proof}

\begin{definition}
    Let $V$ be a module in Category $\mathcal{O}$. We define the \textbf{character} of $V$ to be the formal expression
    \[
        \chi_V = \sum_{\lambda} \dim(V_\lambda) e^\lambda
    \]
    where $e^\lambda$ is a formal symbol for $\lambda \in \mathfrak{h}^*$.
\end{definition}

\begin{proposition}[Character of Verma modules]\label{prop:char-verma}
    The character of $M(\lambda)$ is
    \[
        e^\lambda \prod_{\alpha \in \Phi^+} (1 - e^{-\alpha})^{-1}.
    \]
\end{proposition}

\begin{proof}
    Let $v$ be the highest weight vector. We recall from Theorem~8 that the map
    \[
        \xi \mapsto \xi \cdot v
    \]
    from $U(\mathfrak{n}^-)$ to $M(\lambda)$ is a vector space isomorphism. So by the PBW theorem a basis of $M(\lambda)$ consists of the vectors
    \[
        \left( \prod_{\alpha \in \Phi^+} x_{-\alpha}^{k_\alpha} \right) v, \qquad k_\alpha \geq 0,
    \]
    where the positive roots $\Phi^+$ are taken in some fixed definite order. The weight of this vector is
    \[
        \lambda - \sum_{\alpha \in \Phi^+} k_\alpha \alpha,
    \]
    so
    \[
        \chi_V = e^\lambda \prod_{\alpha \in \Phi^+} e^{-k_\alpha \alpha}
        = e^\lambda \prod_{\alpha \in \Phi^+} (1 - e^{-\alpha})^{-1}.
    \] as desired.
\end{proof}

\begin{remark}
    Note that we get a geometric series at the end because $M(\lambda)$ is a Verma module: it has no relations among the negative root vectors beyond the Lie algebra relations themselves. This is what makes Verma modules universal highest weight modules: you can push down indefinitely.
\end{remark}

\begin{definition}
    Let $V$ be a module in Category $\mathcal{O}$. A nonzero vector $v \in V$ is called \textbf{primitive} if there exists a proper submodule $U \subset V$ such that $v \notin U$ but $x_\alpha v \in U$ for all $\alpha \in \Phi^+$ (or equivalently, for all simple roots). We can take $U=0$, so if $x_\alpha v=0$ then $v$ is primitive. In other words, a highest weight vector is a primitive vector. More generally, $v$ being primitive means that the image of $v$ in $V/U$ is a highest weight vector for some proper submodule $U$ of $V$. We will call $\mu$ a \textbf{primitive weight} if $V_\mu$ contains a primitive vector.
\end{definition}

A primitive vector is like a “hidden” highest weight vector, but visible only in a quotient.

\begin{proposition}
    Let $V$ be a module in Category $\mathcal{O}$. Then $V$ is generated by its primitive vectors.
\end{proposition}

\begin{proof}
    If not, consider the submodule $U$ generated by the primitive vectors. Then $Q = V/U$ would be a nonzero submodule. If we choose a nonzero vector in $Q$ whose weight is maximal with respect to $\preceq$, then its preimage in $V$ would be a primitive vector, which is a contradiction.
\end{proof}

\begin{proposition}
    Let $V$ be a module in Category $\mathcal{O}$. Assume that $V$ has only a finite number of weights. Then $V$ has finite length. That is, it has a composition series
    \[
        V = V_m \supset V_{m-1} \supset \cdots \supset V_0 = 0
    \]
    such that each quotient $V_i/V_{i-1}$ is irreducible, isomorphic to $L(\mu)$, where $\mu$ is a primitive weight of $V$. \textbf{(The quotients $V_i/V_{i-1}$ are called composition factors, and they are independent of the composition series, by the Jordan–Hölder theorem.)}
\end{proposition}

\begin{proof}
    We argue by induction on the number of linearly independent primitive vectors.

    Choose a primitive weight $\mu$ that is maximal with respect to $\preceq$. Then clearly a primitive vector $v$ of weight $\mu$ must be a highest weight vector, so $W = U(\mathfrak{g}) \cdot v = U(\mathfrak{n}^-)v$ is a highest weight module. It has a maximal submodule $W'$ and the quotient $Q = W/W'$ is irreducible. Both $V/W$ and $W'$ have fewer independent primitive vectors than $V$ (note that there are finitely many weights and each weight space is finite dimensional since we are in Category $\mathcal{O}$), so by induction they have finite length. Since $V/W$, $W'$ and the irreducible quotient $W/W'$ all have finite length, it follows that $V$ has finite length.
\end{proof}

\begin{proposition}[Character of irreducible highest weight modules]\label{prop:char-irr}
    Let $\lambda \in \mathfrak{h}^*$. Then the character $\chi_{L(\lambda)}$ is of the form
    \begin{align}
        \chi_{L(\lambda)} = \sum_{\substack{\mu \preceq \lambda \\ |\mu+\rho|^2 = |\lambda+\rho|^2}} c_\mu \chi_{M(\mu)}
    \end{align}
    where $c_\lambda = 1$.
\end{proposition}

\begin{proof}
    The weight $\mu$ of a primitive vector must satisfy $\mu \preceq \lambda$ and
    $|\mu+\rho|^2 = |\lambda+\rho|^2$.

    Since the inner product is positive definite, this implies that there are only a finite number of possible weights for primitive vectors (because $|\mu+\rho|^2 = |\lambda+\rho|^2 $ cuts out a sphere, and the lattice of weights intersected with that sphere is finite). $M(\mu)$ has finite length because only finitely many irreducibles can appear as factors, and modules in Category $\mathcal{O}$ have finite dimensional weight spaces. Also the composition factors of $M(\mu)$ must be $L(\nu)$ where $|\nu+\rho|^2 = |\mu+\rho|^2 = |\lambda+\rho|^2$. This is because every composition factor is a highest weight module and every irreducible highest weight module is of the form $L(\nu)$.

    Let $d(\mu,\nu)$ be the multiplicity of such $L(\nu)$. Then
    \[
        \chi_{M(\mu)} = \sum_{\substack{\nu \preceq \mu \\ |\nu+\rho|^2 = |\lambda+\rho|^2}}
        d(\mu,\nu)\chi_{L(\nu)}.
    \]

    Now the matrix $d(\mu,\nu)$ indexed by pairs $\mu,\nu$ is triangular since
    $d(\mu,\mu) = 1$ and $d(\mu,\nu) = 0$ unless $\nu \preceq \mu$. So it is invertible and we may write
    \[
        \chi_{L(\mu)} = \sum_{\substack{\nu \preceq \mu \\ |\nu+\rho|^2 = |\lambda+\rho|^2}}
        d'(\mu,\nu)\chi_{M(\nu)}.
    \]

    Applying this to $\mu = \lambda$ gives (2).
\end{proof}

We will define the \textbf{Weyl denominator}
\[
    \Delta = e^\rho \prod_{\alpha \in \Phi^+} (1 - e^{-\alpha}).
\]

\begin{lemma}
    Let $w \in W$ (the Weyl group). Then
    \[
        w(\Delta) = \operatorname{sgn}(w)\Delta.
    \]
\end{lemma}

\begin{proof}
    It is sufficient to check this if $w = s_{\alpha_i}$ is a simple reflection. We recall that
    $s_{\alpha_i}$ maps the simple root $\alpha_i$ to $-\alpha_i$ and it permutes the remaining
    positive roots. Moreover $s_{\alpha_i}(\rho) = \rho - \alpha_i$. So if we write
    \[
        \Delta = e^\rho (1 - e^{-\alpha_i}) \prod_{\substack{\alpha \in \Phi^+ \\ \alpha \neq \alpha_i}}
        (1 - e^{-\alpha}),
    \]
    then $s_i$ maps $e^\rho (1 - e^{-\alpha_i})$ to
    \[
        e^{\rho-\alpha_i}(1 - e^{\alpha_i}) = - e^\rho (1 - e^{-\alpha_i}),
    \]
    and it fixes the product. Hence $s_i(\Delta) = -\Delta$.
\end{proof}

\begin{theorem}[Weyl Character Formula]
    Let $V$ be a finite-dimensional irreducible representation of $\mathfrak{g}$. Thus by Theorem \ref{thm:fd-classification} there is a dominant integral weight $\lambda$ such that $V = L(\lambda)$. We have
    \[
        \chi_V = \Delta^{-1} \sum_{w \in W} \operatorname{sgn}(w) e^{w(\lambda + \rho)}
    \]
    where $W$ is the Weyl group and
    \[
        \Delta = e^{\rho} \prod_{\alpha \in \Phi^+} (1 - e^{-\alpha}).
    \]
\end{theorem}

The following argument is due to Kac, improving the proof of BGG.  As an application, Kac extended the applicability of the Weyl character formula for characters of integrable representations of infinite-dimensional Kac-Moody Lie algebras. We will discuss this in more detail in the section, following Chapter 10 of Kac \cite{kac}.

\begin{proof}
    Using Proposition \ref{prop:char-verma} we may rewrite (2) in the form (note that the $\rho$ in the exponent comes from dividing by $\Delta$)
    \[
        \chi_{L(\lambda)} = \sum_{\substack{\mu \preceq \lambda \\ |\mu+\rho|^2 = |\lambda+\rho|^2}} c_{\mu} e^{\mu+\rho} \Delta^{-1}
    \]
    It may be simpler to write this as
    \[
        \chi_{L(\lambda)} = \sum_{\mu \in P^+} c_{\mu} e^{\mu+\rho} \Delta^{-1}
    \]
    and remember that $c_{\mu} = 0$ unless $\mu \preceq \lambda$ and $|\mu + \rho|^2 = |\lambda + \rho|^2$. We claim that if $w \in W$, then
    \begin{align}
        c_{\mu} = \operatorname{sgn}(w) c_{w \circ \mu}.
    \end{align}
    Indeed, since $\chi_{L(\lambda)}$ is invariant under the action of $W$, and since $w(\Delta) = \operatorname{sgn}(w)\Delta$, we have an identity
    \[
        \sum_{\mu \in P^+} c_{\mu} e^{\mu+\rho} \Delta^{-1} = \sum_{\mu \in P^+} \operatorname{sgn}(w)c_{\mu} e^{w(\mu+\rho)} \Delta^{-1}
    \]
    and comparing the coefficients of $e^{w \circ \mu} = e^{w(\mu+\rho)-\rho}$ on both sides of this equation gives (2).

    We know that $c_{\lambda} = 1$, since this is part of Proposition \ref{prop:char-irr}. So by (2), we will have terms corresponding to $\mu$ of the form $w \circ \lambda$ and the sum of these terms is
    \[
        \Delta^{-1} \sum_{w \in W} c_{w \circ \lambda} e^{w(\lambda+\rho)-\rho} e^{\rho} = \Delta^{-1} \sum_{w \in W} \operatorname{sgn}(w) e^{w(\lambda+\rho)}.
    \]

    This is the right hand side of the Weyl character formula, so our task is to show that these are the \textbf{only terms}. That is, we must show that $c_\mu = 0$ unless $\mu$ is of the form $w \circ \lambda$ for some $w \in W$.

    Therefore we start with $\mu$ such that $c_\mu \neq 0$. By Proposition \ref{prop:fundamental_domain}, there exists $w \in W$ such that $w(\mu + \rho)$ is dominant. Let $\nu = w \circ \mu = w(\mu + \rho) - \rho$. We will show that $\nu = \lambda$. In any case by (2), $c_\nu \neq 0$ and so $\nu \preccurlyeq \lambda$ and $|\lambda + \rho|^2 = |\nu + \rho|^2$. We write
    \[
        \lambda - \nu = \sum_{\alpha \in \Phi^+} k_\alpha \alpha,
    \]
    where since $\nu \preccurlyeq \lambda$ we have $k_\alpha \geq 0$. We note the identity, for $a, b \in \mathfrak{h}^*$:
    \[
        |a|^2 - |b|^2 = (a+b|a-b).
    \]

    We apply this and learn that
    \[
        |\lambda + \rho|^2 - |\nu + \rho|^2 \;=\;
        \Bigl( \lambda + \nu + 2\rho \,\Big|\, \sum_{\alpha \in \Phi^+} k_\alpha \alpha \Bigr).
    \] Now $\lambda$ and $\nu + \rho = w(\mu + \rho)$ are both dominant, so $\lambda + \nu + 2\rho$ is \textbf{strongly dominant} meaning
    \[
        (\alpha^\vee \,|\, \lambda + \nu + 2\rho) > 0
    \]
    for all positive roots $\alpha$. So $|\lambda + \rho|^2 = |\nu + \rho|^2$ implies that $k_\alpha = 0$ for all $\alpha$ and so $\nu = \lambda$.
\end{proof}

\section{Introduction to infinite-dimensional Lie algebras}
We begin with some definitions and constructions that will allow us to define Kac-Moody Lie algebras. Then we introduce key tools for studying their representations, such as an invariant bilinear form and the generalized Casimir operator. This will enable us to formulate and prove a version of the Weyl character formula.
This section follows \cite{kac}.
\subsection{Basic definitions}
\begin{definition}
    A \textbf{Cartan matrix} is a square integer matrix $A = (a_{ij})$ of rank $l$ such that
    \begin{itemize}
        \item $a_{ii} = 2$ for all $i$,
        \item $a_{ij} \leq 0$ for $i \neq j$,
        \item $a_{ij} = 0$ if and only if $a_{ji} = 0$.
    \end{itemize}
    A \textbf{realization} of a Cartan matrix $A$ is a triple $(\mathfrak{h}, \Pi, \Pi^\vee)$ where $\mathfrak{h}$ is a complex vector space, $\Pi = \{\alpha_1, \dots, \alpha_n\} \subset \mathfrak{h}^*$ and $\Pi^\vee = \{\alpha_1^\vee, \dots, \alpha_n^\vee\} \subset \mathfrak{h}$ are linearly independent sets such that $\alpha_j(\alpha_i^\vee) = a_{ij}$ for all $i,j$ and $\dim(\mathfrak{h}) = 2n - l$.
\end{definition}

\begin{remark}[Finite-dimensional Cartan matrices]
    In the finite-dimensional case, the Cartan matrix is invertible and positive definite and $l = n$, so $\dim(\mathfrak{h}) = n$. The set of simple roots $\Pi = {\alpha_1, \dots, \alpha_n}$ is a basis of the real vector space spanned by the roots $E = \mathbb{R}\Phi \subseteq \mathfrak{h}^*$. Similarly, the set of simple coroots $\Pi^\vee = {\alpha_1^\vee, \dots, \alpha_n^\vee}$ is a basis of $E^\vee = \mathbb{R}\Phi^\vee \subseteq \mathfrak{h}$.
\end{remark}

Denote by $Q$ the root lattice, i.e. the integer span of the simple roots $\Pi$. Let $Q^+ = \sum_{i=1}^n \mathbb{Z}_{\geq 0} \alpha_i$ be the positive cone in $Q$. We write $\beta \geq 0$ if $\beta \in Q^+$ and $\beta > 0$ if $\beta \in Q^+ \setminus \{0\}$. We define a partial order on $\mathfrak{h}^*$ by $\lambda \preceq \mu$ if and only if $\mu - \lambda \geq 0$. The sum of the coefficients of $\beta = \sum_i k_i \alpha_i$ is called the \textbf{height} of $\beta$ and denoted $\operatorname{ht}(\beta) = \sum_i k_i$.

\begin{definition}
    [Universal Lie algebra associated to a Cartan matrix] Let $A = (a_{ij})$ be an $n \times n$-matrix over $\mathbb{C}$, and let
    $(\mathfrak{h}, \Pi, \Pi^\vee)$ be a realization of $A$. We introduce
    an auxiliary Lie algebra $\tilde{\mathfrak{g}}(A)$ with the generators
    $e_i, f_i \ (i=1,\dots,n)$ and $\mathfrak{h}$, and the following defining relations:
    \[
        \begin{aligned}
            [e_i, f_j] & = \delta_{ij}\alpha_i^\vee         &  & (i,j=1,\dots,n),                     \\
            [h,h']     & = 0                                &  & (h,h' \in \mathfrak{h}),             \\
            [h, e_i]   & = \langle \alpha_i, h \rangle e_i,                                           \\
            [h, f_i]   & = -\langle \alpha_i, h \rangle f_i &  & (i=1,\dots,n;\, h \in \mathfrak{h}).
        \end{aligned}
    \]

    By the uniqueness of the realization of $A$ it is clear that
    $\tilde{\mathfrak{g}}(A)$ depends only on $A$.
\end{definition}

\begin{remark}
    [What are coroots?] In the finite dimensional case, coroots come from the $\mathfrak{sl}_2$-subalgebras attached to each root: concretely, they are the Cartan elements $h_\alpha := [e_\alpha, f_\alpha]$ normalized with the Killing form so that root-coroot evaluation pairing are integers.
\end{remark}

Denote by $\tilde{\mathfrak{n}}_+$ (resp.\ $\tilde{\mathfrak{n}}_-$) the subalgebra of $\tilde{\mathfrak{g}}(A)$ generated by $e_1,\dots,e_n$ (resp.\ $f_1,\dots,f_n$).

\begin{theorem}[Properties of the universal Lie algebra associated to a Cartan matrix]\label{thm:universal-lie-alg}
    Let $\tilde {\mathfrak{g}}(A)$ be the Lie algebra associated to a Cartan matrix $A$ with realization $(\mathfrak{h}, \Pi, \Pi^\vee)$.
    \begin{enumerate}[label=\alph*)]
        \item $\tilde{\mathfrak{g}}(A) = \tilde{\mathfrak{n}}_- \oplus \mathfrak{h} \oplus \tilde{\mathfrak{n}}_+$ \quad (direct sum of vector spaces).

        \item $\tilde{\mathfrak{n}}_+$ (resp.\ $\tilde{\mathfrak{n}}_-$) is freely generated by $e_1,\dots,e_n$ (resp.\ $f_1,\dots,f_n$).

        \item The map $e_i \mapsto -f_i$, $f_i \mapsto -e_i \ (i=1,\dots,n)$,
              $h \mapsto -h \ (h\in\mathfrak{h})$, can be uniquely extended to an involution
              $\tilde{\omega}$ of the Lie algebra $\tilde{\mathfrak{g}}(A)$.

        \item With respect to $\mathfrak{h}$ one has the root space decomposition:
              \[
                  \tilde{\mathfrak{g}}(A)
                  = \left( \bigoplus_{\substack{\alpha \in Q_+ \\ \alpha \neq 0}}
                  \tilde{\mathfrak{g}}_{-\alpha} \right)
                  \oplus \mathfrak{h}
                  \oplus \left( \bigoplus_{\substack{\alpha \in Q_+ \\ \alpha \neq 0}}
                  \tilde{\mathfrak{g}}_{\alpha} \right),
              \]
              where
              \[
                  \tilde{\mathfrak{g}}_{\alpha} = \{ x \in \tilde{\mathfrak{g}}(A) \mid [h,x] = \alpha(h)x
                  \ \text{for all } h \in \mathfrak{h} \}.
              \]
              Furthermore, $\dim \tilde{\mathfrak{g}}_{\alpha} < \infty$, and
              $\tilde{\mathfrak{g}}_{\alpha} \subset \tilde{\mathfrak{n}}_\pm$
              for $\pm\alpha \in Q_+, \ \alpha \neq 0$.

        \item Among the ideals of $\tilde{\mathfrak{g}}(A)$ intersecting $\mathfrak{h}$ trivially,
              there exists a unique maximal ideal $\mathfrak{r}$. Furthermore,
              \[
                  \mathfrak{r}
                  = (\mathfrak{r} \cap \tilde{\mathfrak{n}}_-) \oplus (\mathfrak{r} \cap \tilde{\mathfrak{n}}_+)
                  \quad \text{(direct sum of ideals)}.
              \]
    \end{enumerate}
\end{theorem}

\begin{proof}
    Let $V$ be the $n$-dimensional complex vector space with a basis
    $v_1, \ldots, v_n$ and let $\lambda$ be a linear function on $\mathfrak{h}$.
    We define an action of the generators of $\tilde{\mathfrak{g}}(A)$ on the
    tensor algebra $T(V)$ over $V$ by
    \begin{align*}
        \alpha)\quad & f_i(a) = v_i \otimes a \quad \text{for } a \in T(V);                    \\
        \beta)\quad  & h(1) = \langle \lambda, h \rangle 1, \quad
        \text{and inductively on $s$,}                                                         \\
                     & h(v_j \otimes a) = -\langle \alpha_j, h \rangle v_j \otimes a
        + v_j \otimes h(a)
        \quad \text{for } a \in T^{s-1}(V), \ j=1,\ldots,n;                                    \\
        \gamma)\quad & e_i(1) = 0, \quad \text{and inductively on $s$,}                        \\
                     & e_i(v_j \otimes a) = \delta_{ij}\,\alpha_i^\vee(a) + v_j \otimes e_i(a)
        \quad \text{for } a \in T^{s-1}(V), \ j=1,\ldots,n.
    \end{align*}

    This defines a representation of the Lie algebra $\tilde{\mathfrak{g}}(A)$ on the space $T(V)$. To see that, we have to check all of the relations. Provided one does that, the statements of the theorem quickly follow.

    Using the relations it is easy to show by induction on $s$ that a product of $s$
    elements from the set $\{e_i, f_i (i = 1, \ldots, n); \ h\}$ lies in
    $\tilde{\mathfrak{n}}_- + \mathfrak{h} + \tilde{\mathfrak{n}}_+$. Let now
    $u = n_- + h + n_+ = 0$, where $n_\pm \in \tilde{\mathfrak{n}}_\pm$ and
    $h \in \mathfrak{h}$. Then in the representation $T(V)$ we have
    \[
        u(1) = n_-(1) + \langle \lambda, h \rangle = 0.
    \]
    It follows that $\langle \lambda, h \rangle = 0$ for every
    $\lambda \in \mathfrak{h}^*$ and hence $h = 0$.

    Furthermore, using the map $f_i \mapsto v_i$, we see that the tensor algebra $T(V)$ is an associative enveloping algebra of the Lie algebra $\tilde{\mathfrak{n}}_-$. Since $T(V)$ is a free associative algebra, we conclude that $T(V)$ is automatically the universal enveloping algebra $U(\tilde{\mathfrak{n}}_-)$ of $\tilde{\mathfrak{n}}_-$, the map $n_- \mapsto n_-(1)$ being the canonical embedding $\tilde{\mathfrak{n}}_- \hookrightarrow U(\tilde{\mathfrak{n}}_-)$. Hence $n_- = 0$ and we obtain the triangular decomposition of $\tilde{\mathfrak{g}}(A)$, proving a). Moreover, by the Poincaré--Birkhoff--Witt theorem, $\tilde{\mathfrak{n}}_-$ is freely generated by $f_1, \ldots, f_n$. The statement c) is obvious. Now applying $\tilde{\omega}$ we deduce that $\tilde{\mathfrak{n}}_+$ is freely generated by $e_1, \ldots, e_n$, proving b).

    The relations make $e_i, f_i$ weight vectors, $\operatorname{ad} h$ acts diagonally, eigenvectors with distinct eigenvalues are independent. Thus we get the decomposition d). The bound on the weight space dimension comes from the fact that each root space $\tilde{\mathfrak{g}}_\alpha$ is generated by commutators of $\operatorname{ht}(\alpha)$ simple generators. There are at most $n^{\operatorname{ht}(\alpha)}$ such brackets, so $\dim \tilde{\mathfrak{g}}_\alpha \leq n^{\operatorname{ht}(\alpha)}$.

    To prove e), note that for any ideal $i$ of $\tilde{\mathfrak{g}}(A)$ one has (by the proposition to follow)
    \[
        i = \bigoplus_{\alpha} \bigl(\tilde{\mathfrak{g}}_{\alpha} \cap i \bigr).
    \]
    Hence the sum of ideals which intersect $\mathfrak{h}$ trivially, itself intersects $\mathfrak{h}$ trivially, and the sum of all ideals with this property is the unique maximal ideal $\mathfrak{r}$ which intersects $\mathfrak{h}$ trivially. In particular, we obtain that (e) is a direct sum of vector spaces. But, clearly,
    \[
        [f_i, \, \mathfrak{r} \cap \tilde{\mathfrak{n}}_+] \subset \tilde{\mathfrak{n}}_+.
    \]
    Hence
    \[
        [\tilde{\mathfrak{g}}(A), \, \mathfrak{r} \cap \tilde{\mathfrak{n}}_+]
        \subset \mathfrak{r} \cap \tilde{\mathfrak{n}}_+;
    \]
    similarly,
    \[
        [\tilde{\mathfrak{g}}(A), \, \mathfrak{r} \cap \tilde{\mathfrak{n}}_-]
        \subset \mathfrak{r} \cap \tilde{\mathfrak{n}}_-.
    \]
    This shows that (e) is a direct sum of ideals.
\end{proof}

\begin{proposition}
    Let $\mathfrak{h}$ be a commutative Lie algebra, $V$ a diagonalizable
    $\mathfrak{h}$-module, i.e.
    \begin{equation}\label{1.5.1}
        V = \bigoplus_{\lambda \in \mathfrak{h}^*} V_\lambda,
        \qquad
        V_\lambda = \{ v \in V \mid h(v) = \lambda(h)v \ \text{for all } h \in \mathfrak{h} \}.
    \end{equation}
    Then any submodule $U$ of $V$ is graded with respect to the gradation \eqref{1.5.1}.
\end{proposition}

\begin{proof}
    Any $v \in V$ can be written in the form
    \[
        v = \sum_{j=1}^m v_j, \qquad v_j \in V_{\lambda_j},
    \]
    and there exists $h \in \mathfrak{h}$ such that $\lambda_j(h)$
    ($j=1,\dots,m$) are distinct. We have for $v \in U$:
    \[
        h^k(v) = \sum_{j=1}^m \lambda_j(h)^k v_j \in U
        \qquad (k=0,1,\dots,m-1).
    \]
    This is a system of linear equations with a nondegenerate matrix. Hence all $v_j$ lie in $U$. This also shows that the sum in \eqref{1.5.1} is direct because if $v=0$ then all $h^k(v) = 0$ and we can apply the invertible matrix to conclude that all $v_j = 0$.
\end{proof}

Given a complex $n \times n$-matrix $A$, we can now define the main object
of our study: the Lie algebra $\mathfrak{g}(A)$.
\begin{definition}
    [Kac-Moody algebra]
    Let $(\mathfrak{h}, \Pi, \Pi^\vee)$ be a realization of $A$ and let
    $\tilde{\mathfrak{g}}(A)$ be the Lie algebra on generators
    $e_i, f_i \ (i=1,\dots,n)$ and $\mathfrak{h}$, and the defining relations
    (1.2.1). By Theorem \ref{thm:universal-lie-alg} the natural map
    $\mathfrak{h} \to \tilde{\mathfrak{g}}(A)$ is an embedding. Let $\mathfrak{r}$
    be the maximal ideal in $\tilde{\mathfrak{g}}(A)$ which intersects
    $\mathfrak{h}$ trivially. We set:
    \[
        \mathfrak{g}(A) = \tilde{\mathfrak{g}}(A)/\mathfrak{r}.
    \]
    The matrix $A$ is called the \textbf{Cartan matrix} of the Lie algebra $\mathfrak{g}(A)$, and $n$ is called the \textbf{rank} of $\mathfrak{g}(A)$. The Lie algebra $\mathfrak{g}(A)$ whose Cartan matrix is a generalized Cartan matrix is called a \textbf{Kac-Moody algebra}.
\end{definition}

\begin{remark}[Interpreting the maximal ideal which meets the Cartan subalgebra trivially]
    It is true but not obvious that the maximal ideal $\mathfrak{r}$ which meets the Cartan subalgebra $\mathfrak{h}$ trivially is generated by the so-called \textbf{Serre relations}:


    For $i \neq j$,
    \[
        (\mathrm{ad}\, e_i)^{\,1-a_{ij}}(e_j) = 0, \qquad
        (\mathrm{ad}\, f_i)^{\,1-a_{ij}}(f_j) = 0,
    \]
    where $a_{ij}$ are entries of the Cartan matrix.
    These relations are what turn the free Lie algebras $\tilde{\mathfrak{n}}_\pm$ into the correct nilpotent subalgebras.

    The Serre relations can be understood from the representation theory of $\mathfrak{sl}_2$. Inside $\mathfrak{g}(A)$, consider the subalgebra
    \[
        \mathfrak{sl}_2(i) = \langle e_i, f_i, h_i \rangle \cong \mathfrak{sl}_2.
    \]
    For fixed $i$, every other generator $e_j$ or $f_j$ is a weight vector for this copy of $\mathfrak{sl}_2$. The Cartan matrix entry $a_{ij} = \langle \alpha_j, \alpha_i^\vee \rangle$ tells you the weight of $e_j$ relative to $\mathfrak{sl}_2(i)$. Thus, $e_j$ generates an $\mathfrak{sl}_2(i)$-submodule.

    But in an $\mathfrak{sl}_2$-representation, if a vector has weight $m$, then applying $e_i$ more than $m$ times kills it. This is exactly what the Serre relation enforces:
    \[
        (\mathrm{ad}\, e_i)^{1-a_{ij}}(e_j) = 0
    \]
    is the statement that $e_j$ generates an $\mathfrak{sl}_2(i)$-submodule of dimension $(-a_{ij})+1$.
\end{remark}


The quadruple $(\mathfrak{g}(A),\mathfrak{h},\Pi,\Pi^\vee)$ is called the
\textbf{quadruple associated to the matrix $A$}. Two quadruples
$(\mathfrak{g}(A),\mathfrak{h},\Pi,\Pi^\vee)$ and
$(\mathfrak{g}(A_1),\mathfrak{h}_1,\Pi_1,\Pi_1^\vee)$ are called
\textbf{isomorphic} if there exists a Lie algebra isomorphism
$\varphi : \mathfrak{g}(A)\to \mathfrak{g}(A_1)$ such that
$\varphi(\mathfrak{h})=\mathfrak{h}_1$, $\varphi(\Pi^\vee)=\Pi_1^\vee$
and $\varphi^*(\Pi_1)=\Pi$.


We keep the same notation for the images of $e_i,f_i,\mathfrak{h}$ in
$\mathfrak{g}(A)$. The subalgebra $\mathfrak{h}$ of $\mathfrak{g}(A)$ is
called the \textbf{Cartan subalgebra}. The elements $e_i,f_i \ (i=1,\dots,n)$
are called the \textbf{Chevalley generators}. In fact, they generate the
\textbf{derived subalgebra} $\mathfrak{g}'(A)=[\mathfrak{g}(A),\mathfrak{g}(A)]$.
Furthermore,
\[
    \mathfrak{g}(A) = \mathfrak{g}'(A) + \mathfrak{h}
\]
with $\mathfrak{g}(A)=\mathfrak{g}'(A)$ if and only if $\det A \neq 0$.

We set $\mathfrak{h}' = \sum_{i=1}^n \mathbb{C}\alpha_i^\vee$. Then
$\mathfrak{g}'(A)\cap \mathfrak{h} = \mathfrak{h}'$;
$\mathfrak{g}'(A)\cap \mathfrak{g}_\alpha = \mathfrak{g}_\alpha$ if $\alpha\neq 0$.

It follows from (1.2.2) that we have the following \textbf{root space decomposition}
with respect to $\mathfrak{h}$:
\begin{equation}\label{1.3.1}
    \mathfrak{g}(A) = \bigoplus_{\alpha \in Q} \mathfrak{g}_\alpha.
\end{equation}
Here,
\[
    \mathfrak{g}_\alpha = \{ x \in \mathfrak{g}(A) \mid [h,x] = \alpha(h)x
    \ \text{for all } h \in \mathfrak{h}\}
\]
is the \textbf{root space} attached to $\alpha$. Note that
$\mathfrak{g}_0 = \mathfrak{h}$. The number
$\mathrm{mult}\,\alpha := \dim \mathfrak{g}_\alpha$ is called the
\textbf{multiplicity} of $\alpha$. Note that
\begin{equation}\label{1.3.2}
    \mathrm{mult}\,\alpha \leq n^{|\mathrm{ht}\,\alpha|}
\end{equation} by Theorem \ref{thm:universal-lie-alg} d).

An element $\alpha \in Q$ is called a \textbf{root} if $\alpha \neq 0$ and
$\mathrm{mult}\,\alpha \neq 0$. A root $\alpha > 0$ (resp.\ $\alpha < 0$)
is called \textbf{positive} (resp.\ \textbf{negative}). It follows from the root space decomposition that every root is either positive or negative. Denote by $\Delta$, $\Delta_+$
and $\Delta_-$ the sets of all roots, positive and negative roots respectively.
Then
\[
    \Delta = \Delta_+ \,\dot{\cup}\, \Delta_- \qquad \text{(a disjoint union).}
\]

Sometimes we will write $\Delta(A), Q(A), \dots$ in order to emphasize the
dependence on $A$.

Let $\mathfrak{n}_+$ (resp.\ $\mathfrak{n}_-$) denote the subalgebra of
$\mathfrak{g}(A)$ generated by $e_1,\dots,e_n$ (resp.\ $f_1,\dots,f_n$).
By Theorem \ref{thm:universal-lie-alg} e) and the definition of $\mathfrak{g}(A)$, we have the \emph{triangular decomposition}
\[
    \mathfrak{g}(A) = \mathfrak{n}_- \oplus \mathfrak{h} \oplus \mathfrak{n}_+
    \qquad \text{(direct sum of vector spaces).}
\] because the ideal $\mathfrak{r}$ is graded and hence respects the triangular decomposition of $\tilde{\mathfrak{g}}(A)$.

Note that $\mathfrak{g}_\alpha \subset \mathfrak{n}_+$ if $\alpha>0$ and
$\mathfrak{g}_\alpha \subset \mathfrak{n}_-$ if $\alpha<0$. In other words,
for $\alpha>0$ (resp.\ $\alpha<0$), $\mathfrak{g}_\alpha$ is the linear span
of the elements of the form
\[
    [\dots [[e_{i_1},e_{i_2}],e_{i_3}] \dots e_{i_s}]
    \quad (\text{resp.\ } [\dots [[f_{i_1},f_{i_2}],f_{i_3}] \dots f_{i_s}]),
\]
such that $\alpha_{i_1}+\cdots+\alpha_{i_s} = \alpha$
(resp.\ $= -\alpha$). It follows immediately that
\begin{equation}\label{1.3.3}
    \mathfrak{g}_{\alpha_i} = \mathbb{C}e_i, \qquad
    \mathfrak{g}_{-\alpha_i} = \mathbb{C}f_i, \qquad
    \mathfrak{g}_{s\alpha_i} = 0 \quad \text{if } |s|>1.
\end{equation}
because for example the $2\alpha_i$ root space is spanned by $[e_i,e_i] = 0$.

Since every root is either positive or negative, \eqref{1.3.3} implies the
following important fact:

\begin{lemma}{\label{lem:rt-string}}
    If $\beta \in \Delta_+ \setminus \{\alpha_i\}$, then
    $$(\beta + \mathbb{Z}\alpha_i)\cap \Delta \subset \Delta_+$$
\end{lemma}
\begin{proof}
    Suppose $\beta\neq \alpha_i$ is positive, but $\beta - q\alpha_i$ is negative for some $q$. Then the string must pass through $\beta - r\alpha_i = 0$ or $-\alpha_i$ at some step $r \leq q$. But the only multiples of $\alpha_i$ that are roots are $\pm \alpha_i$. So the only way to hit a negative root is if the string actually reaches $-\alpha_i$. If $\beta - r\alpha_i = -\alpha_i$, then $\beta = (r-1)\alpha_i$. But $\beta$ is a root and not equal to $\alpha_i$. The only possible multiples of $\alpha_i$ that are roots are $\pm\alpha_i$. So $\beta = (r-1)\alpha_i$ is impossible unless $\beta=\alpha_i$.
\end{proof}

\begin{remark}[Finiteness of root strings]
    Using the interpretation of the Serre relations from the representation theory of $\mathfrak{sl}_2$, one sees that these root strings are in fact finite. Look at the subalgebra
    $\mathfrak{sl}_2(i) = \langle e_i, f_i, h_i \rangle$. For each root $\beta$, the root space $\mathfrak{g}_\beta$ is a weight space of $\mathfrak{sl}_2(i)$ with weight $\langle \beta, \alpha_i^\vee \rangle$. Acting with $\mathrm{ad}\, e_i$ and $\mathrm{ad}\, f_i$ generates a finite-dimensional $\mathfrak{sl}_2$-module, because the Serre relations
    \[
        (\mathrm{ad}\, e_i)^{1-a_{ij}}(e_j) = 0, \quad
        (\mathrm{ad}\, f_i)^{1-a_{ij}}(f_j) = 0
    \]
    kill sufficiently long strings.

    Thus the $\alpha_i$-string through $\beta$ has finite length.
\end{remark}

\begin{lemma}
    Let $a \in \mathfrak{n}_+$ be such that $[a,f_i] = 0$ for all $i=1,\dots,n$.
    Then $a=0$. Similarly, for $a \in \mathfrak{n}_-$, if $[a,e_i]=0$ for all
    $i=1,\dots,n$, then $a=0$.
\end{lemma}

\begin{proof}
    Let $a \in \mathfrak{n}_+$ be such that $[a,\mathfrak{g}_{-1}(1)] = 0$.
    Then it is easy to see that
    \[
        \sum_{i,j \geq 0} (\operatorname{ad}\, \mathfrak{g}_1(1))^i
        (\operatorname{ad}\, \mathfrak{h})^j a
    \]
    is a subspace of $\mathfrak{n}_+ \subset \mathfrak{g}(A)$, which is invariant
    with respect to $\operatorname{ad}\,\mathfrak{g}_1(1)$,
    $\operatorname{ad}\,\mathfrak{h}$ and
    $\operatorname{ad}\,\mathfrak{g}_{-1}(1)$ (the condition on $a$ is used only
    in the last case). Hence if $a \neq 0$, we obtain a nonzero ideal in
    $\mathfrak{g}(A)$ which intersects $\mathfrak{h}$ trivially. This contradicts
    the definition of $\mathfrak{g}(A)$.
\end{proof}

\begin{remark}
    Sometimes it is useful to consider the Lie algebra $\mathfrak{g}'(A)$ instead
    of $\mathfrak{g}(A)$. Let us give a more direct construction of
    $\mathfrak{g}'(A)$. Denote by $\tilde{\mathfrak{g}}'(A)$ the Lie algebra on
    generators $e_i,f_i,\alpha_i^\vee \ (i=1,\dots,n)$ and defining relations
    \[
        [e_i,f_j] = \delta_{ij}\alpha_i^\vee, \qquad
        [\alpha_i^\vee,\alpha_j^\vee]=0, \qquad
        [\alpha_i^\vee,e_j] = a_{ij}e_j, \qquad
        [\alpha_i^\vee,f_j] = -a_{ij}f_j.
    \]

    Let $Q$ be a free abelian group on generators $\alpha_1,\dots,\alpha_n$.
    Introduce a $Q$-gradation
    \[
        \tilde{\mathfrak{g}}'(A) = \bigoplus_{\alpha} \tilde{\mathfrak{g}}'_\alpha
    \]
    setting
    \[
        \deg e_i = \alpha_i = -\deg f_i, \qquad
        \deg \alpha_i^\vee = 0.
    \]

    There exists a unique maximal $Q$-graded ideal
    $\mathfrak{r} \subset \tilde{\mathfrak{g}}'(A)$ intersecting
    $\tilde{\mathfrak{g}}'_0 \ (= \sum_i \mathbb{C}\alpha_i^\vee)$ trivially.
    Then
    \[
        \mathfrak{g}'(A) = \tilde{\mathfrak{g}}'(A)/\mathfrak{r}.
    \]

    Note that this definition works for an infinite $n$ as well.
\end{remark}

\begin{remark}
    In the presentation of $\mathfrak{g}(A)$, you start with a Cartan subalgebra $\mathfrak{h}$ large enough so that you can realize the simple roots $\alpha_i$ and simple coroots $\alpha_i^\vee$ as linear maps. In general,
    \[
        \dim \mathfrak{h} = 2n - \operatorname{rank}(A).
    \]
    So if $A$ is singular (affine/indefinite type), then $\mathfrak{h}$ strictly contains $\mathfrak{h}' = \mathrm{span}\{\alpha_i^\vee\}$.
    In the presentation of $\mathfrak{g}'(A)$, you only build in the “minimal Cartan” generated by the simple coroots:
    \[
        \mathfrak{h}' = \sum_i \mathbb{C} \alpha_i^\vee.
    \]
    If $A$ is invertible (finite type): then $\mathfrak{h} = \mathfrak{h}'$, so $\mathfrak{g}(A) = \mathfrak{g}'(A)$. If $A$ is singular (e.g. affine type): then $\mathfrak{h}$ has more dimensions than $\mathfrak{h}'$, and these extra directions give rise to central elements and sometimes a degree derivation. In this case, $\mathfrak{g}(A) = \mathfrak{g}'(A) \oplus (\mathfrak{h}/\mathfrak{h}')$.
\end{remark}

\begin{proposition}[Center of a Kac-Moody algebra]
    The center of the Lie algebra $\mathfrak{g}(A)$ or $\mathfrak{g}'(A)$ is equal to
    \[
        \mathfrak{c} := \{ h \in \mathfrak{h} \mid \langle \alpha_i, h \rangle = 0
        \ \text{for all } i=1,\dots,n \}.
    \]
    Furthermore, $\dim \mathfrak{c} = n-\ell$.
\end{proposition}

\begin{proof}
    Let $c$ lie in the center; write $c = \sum_i c_i$ with respect to the principal
    gradation. Then $[c,\mathfrak{g}_{-1}(1)] = 0$ implies
    $[c_i,\mathfrak{g}_{-1}(1)] = 0$ and hence, by Lemma~1.5, $c_i = 0$ for $i>0$.
    Similarly, $c_i=0$ for $i<0$. Hence $c \in \mathfrak{h}$ and
    $[c,e_i] = \langle \alpha_i,c \rangle e_i = 0$ implies that
    $\langle \alpha_i,c \rangle = 0$ ($i=1,\dots,n$). Conversely, if $c \in \mathfrak{h}$
    and the latter condition holds, $c$ commutes with all Chevalley generators and, therefore, lies in the center. The simple roots $\alpha_1,\dots,\alpha_n$ are linear functionals on $\mathfrak{h}$. They span a subspace of $\mathfrak{h}^*$ of dimension $\ell = \mathrm{rank}(A)$. Therefore, the common kernel
    \[
        \{ h \in \mathfrak{h} : \alpha_i(h)=0 \;\;\forall i\}
    \]
    has dimension $n-\ell$.

    Finally, $\mathfrak{c} \subset \mathfrak{h}'$ since in the contrary case, then there would exist some extra element $c \in \mathfrak{h} \setminus \mathfrak{h}'$ that is annihilated by every simple root. That would mean the simple roots $\{\alpha_i\}$ vanish on a larger subspace of $\mathfrak{h}$ than expected, so they would not be linearly independent in $\mathfrak{h}^*$, contradicting the axioms of a realization.
\end{proof}

\subsection{Invariant bilinear form}
\begin{definition}
    A Cartan matrix $A$ is called \textbf{symmetrizable} if there exists a diagonal matrix $D = \mathrm{diag}(d_1,\dots,d_n)$ with positive entries $d_i$ such that $DA$ is symmetric.
\end{definition}

Let $A$ be a symmetrizable matrix with a fixed decomposition and let $(\mathfrak{h}, \Pi, \Pi^\vee)$ be a realization of $A$. Fix a complementary subspace $\mathfrak{h}''$ to $\mathfrak{h}' = \sum \mathbb{C}\alpha_i^\vee$ in $\mathfrak{h}$, and define a symmetric bilinear $\mathbb{C}$-valued form $(\,.\mid.\,)$ on $\mathfrak{h}$ by the following two equations:
\begin{align}\label{form-on-h}
    (\alpha_i^\vee \mid h) & = \langle \alpha_i, h \rangle \epsilon_i, \quad \text{for } h \in \mathfrak{h},\ i = 1,\ldots,n \\
    (h' \mid h'')          & = 0, \quad \text{for } h', h'' \in \mathfrak{h}''
\end{align}
Since $\alpha_1^\vee, \ldots, \alpha_n^\vee$ are linearly independent and since \begin{equation}
    (\alpha_i^\vee \mid \alpha_j^\vee) = b_{ij}\,\epsilon_i\epsilon_j,
    \qquad (i,j=1,\ldots,n)
\end{equation}
there is no ambiguity in the definition of $(\,.\mid.\,)$.

\begin{lemma}\label{lem:bilinear-form-nondeg}
    Let $\mf g(A)$ be the Kac-Moody algebra associated to a symmetrizable matrix $A$. Then the following holds:
    \begin{enumerate}
        \item The kernel of the restriction of the bilinear form $(\,.\mid.\,)$ to $\mathfrak{h}'$
              coincides with $\mathfrak{c}$.
        \item The bilinear form $(\,.\mid.\,)$ is nondegenerate on $\mathfrak{h}$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    (a) follows from Proposition 1.6.
    If now for all $h \in \mathfrak{h}$ we have
    \[
        0 = \Big(\sum_{i=1}^n c_i \alpha_i^\vee \,\Big|\, h\Big)
        = \Big\langle \sum_{i=1}^n c_i \epsilon_i \alpha_i,\, h \Big\rangle,
    \]
    then
    \[
        \sum_{i=1}^n c_i \epsilon_i \alpha_i = 0
    \]
    and hence $c_i = 0$, $i=1,\ldots,n$, proving (b).
\end{proof}

\begin{remark}
    If $A$ is symmetric, you are in the “simply-laced” world (types $A$, $D$, $E$ or untwisted affine). If $A$ is symmetrizable but not symmetric, you are in the “multiply-laced” world (types $B$, $C$, $F$, $G$ or twisted affine).

    Every symmetrizable GCM gives rise to a Kac-Moody algebra that has:
    \begin{itemize}
        \item A symmetric, invariant bilinear form on $\mathfrak{g}$.
        \item A Weyl group that acts as isometries with respect to this form.
        \item A root system with well-behaved reflection geometry.
    \end{itemize}

    If $A$ were not symmetrizable, these structures might not exist at all (the theory gets pathological).

    In fact, there is this tension between being symmetrizable and insisting that the Cartan matrix have $2$s on the diagonal. If you drop the condition that the diagonal entries are $2$, then you could avoid worrying about symmetrizability.
\end{remark}

Since the bilinear form $(\,.\mid.\,)$ is nondegenerate, we have an isomorphism
\[
    \nu : \mathfrak{h} \;\to\; \mathfrak{h}^*
\]
defined by
\[
    \langle \nu(h), h_1 \rangle = (h \mid h_1),
    \qquad h,h_1 \in \mathfrak{h},
\]
and the induced bilinear form $(\,.\mid.\,)$ on $\mathfrak{h}^*$.

We had defined the bilinear form on $\mathfrak{h}$ by
$(\alpha_i^\vee \mid h) = \langle \alpha_i, h \rangle \epsilon_i$ for $h \in \mathfrak{h}$, so rewriting gives
\begin{equation}
    \nu(\alpha_i^\vee) = \epsilon_i \alpha_i,
    \qquad i=1,\ldots,n.
\end{equation}
Now observe that
$(\alpha_i \mid \alpha_j) := (\nu^{-1}(\alpha_i) \mid \nu^{-1}(\alpha_j))$. We know that $\nu(\alpha_i^\vee) = \epsilon_i \alpha_i$, so $\nu^{-1}(\alpha_i) = \tfrac{1}{\epsilon_i}\,\alpha_i^\vee$.

Therefore, \begin{align*}
    (\alpha_i \mid \alpha_j) & = \Big(\tfrac{1}{\epsilon_i}\alpha_i^\vee \;\Big|\; \tfrac{1}{\epsilon_j}\alpha_j^\vee \Big) \\
                             & = \frac{1}{\epsilon_i \epsilon_j} (\alpha_i^\vee \mid \alpha_j^\vee)                         \\
                             & = \frac{1}{\epsilon_i \epsilon_j} (b_{ij} \epsilon_i \epsilon_j)                             \\
                             & = b_{ij}.
\end{align*}
where we invoke equation (8) in the last line.


\begin{theorem}[Invariant bilinear form on a symmetrizable Kac-Moody algebra]\label{thm:invariant-bilinear-form}
    Let $\mathfrak{g}(A)$ be a symmetrizable Lie algebra. Since $A$ is symmetrizable, fix a symmetrization $A = DB$ as above. Then there exists a nondegenerate symmetric bilinear $\mathbb{C}$-valued form
    $(\,.\mid.\,)$ on $\mathfrak{g}(A)$ such that:
    \begin{enumerate}[label=\alph*)]
        \item $(\,.\mid.\,)$ is invariant, i.e.
              \[
                  ([x,y]\mid z) = (x \mid [y,z])
                  \qquad \text{for all } x,y,z \in \mathfrak{g}(A).
              \]
        \item $(\,.\mid.\,)|_{\mathfrak{h}}$ is nondegenerate and defined by
              \begin{align*}
                  (\alpha_i^\vee \mid h) & = \langle \alpha_i, h \rangle \epsilon_i, \quad \text{for } h \in \mathfrak{h},\ i = 1,\ldots,n \\
                  (h' \mid h'')          & = 0, \quad \text{for } h', h'' \in \mathfrak{h}''
              \end{align*}
        \item $(\mathfrak{g}_\alpha \mid \mathfrak{g}_\beta) = 0
                  \quad \text{if } \alpha+\beta \neq 0$.
        \item $(\,.\mid.\,)|_{\mathfrak{g}_\alpha \oplus \mathfrak{g}_{-\alpha}}$
              is nondegenerate for $\alpha \neq 0$, and hence
              $\mathfrak{g}_\alpha$ and $\mathfrak{g}_{-\alpha}$ are
              nondegenerately paired by $(\,.\mid.\,)$.
        \item $[x,y] = (x \mid y)\,\nu^{-1}(\alpha)$
              for $x \in \mathfrak{g}_\alpha$, $y \in \mathfrak{g}_{-\alpha}$,
              $\alpha \in \Delta$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Consider the principal $\mathbb{Z}$-gradation
    \[
        \mathfrak{g}(A) = \bigoplus_{j \in \mathbb{Z}} \mathfrak{g}_j,
        \qquad
        \mathfrak{g}(N) = \bigoplus_{j=-N}^N \mathfrak{g}_j
        \quad \text{for } N=0,1,\ldots
    \] where $\mf g_j$ is the subspace of roots of height $j$.

    Define a bilinear symmetric form $(\,.\mid.\,)$ on $\mathfrak{g}(0) = \mathfrak{h}$ by (2.1.2) and (2.1.3)
    and extend it to $\mathfrak{g}(1)$ by
    \begin{align}
        (e_i \mid f_j)                             & = \delta_{ij} \epsilon_i \qquad (i,j=1,\ldots,n), \\
        (\mathfrak{g}_0 \mid \mathfrak{g}_{\pm 1}) & = 0, \qquad
        (\mathfrak{g}_{\pm 1} \mid \mathfrak{g}_{\pm 1}) = 0.
    \end{align}

    Then the form $(\,.\mid.\,)$ on $\mathfrak{g}(1)$ satisfies invariance as long as both $[x,y]$ and $[y,z]$ lie in $\mathfrak{g}(1)$. Indeed every bracket between $e_i$, $f_j$ and $h$ remains in $\mathfrak{g}(1)$ and the only nontrivial check is
    \[
        ([e_i,f_j]\mid h) = (e_i \mid [f_j,h]) \quad \text{for } h \in \mathfrak{h},
    \]
    or, equivalently,
    \[
        \delta_{ij}(\alpha_i^\vee \mid h) = \delta_{ij}\epsilon_i \langle \alpha_j,h \rangle,
    \]
    which is indeed true.

    Now we extend $(\,.\mid.\,)$ to a bilinear form on the space $\mathfrak{g}(N)$ by induction on $N\geq 1$
    so that $(\mathfrak{g}_i \mid \mathfrak{g}_j)=0$ if $|i|,|j|\leq N$ and $i+j\neq 0$, and also condition a)
    is satisfied as long as both $[x,y]$ and $[y,z]$ lie in $\mathfrak{g}(N)$. Suppose this is already defined on $\mathfrak{g}(N-1)$;
    then we have only to define $(x\mid y)$ for $x \in \mathfrak{g}_{\pm N}, y \in \mathfrak{g}_{\mp N}$.
    We can write $y = \sum_i [u_i,v_i]$, where $u_i$ and $v_i$ are homogeneous elements of nonzero degree which lie in $\mathfrak{g}(N-1)$.
    Then $[x,u_i] \in \mathfrak{g}(N-1)$ and we set
    \[
        (x \mid y) = \sum_i ([x,u_i] \mid v_i).
    \]





    To show that this is well defined, we prove that if $i,j,s,t \in \mathbb{Z}$ are such that
    $|i|+|j|=|s|+|t|=N$, $i+j+s+t=0$, $|i|,|j|,|s|,|t|<N$ and $x_i \in \mathfrak{g}_i$,
    $x_j \in \mathfrak{g}_j$, $x_s \in \mathfrak{g}_s$, $x_t \in \mathfrak{g}_t$, then we have (on $\mathfrak{g}(N-1)$)
    \[
        ([[x_i,x_j],x_s]\mid x_t) = (x_i \mid [[x_j,x_s],x_t]).
    \]

    To explain why this is what we need to check, fix $x\in \mathfrak g_{\pm N}$. Define a bilinear map
    \[
        \beta_x:\;\bigoplus_{p+q=\mp N}\; \mathfrak g_p\otimes \mathfrak g_q \;\longrightarrow\; \mathbb C,\qquad
        \beta_x(u\otimes v):=([x,u]\mid v),
    \]
    where $u,v$ are homogeneous, $|p|,|q|<N$.

    There is a bracket map
    \[
        L:\;\bigoplus_{p+q=\mp N}\; \mathfrak g_p\otimes \mathfrak g_q \;\longrightarrow\; \mathfrak g_{\mp N},\qquad
        L(u\otimes v)=[u,v].
    \]

    Our definition says $(x\mid \cdot)$ on $\mathfrak g_{\mp N}$ should be the linear functional that satisfies
    \[
        (x\mid [u,v])=\beta_x(u\otimes v).
    \]
    This is well defined iff $\beta_x$ vanishes on $\ker L$; i.e.\ $\beta_x$ depends only on $[u,v]$, not on the particular decomposition. In particular, a choice of decomposition $y=\sum [u_i,v_i]$ corresponds to choosing a preimage of $y$ in $V$. If $\tilde y_1,\tilde y_2$ are two different preimages of the same $y$, then their difference lies in the kernel of $L$: $\tilde y_1-\tilde y_2\in \ker L$.
    So we need to show that $\beta_x$ vanishes on $\ker L$. The kernel of $L$ is generated by elements of two types:
    \begin{itemize}
        \item $u\otimes v + v\otimes u$ (skew-symmetry)
        \item $[u,v]\otimes w + [v,w]\otimes u + [w,u]\otimes v$ (Jacobi)
    \end{itemize}


    We get skew symmetry from the invariance of the form on $\mathfrak g(N-1)$:
    \begin{align*}
        \beta_x(u\otimes v)+\beta_x(v\otimes u)
         & =([x,u]\mid v)+([x,v]\mid u)    \\
         & =(x\mid [u,v])+(x\mid [v,u])=0,
    \end{align*}
    using invariance of the form on $\mathfrak g(N-1)$ (true by induction) and $[v,u]=-[u,v]$. So $\beta_x$ vanishes on $u\otimes v+v\otimes u$.

    To check Jacobi, consider homogeneous $x_i\in \mathfrak g_i$, $x_j\in \mathfrak g_j$, $x_s\in \mathfrak g_s$, $x_t\in \mathfrak g_t$ with
    $|i|+|j|=|s|+|t|=N$, $i+j+s+t=0$, and $|i|,|j|,|s|,|t|<N$.
    The identity quoted in the text,
    \[
        ([[x_i,x_j],x_s]\mid x_t)=(x_i\mid [[x_j,x_s],x_t]),
    \]
    implies that $\beta_{x_i}$ kills the Jacobi generator:
    \[
        \beta_{x_i}([x_j,x_s]\otimes x_t)+\beta_{x_i}([x_s,x_t]\otimes x_j)+\beta_{x_i}([x_t,x_j]\otimes x_s)=0.
    \]

    Indeed, if we had the identity, then we would have
    \begin{align*}
        \beta_{x_i}([x_j,x_s]\otimes x_t)
         & =([x_i,[x_j,x_s]]\mid x_t)
        =(x_i\mid [[x_j,x_s],x_t]),   \\
        \beta_{x_i}([x_s,x_t]\otimes x_j)
         & =([x_i,[x_s,x_t]]\mid x_j)
        =(x_i\mid [[x_s,x_t],x_j]),   \\
        \beta_{x_i}([x_t,x_j]\otimes x_s)
         & =([x_i,[x_t,x_j]]\mid x_s)
        =(x_i\mid [[x_t,x_j],x_s]).
    \end{align*}
    and adding these three equations gives
    \[
        \beta_{x_i}(J)
        =(x_i\mid \,[[x_j,x_s],x_t]+[[x_s,x_t],x_j]+[[x_t,x_j],x_s]\,)
        = (x_i\mid 0)=0,
    \] Thus $\beta_x$ vanishes on the Jacobi-type tensors.

    Now we check the identity using the invariance of $(\,.\mid.\,)$ on $\mathfrak{g}(N-1)$ and the Lie algebra axioms, we have
    \begin{align*}
        ([[x_i,x_j],x_s]\mid x_t)
         & = (([x_i,x_j],x_s] \mid x_t) - ([ [x_j,x_s],x_i]\mid x_t) \\
         & = ([x_i,x_j]\mid [x_s,x_t]) + (x_i \mid [[x_j,x_s],x_t])  \\
         & = (x_i \mid [x_s,[x_j,x_t]]) + ([x_j,x_s]\mid [x_i,x_t])  \\
         & = (x_i \mid [[x_j,x_s],x_t]),
    \end{align*}
    as required. So the identity holds, and hence $\beta_x$ vanishes on $\ker L$. This shows that $(x\mid y)$ is well defined.

    If now $x=\sum_i [u_i',v_i']$, then by definition and by the relation above we have
    \[
        (x \mid y) = \sum_i ([x,u_i]\mid v_i)
        = \sum_i (u_i' \mid [v_i',y]).
    \]
    Hence this is independent of the choice of the expressions for $x$ and $y$.

    It is clear from the definition that a) holds on $\mathfrak{g}(N)$ whenever $[x,y]$ and $[y,z]$
    lie in $\mathfrak{g}(N)$. Hence we have constructed a bilinear form $(\,.\mid.\,)$ on $\mathfrak{g}$
    such that a) and b) hold. Its restriction to $\mathfrak{h}$ is nondegenerate by Lemma \ref{lem:bilinear-form-nondeg} b).

    The form $(\,.\mid.\,)$ satisfies c) since for $h \in \mathfrak{h}$, $x \in \mathfrak{g}_\alpha$ and $y \in \mathfrak{g}_\beta$ we have, by the invariance property:
    \[
        0 = ([h,x]\mid y) + (x \mid [h,y])
        = (\langle \alpha,h\rangle + \langle \beta,h\rangle)(x \mid y).
    \]

    For $x \in \mathfrak{g}_\alpha$, $y \in \mathfrak{g}_{-\alpha}$ where $\alpha \in \Delta$, and $h \in \mathfrak{h}$, we have
    \[
        ([x,y] - (x \mid y)\nu^{-1}(\alpha) \mid h)
        = (x \mid [y,h]) - (x \mid y)\langle \alpha,h\rangle = 0.
    \]
    which combined with b) gives e).

    It follows from b), c) and e) that the bilinear form $(\,.\mid.\,)$ is symmetric. If d) fails, then by c) the form $(\,.\mid.\,)$ is degenerate. Let $\mathfrak{i} = \ker(\,.\mid.\,)$ is an ideal by invariance, and by b) we have $\mathfrak{i}\cap \mathfrak{h}=0$, which contradicts the definition of $\mathfrak{g}(A)$. Therefore d) holds as well.
\end{proof}

\begin{remark}[Uniqueness of the invariant bilinear form]
    Such a form is uniquely determined once you ask for a) and b).

    First you can deduce that $\mf g_\alpha$ and $\mf g_\beta$ are orthogonal unless $\alpha+\beta=0$.
    Take $x \in \mathfrak{g}_\alpha$, $y \in \mathfrak{g}_\beta$, and $h \in \mathfrak{h}$. Recall the weight-space relation:
    $[h, x] = \alpha(h)x$, $[h,y] = \beta(h)y$. By invariance,
    \[
        ([h,x]\mid y)=(h\mid [x,y]).
    \]
    Since $[h,x]=\alpha(h)x$ and $[x,y]\in\mathfrak{g}_{\alpha+\beta}$, we obtain
    \begin{equation}\label{eq:basic}
        \alpha(h)\,(x\mid y)=(h\mid z),\qquad z:=[x,y]\in\mathfrak{g}_{\alpha+\beta}.
    \end{equation}
    Now take $h'\in\mathfrak{h}$. Using invariance again and $[\mathfrak{h},\mathfrak{h}]=0$,
    \[
        0=([h',h]\mid z)=(h'\mid [h,z])=(\alpha+\beta)(h)\,(h'\mid z)\quad\text{for all }h,h'\in\mathfrak{h}.
    \]
    If $\alpha+\beta\neq 0$, choose $h$ with $(\alpha+\beta)(h)\neq 0$; then $(h'\mid z)=0$ for all
    $h'\in\mathfrak{h}$, so in particular $(h\mid z)=0$ for every $h\in\mathfrak{h}$. Returning to
    \eqref{eq:basic}, we get $\alpha(h)(x\mid y)=0$ for all $h\in\mathfrak{h}$; choosing $h$ with
    $\alpha(h)\neq 0$ (since $\alpha\neq 0$ on $\mathfrak{g}_\alpha$) yields $(x\mid y)=0$.
    Hence $(\mathfrak{g}_\alpha\mid\mathfrak{g}_\beta)=0$ unless $\alpha+\beta=0$.


    Now, take $h \in \mathfrak{h}$. Invariance gives $(h \mid [e_i, f_i]) = ([h, e_i] \mid f_i)$.
    On the LHS: $[e_i, f_i] = \alpha_i^\vee$, so $(h \mid [e_i, f_i]) = (h \mid \alpha_i^\vee)$. On the RHS: $[h, e_i] = \langle \alpha_i, h \rangle e_i$, so
    $([h, e_i] \mid f_i) = \langle \alpha_i, h \rangle (e_i \mid f_i)$.

    Thus $(h \mid \alpha_i^\vee) = \langle \alpha_i, h \rangle (e_i \mid f_i)$ which determines how $e_i$ and $f_i$ pair against each other. Using invariance and inducting on height, you can determine how any two root vectors pair against each other.
\end{remark}

Suppose that $A = (a_{ij})$ is a symmetrizable generalized Cartan
matrix. Fix a decomposition
\[
    A = \operatorname{diag}(\epsilon_1,\dots,\epsilon_n)(b_{ij})_{i,j=1}^n
\]
where $\epsilon_i$ are \textbf{positive} rational numbers and $(b_{ij})$ is a
symmetric rational matrix.

\begin{lemma}
    Such a decomposition always exists.
\end{lemma}
\begin{proof}
    This is equivalent to a system of homogeneous linear equations
    and inequalities over $\mathbb{Q}$ with unknowns $\epsilon_i^{-1}$ and $b_{ij}$:
    \[
        \epsilon_i^{-1}\neq 0, \qquad
        \operatorname{diag}(\epsilon_1^{-1},\dots,\epsilon_n^{-1})A = (b_{ij}),
        \qquad b_{ij}=b_{ji}.
    \]
    By definition, it has a solution over $\mathbb{C}$. Hence, it has a
    solution over $\mathbb{Q}$. We can assume that $A$ is indecomposable, meaning that $A$ is not a direct sum of two smaller Cartan matrices. Then for any $1<j\leq n$ there exists a sequence
    \[
        1=i_1<i_2<\cdots<i_{k-1}<i_k=j
    \]
    such that $a_{i_s,i_{s+1}}<0$. We have:
    \[
        a_{i_s,i_{s+1}}\epsilon_{i_{s+1}}
        = a_{i_{s+1},i_s}\epsilon_{i_s}\quad (s=1,\dots,k-1).
    \]
    Hence $\epsilon_j\epsilon_1>0$ for all $j$ because $a_{i_s,i_{s+1}}$ and $a_{i_{s+1},i_s}$ have the same sign.
\end{proof}

\begin{remark}
    [Dynkin diagram interpretation] Given a Cartan matrix, put an edge between two indices if $a_{ij}\neq 0$. Then $A$ is indecomposable iff the graph is connected. Then it is obvious every index $j$ has a path to $1$, and the above argument shows that all $\epsilon_j$ have the same sign as $\epsilon_1$. We can then scale all $\epsilon_j$ by a constant to make them all positive.
\end{remark}
From this we also deduce that if $A$ is indecomposable, then the matrix $\operatorname{diag}(\epsilon_1,\dots,\epsilon_n)$ is uniquely determined up to a constant factor.

Fix a nondegenerate bilinear symmetric form $(\,.\mid.\,)$ associated to
the decomposition above as defined above. Recall that \begin{align*}
    (\alpha_i\mid \alpha_j) = b_{ij} = \frac{a_{ij}}{\epsilon_i}.
\end{align*}
so $i=j$ gives $(\alpha_i\mid \alpha_i) = 2\epsilon_i^{-1} > 0$. If $i\neq j$, then $a_{ij}\leq 0$ and hence $(\alpha_i\mid \alpha_j)\leq 0$. By definition of $\nu$ (the identification $\mathfrak{h}\to\mathfrak{h}^*$ using the form), we had
$\nu(\alpha_i^\vee)=\epsilon_i\alpha_i$.
Applying $\nu^{-1}$,
$\alpha_i^\vee = \frac{1}{\epsilon_i}\nu^{-1}(\alpha_i)$.
But we also know
$\epsilon_i=(\alpha_i\mid \alpha_i)/2$.

We deduce that:
\[
    (\alpha_i\mid \alpha_i)>0 \quad (i=1,\dots,n),
\]
\[
    (\alpha_i\mid \alpha_j)\leq 0 \quad (i\neq j),
\]
\[
    \alpha_i^\vee = \frac{2}{(\alpha_i\mid \alpha_i)}\,\nu^{-1}(\alpha_i).
\]
Hence we obtain the usual expression for the generalized Cartan matrix:
\[
    A=\left(\frac{2(\alpha_i\mid \alpha_j)}{(\alpha_i\mid \alpha_i)}\right)_{i,j=1}^n.
\]

\begin{remark}[Two pairings between $\mf h$ and $\mf h^*$]
    We have two natural pairings between $\mathfrak{h}$ and $\mathfrak{h}^*$:
    \begin{itemize}
        \item The canonical pairing $\langle \,.\,,\,.\,\rangle$.
        \item The pairing induced by the bilinear form $(\,.\mid.\,)$
        \end{itemize}
    The $\nu$ map identifies these two pairings. In particular, we have
    \[
        \langle \alpha_j, \alpha_i^\vee \rangle = (\nu
        (\alpha_i^\vee) \mid \alpha_j) = \epsilon_i (\alpha_i \mid \alpha_j).
    \] essentially because we defined the invariant pairing on $\mathfrak{h}$ by
$(\alpha_i^\vee \mid h) = \langle \alpha_i, h \rangle \epsilon_i$ for $h \in \mathfrak{h}$, so rewriting gives
\begin{align*}
    \nu(\alpha_i^\vee) = \epsilon_i \alpha_i,
    \qquad i=1,\ldots,n.
\end{align*}
    They are related by
    \[
        a_{ij} = \langle \alpha_i, \alpha_j^\vee \rangle = \frac{2(\alpha_i\mid \alpha_j)}{(\alpha_i\mid \alpha_i)}.
    \]
\end{remark}

We extend the bilinear form $(\,.\mid.\,)$ from $\mathfrak h$ to an
invariant symmetric bilinear form on the entire Kac–Moody algebra
$\mathfrak g(A)$. By Theorem \ref{thm:invariant-bilinear-form} such a form exists and satisfies all the properties stated there. It is an exercise to show that such a form is unique. The bilinear form $(\,.\mid.\,)$ on the Kac-Moody algebra $\mathfrak g(A)$ provided by Theorem \ref{thm:invariant-bilinear-form} and
satisfying the above is called a \textbf{standard invariant form}.

\subsection{Generalized Casimir operator}
Let $\mathfrak{g}(A)$ be a Lie algebra associated to a matrix $A$,
$\mathfrak{h}$ the Cartan subalgebra,
$\mathfrak{g} = \bigoplus_\alpha \mathfrak{g}_\alpha$ the root space decomposition with respect to $\mathfrak{h}$.
A $\mathfrak{g}(A)$-module (resp.\ $\mathfrak{g}'(A)$-module) $V$ is called \textbf{restricted} if for every $v\in V$,
we have $\mathfrak{g}_\alpha(v)=0$ for all but a finite number of positive roots $\alpha$.

It is clear that every submodule or quotient of a restricted module is restricted, and that the direct sum or tensor product of a finite number of restricted modules is also restricted. Examples of restricted modules will be constructed later (see Exercise 2.9 and Chapter 9).

Assume now that $A$ is symmetrizable and that $(\,.\mid.\,)$ is a bilinear form provided by Theorem \ref{thm:invariant-bilinear-form}.

Given a restricted $\mathfrak{g}(A)$-module $V$, we introduce a linear operator $\Omega$ on the vector space $V$, called the (generalized) \textbf{Casimir operator}, as follows.

First, introduce a linear function $\rho \in \mathfrak{h}^*$ by equations
\[
    \langle \rho,\alpha_i^\vee \rangle = \tfrac{1}{2}a_{ii} \quad (i=1,\dots,n).
\]
If $\det A=0$, this does not define $\rho$ uniquely, and we pick any solution.
It follows from (2.1.5) and (2.1.6) that
\begin{equation}\label{eq:rho-action}
    (\rho\mid \alpha_i) = \tfrac{1}{2}(\alpha_i\mid \alpha_i), \qquad (i=1,\dots,n).
\end{equation}

Further, for each positive root $\alpha$ we choose a basis $\{e_\alpha^{(i)}\}$ of the space $\mathfrak{g}_\alpha$,
and let $\{e_{-\alpha}^{(i)}\}$ be the dual basis of $\mathfrak{g}_{-\alpha}$. Note that the root spaces need not be $1$-dimensional. We define an operator $\Omega_0$ on $V$ by
\[
    \Omega_0 = 2 \sum_{\alpha\in \Delta_+}\;\sum_i e_{-\alpha}^{(i)}e_\alpha^{(i)}.
\]
One could easily check that this is independent of the choice of bases.
Since for each $v\in V$, only a finite number of summands $e_{-\alpha}^{(i)}e_\alpha^{(i)}(v)$ are nonzero, $\Omega_0$ is well defined on $V$.

Let $u_1,u_2,\dots$ and $u^1,u^2,\dots$ be dual bases of $\mathfrak{h}$.
The generalized Casimir operator is defined by
\[
    \Omega = 2\nu^{-1}(\rho) + \sum_i u^i u_i + \Omega_0.
\]

\begin{remark}
    The generalized Casimir operator $\Omega$ was introduced by Kac. The idea of its definition is borrowed from physics. We take the usual definition of the Casimir operator:
    \[
        \Omega = \sum_{\alpha>0}\;\sum_i
        \Bigl(e_{-\alpha}^{(i)}e_\alpha^{(i)} + e_\alpha^{(i)}e_{-\alpha}^{(i)}\Bigr)
        + \sum_i u_i u^i,
    \]
    we rewrite it by using commutation relations:
    \[
        \Omega = \sum_{\alpha>0} \nu^{-1}(\alpha)
        + 2\sum_{\alpha>0}\;\sum_i e_{-\alpha}^{(i)}e_\alpha^{(i)}
        + \sum_i u_i u^i,
    \]
    and then replace the first summand, which makes no sense, by a finite   quantity $2\nu^{-1}(\rho)$.
\end{remark}

We record the following simple formula:
\begin{equation}\label{eq:bilinear-dual}
    \sum_i \langle \lambda,u^i \rangle \langle \mu,u_i \rangle = (\lambda\mid \mu),
\end{equation}
which is clear from
\[
    \lambda = \sum_i \langle \lambda,u^i\rangle \nu(u_i)
    = \sum_i \langle \lambda,u_i\rangle \nu(u^i).
\]
We make one more simple computation.
For $x\in \mathfrak{g}_\alpha$ one has
\[
    \Bigl[\sum_i u^i u_i,\, x\Bigr]
    = \sum_i \langle \alpha,u^i\rangle x u_i + \sum_i u^i \langle \alpha,u_i\rangle x
    = \sum_i \langle \alpha,u^i\rangle \langle \alpha,u_i\rangle x
    + x\Bigl(\sum_i u^i \langle \alpha,u_i\rangle + u_i \langle \alpha,u^i\rangle\Bigr).
\]
Hence, we have
\begin{align}\label{eq:commutator-Cartan}
    \Bigl[\sum_i u^i u_i,\, x\Bigr] = x\bigl((\alpha\mid \alpha) + 2\nu^{-1}(\alpha)\bigr),
    \qquad x\in \mathfrak{g}_\alpha.
\end{align}



\begin{lemma}[Important calculation]\label{lem:important-calculation}
    If $\alpha,\beta \in \Delta$ and $z \in \mathfrak{g}_{\beta-\alpha}$, then in $\mathfrak{g}(A)\otimes \mathfrak{g}(A)$ we have
    \[
        \sum_i e_{-\alpha}^{(i)} \otimes [z,e_\alpha^{(i)}]
        = \sum_i [e_{-\beta}^{(i)},z] \otimes e_\beta^{(i)}.
    \]
\end{lemma}

\begin{proof}
    We define the bilinear form $(\,.\mid .\,)$ on $\mathfrak{g}(A)\otimes \mathfrak{g}(A)$ by
    \[
        (x\otimes y \mid x_1\otimes y_1) = (x\mid x_1)(y\mid y_1).
    \]
    Pick $e \in \mathfrak{g}_\alpha$ and $f \in \mathfrak{g}_{-\beta}$.
    It suffices to check that pairing both sides of the above identity with $e \otimes f$ gives the same result. We have:
    \begin{align*}
        \sum_s (e_{-\alpha}^{(s)} \otimes [z,e_\alpha^{(s)}] \mid e\otimes f)
         & = \sum_s (e_{-\alpha}^{(s)} \mid e)\,([z,e_\alpha^{(s)}]\mid f) \\
         & = \sum_s (e_{-\alpha}^{(s)} \mid e)(e_\alpha^{(s)} \mid [f,z])  \\
         & = (e\mid [f,z]),
    \end{align*}
    by invariance and the dual basis property. Similarly,
    \begin{align*}
        \sum_s ([e_{-\beta}^{(s)},z]\otimes e_\beta^{(s)} \mid e\otimes f)
         & = \sum_s (e_{-\beta}^{(s)} \mid [z,e])(e_\beta^{(s)}\mid f) \\
         & = ([z,e]\mid f).
    \end{align*}
    Applying again invariance gives the result.
\end{proof}

\begin{corollary}
    In the notation of the lemma, we have
    \[
        \sum_i [e_{-\alpha}^{(i)},[z,e_\alpha^{(i)}]]
        = -\sum_i [[z,e_{-\beta}^{(i)}],e_\beta^{(i)}] \quad \text{in } \mathfrak{g}(A),
    \]
    and
    \[
        \sum_i e_{-\alpha}^{(i)}[z,e_\alpha^{(i)}]
        = -\sum_i [z,e_{-\beta}^{(i)}]\, e_\beta^{(i)} \quad \text{in } U(\mathfrak{g}(A)).
    \]
\end{corollary}

\begin{proof}
    Apply to the lemma the linear maps from $\mathfrak{g}(A)\otimes \mathfrak{g}(A)$ to $\mathfrak{g}(A)$ and to $U(\mathfrak{g}(A))$, defined by
    \[
        x\otimes y \mapsto [x,y],
        \qquad
        x\otimes y \mapsto xy,
    \]
    respectively.
\end{proof}

Consider the root space decomposition of $U(\mathfrak{g}(A))$ with respect to $\mathfrak{h}$:
\[
    U(\mathfrak{g}(A)) = \bigoplus_{\beta \in Q} U_\beta, \qquad
    U_\beta = \{ x \in U(\mathfrak{g}(A)) \mid [h,x] = \langle \beta,h\rangle x \;\;\text{for all } h \in \mathfrak{h}\}.
\]
Recall that $\mf g'(A) = [\mf g(A),\mf g(A)]$ is the derived algebra of $\mf g(A)$. Put $U'_\beta = U(\mathfrak{g}'(A)) \cap U_\beta$, so that
\[
    U(\mathfrak{g}'(A)) = \bigoplus_\beta U'_\beta.
\]

\begin{theorem}[\label{thm:Casimir-commutes}]
    Let $\mathfrak{g}(A)$ be a symmetrizable Lie algebra.
    \begin{enumerate}[label=\alph*)]
        \item If $V$ is a restricted $\mathfrak{g}'(A)$-module and $u \in U'_\alpha$, then
              \[
                  [\Omega_0,u] = -u\Big( 2(\rho|\alpha) + (\alpha|\alpha) + 2\nu^{-1}(\alpha)\Big).
              \]
        \item If $V$ is a restricted $\mathfrak{g}(A)$-module, then $\Omega$ commutes with the action of $\mathfrak{g}(A)$ on $V$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Part b) follows immediately from a) and the earlier formula \ref{eq:commutator-Cartan}. Note that we only have to check the formula for the derived algebra $\mathfrak{g}'(A)$ since \begin{align*}
        \mf g(A) = \mf g'(A) \oplus \mf h'',
    \end{align*} where $\mf h''$ is the orthogonal complement of the subspace generated by the coroots $\alpha_i^\vee$ in $\mf h$, and in particular $[\mf h'',\mf g(A)]=0$, i.e.\ $\mf h''$ is central.
    The following computation shows that (a) implies (b): Let $u \in U'_\alpha$. Then
    \begin{align*}
        [\Omega_0,u]                & = -u\big(2(\rho \mid \alpha) + (\alpha \mid \alpha) + 2\nu^{-1}(\alpha)\big) \\
        \Big[\sum_i u^i u_i, u\Big] & = u\big((\alpha \mid \alpha) + 2\nu^{-1}(\alpha)\big)                        \\
        [2\nu^{-1}(\rho),u]         & = 2(\rho \mid \alpha)u \quad \text{for } h \in \mathfrak{h}
    \end{align*}
    If a) holds for $u \in U'_\alpha$ and $u_1 \in U'_\beta$, then it holds for $uu_1 \in U'_{\alpha+\beta}$. Indeed:
    \begin{align*}
        [\Omega_0,uu_1] & = [\Omega_0,u]u_1 + u[\Omega_0,u_1]                                                            \\
                        & = -u\big(2(\rho|\alpha) + (\alpha|\alpha) + 2\nu^{-1}(\alpha)\big)u_1                          \\
                        & \quad -uu_1\big(2(\rho|\beta) + (\beta|\beta) + 2\nu^{-1}(\beta)\big)                          \\
                        & = -uu_1\big(2(\rho|\alpha) + (\alpha|\alpha) + 2\nu^{-1}(\alpha)\big)                          \\
                        & \quad + 2(\alpha|\beta)u u_1 + 2(\rho|\beta)uu_1 + (\beta|\beta)uu_1 + 2\nu^{-1}(\beta)uu_1    \\
                        & = -uu_1\big(2(\rho|\alpha+\beta) + (\alpha+\beta|\alpha+\beta) + 2\nu^{-1}(\alpha+\beta)\big).
    \end{align*}

    Hence, since $e_{\alpha_i}, e_{-\alpha_i}$ ($i=1,\dots,n$) generate $\mathfrak{g}'(A)$, it suffices to check the formula for $u = e_{\alpha_i}$ or $u = e_{-\alpha_i}$. Applying \ref{lem:important-calculation} to $z = e_{\alpha_i}$, we have:
    \begin{align*}
        [\Omega_0,e_{\alpha_i}]
         & = 2 \sum_{\alpha \in \Delta_+} \sum_s \big([e_{-\alpha}^{(s)},e_{\alpha_i}]e_\alpha^{(s)} + e_{-\alpha}^{(s)}[e_\alpha^{(s)},e_{\alpha_i}]\big)                                                  \\
         & = 2[e_{-\alpha_i},e_{\alpha_i}]e_{\alpha_i}
        + 2 \sum_{\alpha \in \Delta_+ \setminus \{\alpha_i\}} \bigg(\sum_s [e_{-\alpha}^{(s)},e_{\alpha_i}]e_\alpha^{(s)} + \sum_s e_{-\alpha_i+\alpha}^{(s)}[e_{\alpha-\alpha_i}^{(s)},e_{\alpha_i}]\bigg) \\
         & = -2\nu^{-1}(\alpha_i)e_{\alpha_i}
        = -2(\alpha_i|\alpha_i)e_{\alpha_i} - 2e_{\alpha_i}\nu^{-1}(\alpha_i).
    \end{align*}
    where the second equality follows from reindexing the second summand (note that we have \ref{lem:rt-string}) and the third equality follows from \ref{lem:important-calculation}.
    Thanks to \ref{eq:rho-action} this is exactly the formula in part a) for $u = e_{\alpha_i}$. Similarly,
    \[
        [\Omega_0,e_{-\alpha_i}] = 2e_{-\alpha_i}[e_{\alpha_i},e_{-\alpha_i}]
        = 2e_{-\alpha_i}\nu^{-1}(\alpha_i),
    \]
    which, by \ref{eq:rho-action}, is the same formula for $u = e_{-\alpha_i}$.
\end{proof}

\begin{corollary}[Eigenvalue on highest weight modules]\label{cor:Casimir-eigenvalue}
    If, under the hypotheses of Theorem \ref{thm:Casimir-commutes} b), there exists $v \in V$ such that
    $e_i(v)=0$ for all $i=1,\dots,n$, and $h(v)=\langle \Lambda,h\rangle v$ for some $\Lambda \in \mathfrak{h}^*$ and all $h \in \mathfrak{h}$, then
    \[
        \Omega(v) = (\Lambda+2\rho|\Lambda)v.
    \]
    If, furthermore, $U(\mathfrak{g}(A))v=V$, then
    \[
        \Omega = (\Lambda+2\rho|\Lambda)I_V.
    \]
\end{corollary}
\begin{proof}
    Recall that
    \[
        \Omega \;=\; 2\,\nu^{-1}(\rho)\;+\;\sum_i u^i u_i\;+\;\Omega_0,
        \qquad
        \Omega_0=\sum_{\alpha\in\Delta_+}\sum_s e_{-\alpha}^{(s)}e_{\alpha}^{(s)}.
    \]
    Let $v$ be a highest weight vector, i.e.\ $e_i v=0$ for all $i$ and
    $h\cdot v=\langle\Lambda,h\rangle v$ for all $h\in\mathfrak h$.

    For every $\alpha\in\Delta_+$ we have $e_\alpha^{(s)}v=0$, hence each term in $\Omega_0$ kills $v$, so $\Omega_0 v=0$.

    Since $u_i,u^i\in\mathfrak h$ and $v$ has weight $\Lambda$,
    \[
        u_i v=\langle\Lambda,u_i\rangle v,\qquad
        u^i(u_i v)=\langle\Lambda,u_i\rangle \langle\Lambda,u^i\rangle v.
    \]
    Summing over $i$ and using the duality of $\{u_i\}$ and $\{u^i\}$ for $(\cdot|\cdot)$, we obtain (invoking \ref{eq:bilinear-dual}):
    \[
        \Big(\sum_i u^i u_i\Big)v
        =\sum_i \langle\Lambda,u_i\rangle \langle\Lambda,u^i\rangle v
        =(\Lambda|\Lambda)\,v.
    \]
    Again by the weight action,
    \[
        (2\nu^{-1}(\rho))\cdot v
        =2\langle\Lambda,\nu^{-1}(\rho)\rangle v
        =2(\Lambda|\rho)\,v.
    \]
    This proves the first formula. If moreover $V=U(\mathfrak g(A))v$, then by Theorem~\ref{thm:Casimir-commutes}\,b),
    $\Omega$ commutes with the $\mathfrak g(A)$-action on $V$. Thus for any $x\in U(\mathfrak g(A))$,
    \[
        \Omega(xv)=x\,\Omega v
        =(\Lambda+2\rho|\Lambda)\,xv.
    \]
    Since such $xv$ span $V$, it follows that $\Omega$ acts as the scalar $(\Lambda+2\rho|\Lambda)$ on all of $V$, i.e.
    \[
        \Omega=(\Lambda+2\rho|\Lambda)\,I_V.
    \]
    as desired.
\end{proof}
We introduce another central element defined for $\mathfrak g$-modules $V$ which have a certain finiteness property. \red{What is the relation to the generalized Casimir operator?}
\begin{proposition}
    Let $\mathfrak g$ be a Lie algebra with an invariant non-degenerate bilinear form $(\cdot \mid \cdot)$, let $\{x_i\}$ and $\{y_i\}$ be dual bases (i.e.\ $(x_i \mid y_j) = \delta_{ij}$), and let $V$ be a $\mathfrak g$-module such that for every pair of elements $u,v \in V$, $x_i(u)=0$ or $y_i(v)=0$ for all but a finite number of $i$. Then the operator
    \[
        \Omega_2 := \sum_i x_i \otimes y_i
    \]
    is defined on $V \otimes V$ and commutes with the action of $\mathfrak g$.
\end{proposition}

\begin{proof}
    We have to show that for every $z \in \mathfrak g$,
    \[
        \sum_i \big([z,x_i]\otimes y_i + x_i \otimes [z,y_i]\big) = 0.
    \]

    Write
    \[
        [z,x_i] = \sum_j \alpha_{ij} x_j,\qquad [z,y_i] = \sum_j \beta_{ij} y_j.
    \]
    Taking the inner product of the first equation with $y_j$ and of the second with $x_j$, we obtain
    \[
        \alpha_{ij} = ([z,x_i]\mid y_j),\qquad \beta_{ji} = ([z,y_i]\mid x_j).
    \]

    Using invariance of $(\cdot \mid \cdot)$, we deduce
    \[
        \alpha_{ij} = (z\mid [x_i,y_j]), \qquad \beta_{ji} = (z\mid [y_i,x_j]).
    \]
    Hence
    \[
        \alpha_{ij} = -\beta_{ij}.
    \]
    This proves the proposition.
\end{proof}

To conclude this section, we define the compact form of a Kac-Moody algebra. Let $A$ be an $n\times n$ matrix over $\mathbb{R}$. Let
$(\mathfrak h_\mathbb{R}, \Pi, \Pi^\vee)$ be a realization of the matrix $A$ over $\mathbb{R}$,
i.e.\ $\mathfrak h_\mathbb{R}$ is a real vector space of dimension $2n-\operatorname{rank}(A)$,
so that $(\mathfrak h = \mathbb{C}\otimes_\mathbb{R}\mathfrak h_\mathbb{R}, \Pi, \Pi^\vee)$ is the realization of $A$ over $\mathbb{C}$.

\begin{definition}
    We define the \textbf{compact form} $\mathfrak t(A)$ of $\mathfrak g(A)$ as follows.
    Let $\omega_0$ be the antilinear automorphism of $\mathfrak g(A)$ determined by
    \[
        \omega_0(e_i) = -f_i,\qquad
        \omega_0(f_i) = -e_i \quad (i=1,\dots,n),\qquad
        \omega_0(h) = -h \quad (h\in\mathfrak h_\mathbb{R}).
    \]
    Then $\mathfrak t(A)$ is defined as the fixed point set of $\omega_0$.
    This is a real Lie algebra whose complexification is $\mathfrak g(A)$.
    In the finite-dimensional case, this definition coincides with the usual compact real form.
\end{definition}

\begin{remark}
    Let $(\cdot|\cdot)$ be a symmetric bilinear form on $\mathfrak g(A)$.
    Define a Hermitian form on $\mathfrak g(A)$ by
    \[
        (x|y)_0 := -(\omega_0(x)\mid y).
    \]
    Then $(\cdot|\cdot)_0$ is nondegenerate on each $\mathfrak g_\alpha$, $\alpha\in\Delta\cup\{0\}$,
    and satisfies $(\mathfrak g_\alpha|\mathfrak g_\beta)_0 = 0$ if $\alpha\neq \beta$.
    Moreover, the operators $\operatorname{ad}u$ and $-\operatorname{ad}\,\omega_0(u)$ are adjoint to each other with respect to $(\cdot|\cdot)_0$.
    In particular, the restriction of $(\cdot|\cdot)$ to $\mathfrak t(A)$ is a nondegenerate invariant $\mathbb R$-bilinear form.

    This is the generalization of the familiar fact that a complex semisimple Lie algebra has a compact real form, and the Killing form restricted there is negative definite.
\end{remark}


\section{Integrable modules}
We begin this section by recalling the representation theory of $\mathfrak{sl}_2(\mathbb{C})$. Let
\[
    e = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix},
    \quad
    h = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix},
    \quad
    f = \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}
\]
be the standard basis of $\mathfrak{sl}_2(\C)$. Then
\[
    [e,f] = h,
    \quad [h,e] = 2e,
    \quad [h,f] = -2f.
\]

By an easy induction on $k$ we deduce the following relations in the universal enveloping algebra of $\mathfrak{sl}_2(\C)$:
\begin{align*}
    [h,f^k] & = -2k f^k,                       \\
    [h,e^k] & = 2k e^k                         \\
    [e,f^k] & = -k(k-1) f^{k-1} + k f^{k-1} h.
\end{align*}

\begin{lemma}[Classification of irreducible $\mathfrak{sl}_2(\C)$-modules]\label{lem:sl2-modules}
    \leavevmode
    \begin{enumerate}[label=(\alph*)]
        \item Let $V$ be an $\mathfrak{sl}_2(\C)$-module and let $v \in V$ be such that
              \[
                  h(v) = \lambda v \qquad \text{for some } \lambda \in \C.
              \]
              Set $v_j = (j!)^{-1} f^j(v)$. Then
              \begin{equation}
                  h(v_j) = (\lambda - 2j) v_j. \tag{3.2.3}
              \end{equation}
              If, in addition, $e(v) = 0$, then
              \begin{equation}
                  e(v_j) = (\lambda - j + 1)v_{j-1}. \tag{3.2.4}
              \end{equation}

        \item For each integer $k \geq 0$ there exists a unique, up to isomorphism, irreducible $(k+1)$-dimensional $\mathfrak{sl}_2(\C)$-module. In some $\C$-basis $\{v_j\}_{j=0}^k$ of the space of this module, the action of $\mathfrak{sl}_2(\C)$ looks as follows:
              \[
                  h(v_j) = (k-2j)v_j,
                  \quad f(v_j) = (j+1)v_{j+1},
                  \quad e(v_j) = (k+1-j)v_{j-1}.
              \]
              Here $j=0,\dots,k$ and we assume that $v_{k+1}=0=v_{-1}$.
    \end{enumerate}
\end{lemma}

\begin{example}[Deducing the Serre relations]
Let $\mathfrak{g}(A)$ be a Kac--Moody algebra, $e_i, f_i \ (i=1,\dots,n)$ be
its Chevalley generators. Set 
\[
  \mathfrak{g}_{(i)} = \mathbb{C}e_i + \mathbb{C}\alpha_i^\vee + \mathbb{C}f_i;
\]
then $\mathfrak{g}_{(i)}$ is isomorphic to $\mathfrak{sl}_2(\mathbb{C})$, with standard basis 
$\{e_i, \alpha_i^\vee, f_i\}$.

We can deduce now the following relations between the Chevalley generators:
\begin{equation}\label{eq:Serre}
   (\operatorname{ad} e_i)^{\,1-a_{ij}} e_j = 0, 
   \qquad
   (\operatorname{ad} f_i)^{\,1-a_{ij}} f_j = 0, 
   \quad \text{if } i \neq j.
\end{equation}

We prove the second relation; the first one follows by making use of the 
Chevalley involution $\omega$.

Denote $v = f_j$, $\theta_{ij} = (\operatorname{ad} f_i)^{\,1-a_{ij}} f_j$.  
Consider $\mathfrak{g}(A)$ as a $\mathfrak{g}_{(i)}$-module by restricting the adjoint representation. We have:
\[
  \alpha_i^\vee(v) = -a_{ij} v, 
  \qquad
  e_i(v) = 0 \quad \text{if } i \neq j.
\]

We can now compute \begin{align*}
    [e_i, \theta_{ij}] & = [e_i, \ad^{1-a_{ij}}(f_i) f_j] \\
    &= [e_i, f_i^{1-a_{ij}}] f_j + f_i^{1-a_{ij}} [e_i, f_j] \texty{ (by the Leibniz rule)} \\
    &= [e_i, f_i^{1-a_{ij}}] f_j \\
    &= (1-a_{ij})\bigl(-a_{ij} - (1-a_{ij})+1 \bigr)(\operatorname{ad} f_i)^{-a_{ij}} f_j\\
    &= 0 \quad \text{if } i \neq j.
\end{align*}
where in the second to last line, we invoked the fact that $[e,f^k] = -k(k-1) f^{k-1} + k f^{k-1} h$ and $h = \alpha_i^\vee$ acts on $f_j$ by $-a_{ij}$.

It is also clear that $e_k$ commutes with $\theta_{ij}$ if $k \neq i, k \neq j$
(by relations (1.2.1)), and also if $k=j$ but $a_{ij} \neq 0$. Finally, if $k=j$ and $a_{ij}=0$, we have:
\[
   [e_j, \theta_{ij}] = [e_j, [f_i,f_j]] = a_{ji} f_i = 0 
   \quad \text{(by (C3))}.
\]

So, $[e_k, \theta_{ij}] = 0$ for all $k$ and we apply the following lemma to conclude that $\theta_{ij} = 0$.
\end{example}
\begin{lemma}
Let $a \in \mathfrak{n}_+$ be such that $[a,f_i] = 0$ for all $i=1,\dots,n$. 
Then $a=0$. Similarly, for $a \in \mathfrak{n}_-$, if $[a,e_i] = 0$ for all 
$i=1,\dots,n$, then $a=0$.
\end{lemma}

\begin{proof}
Let $a \in \mathfrak{n}_+$ be such that $[a, \mathfrak{g}_{-1}(1)] = 0$. 
Then it is easy to see that 
\[
   \sum_{i,j \geq 0} (\operatorname{ad}\,\mathfrak{g}_1(1))^i 
   (\operatorname{ad}\,\mathfrak{h})^j a
\]
is a subspace of $\mathfrak{n}_+ \subset \mathfrak{g}(A)$, which is invariant with respect 
to $\operatorname{ad}\,\mathfrak{g}_1(1)$, $\operatorname{ad}\,\mathfrak{h}$ and 
$\operatorname{ad}\,\mathfrak{g}_{-1}(1)$ (the condition on $a$ is used only in the last case). 
Hence if $a \neq 0$, we obtain a nonzero ideal in $\mathfrak{g}(A)$ which intersects 
$\mathfrak{h}$ trivially. This contradicts the definition of $\mathfrak{g}(A)$.
\end{proof}
\begin{remark}
    Note that in the lemma is where we invoke the fact that we have taken the quotient by the largest ideal intersecting $\mathfrak{h}$ trivially. In particular, this is what allows us to deduce the Serre relations \eqref{eq:Serre}.
\end{remark}


\subsection*{Locally nilpotent operators}
Now we need a general fact about a module $V$ over a Lie algebra $\mathfrak{g}$.
\begin{definition}
An element $x\in\mathfrak{g}$ is said to be \textbf{locally nilpotent on $V$}
if for any $v\in V$ there exists a positive integer $N$ such that $x^{N}(v)=0$.
\end{definition}

\begin{lemma}[Local nilpotence lemma]\label{lem:local-nilpotence}
\leavevmode
\begin{enumerate}[label=(\alph*)]
\item Let $y_1,y_2,\dots$ be a system of generators of a Lie algebra $\mathfrak{g}$
and let $x\in\mathfrak{g}$ be such that $(\ad x)^{N_i}y_i=0$ for some positive integers
$N_i$, $i=1,2,\dots$. Then $\ad x$ is locally nilpotent on $\mathfrak{g}$.

\item Let $v_1,v_2,\dots$ be a system of generators of a $\mathfrak{g}$-module $V$,
and let $x\in\mathfrak{g}$ be such that $\ad x$ is locally nilpotent on $\mathfrak{g}$
and $x^{N_i}(v_i)=0$ for some positive integers $N_i$, $i=1,2,\dots$.
Then $x$ is locally nilpotent on $V$.
\end{enumerate}
\end{lemma}

\begin{proof}
Since $\ad x$ is a derivation of $\mathfrak{g}$, one has the Leibniz formula
\[
(\ad x)^k [y,z] \;=\; \sum_{i=0}^{k} \binom{k}{i}
\big[(\ad x)^i y,\, (\ad x)^{k-i} z\big],
\]
proving (a) by induction on the length of commutators in the $y_i$.
Part (b) follows from the following formula (for $\lambda=\mu=0$):
\begin{equation}\label{eq:341}
(x-\lambda-\mu)^k a \;=\; \sum_{s=0}^{k} \binom{k}{s}
\big((\ad x-\lambda)^s a\big)\,(x-\mu)^{k-s}, \qquad k\ge 0,\ \lambda,\mu\in\mathbb{C},
\end{equation}
which holds in any associative algebra. In order to prove \eqref{eq:341}, note
that $\ad x=L_x-R_x$, where $L_x$ and $R_x$ are the operators of left and right
multiplication by $x$, and that $L_x$ and $R_x$ commute (by associativity).
Now we apply the binomial formula to $L_x-\lambda-\mu=(\ad x-\lambda)+(R_x-\mu)$.
\qedhere
\end{proof}

Applying the binomial formula to $\ad x=L_x-R_x$, we obtain another useful
formula (in any associative algebra):
\begin{equation}\label{eq:342}
(\ad x)^k a \;=\; \sum_{s=0}^{k}(-1)^s \binom{k}{s}\; x^{\,k-s}\, a\, x^{\,s}.
\end{equation}

\begin{lemma}[Nilpotence of Chevalley generators]\label{lem:Chevalley-nilpotent}
$\ad e_i$ and $\ad f_i$ are locally nilpotent on $\mathfrak{g}(A)$.
\end{lemma}

\begin{proof}
By the Serre relations \eqref{eq:Serre}, 
\[
   (\ad e_i)^{|a_{ij}|+1}x = 0 = (\ad f_i)^{|a_{ij}|+1}x, \qquad 
   \text{if } x=e_j \text{ or } f_j.
\]
Also,
\[
   (\ad e_i)^2 h = 0 = (\ad f_i)^2 h, \qquad \text{if } h\in \mathfrak{h}.
\]
Now apply Lemma \ref{lem:local-nilpotence}.
\end{proof}


\begin{definition}
    A $\mathfrak{g}(A)$-module $V$ is called \textbf{$\mathfrak{h}$-diagonalizable} if
\[
   V = \bigoplus_{\lambda\in\mathfrak{h}^*} V_\lambda,
   \qquad
   V_\lambda = \{\,v\in V \mid h(v) = \langle \lambda,h\rangle v
   \ \text{for } h\in\mathfrak{h}\,\}.
\]
As usual, $V_\lambda$ is called a \textbf{weight space}, 
$\lambda \in \mathfrak{h}^*$ is called a \textbf{weight} if $V_\lambda \neq 0$, 
and $\dim V_\lambda$ is called the \textbf{multiplicity} of $\lambda$ and is denoted
by $\operatorname{mult}_V \lambda$. Similarly, one defines an $\mathfrak{h}'$-diagonalizable
$\mathfrak{g}'(A)$-module, its weights, etc.

An $\mathfrak{h}$- (resp. $\mathfrak{h}'$-) diagonalizable module over a Kac--Moody
algebra $\mathfrak{g}(A)$ (resp. $\mathfrak{g}'(A)$) is called \textbf{integrable} if all $e_i$ and $f_i$ ($i=1,\dots,n$) are locally nilpotent on $V$.
\end{definition}

Note that the underlying module of the adjoint representation of a Kac--Moody algebra is an integrable module by \ref{lem:Chevalley-nilpotent}.

\begin{proposition}[Weights of integrable modules]\label{prop:weights-integrable}
Let $V$ be an integrable $\mathfrak{g}(A)$-module.
\begin{enumerate}[label=(\alph*)]
\item As a $\mathfrak{g}_{(i)}$-module, $V$ decomposes into a direct sum of finite dimensional irreducible 
$\mathfrak{h}$-invariant modules (hence the action of $\mathfrak{g}_{(i)}$ on $V$ can be ``integrated'' 
to the action of the group $SL_2(\mathbb{C})$).

\item Let $\lambda \in \mathfrak{h}^*$ be a weight of $V$ and let $\alpha_i$ be a simple root of $\mathfrak{g}(A)$. 
Denote by $M$ the set of all $t \in \mathbb{Z}$ such that $\lambda+t\alpha_i$ is a weight of $V$, and let 
$m_t = \mathrm{mult}_V(\lambda+t\alpha_i)$. Then:
\begin{enumerate}[label=(\roman*)]
\item $M$ is the closed interval of integers $[-p,q]$, where $p$ and $q$ are both either nonnegative integers or $\infty$, 
and $p-q=\langle \lambda,\alpha_i^\vee\rangle$ when both $p$ and $q$ are finite; if $\mathrm{mult}_V\lambda<\infty$, then $p$ and $q$ are finite.

\item $e_i: V_{\lambda+t\alpha_i}\to V_{\lambda+(t+1)\alpha_i}$ is an injection for $t\in[-p,-\tfrac{1}{2}\langle\lambda,\alpha_i^\vee\rangle]$; 
in particular, the function $t\mapsto m_t$ increases on this interval.

\item The function $t\mapsto m_t$ is symmetric with respect to $t=-\tfrac{1}{2}\langle\lambda,\alpha_i^\vee\rangle$.

\item If both $\lambda$ and $\lambda+\alpha_i$ are weights, then $e_i(V_\lambda)\neq 0$.
\end{enumerate}
\end{enumerate}
\end{proposition}

\begin{proof}
We have by the Leibniz rule:
\begin{equation}\label{eq:361}
e_i f_i^k(v) = k\bigl(1-k+\langle \lambda,\alpha_i^\vee\rangle\bigr) f_i^{k-1}(v) 
+ f_i^k e_i(v), \qquad v\in V_\lambda.
\end{equation}
We deduce that the subspace
\[
U=\sum_{k,m\ge0} f_i^k e_i^m(v)
\]
is $(\mathfrak{g}_{(i)}+\mathfrak{h})$-invariant. As $e_i$ and $f_i$ are locally nilpotent on $V$, $\dim U < \infty$.
By the Weyl complete reducibility theorem applied to the $\mathfrak{g}_{(i)}$-module $U$, the latter decomposes into a direct sum of finite dimensional 
$\mathfrak{h}$-invariant irreducible $\mathfrak{g}_{(i)}$-modules (cf. Exercise 3.11). So, each $v\in V$ lies in a direct sum of finite dimensional 
$\mathfrak{h}$-invariant irreducible $\mathfrak{g}_{(i)}$-modules, and (a) follows.

For the proof of (b) we use (a) and Lemma \ref{lem:sl2-modules}. Set 
\[
U=\sum_{k\in\mathbb{Z}} V_{\lambda+k\alpha_i};
\]
this is a $(\mathfrak{g}_{(i)}+\mathfrak{h})$-module, which is a direct sum of finite dimensional irreducible $(\mathfrak{g}_{(i)}+\mathfrak{h})$-modules. 
Let $p=-\inf M$, $q=\sup M$. Both $p$ and $q$ are nonnegative as $0\in M$. Now all the statements of (b) follow from Lemma 3.2(b), as 
$\langle \lambda+t\alpha_i,\alpha_i^\vee\rangle=0$ for $t=-\tfrac{1}{2}\langle\lambda,\alpha_i^\vee\rangle$.
\end{proof}

\begin{corollary}
\leavevmode
\begin{enumerate}[label=(\alph*)]
\item If $\lambda$ is a weight of an integrable $\mathfrak{g}(A)$-module $V$ and $\lambda+\alpha_i$ 
(resp. $\lambda-\alpha_i$) is not a weight, then $\langle \lambda,\alpha_i^\vee\rangle \ge 0$ 
(resp. $\langle \lambda,\alpha_i^\vee\rangle \le 0$).

\item If $\lambda$ is a weight of $V$, then $\lambda - \langle \lambda,\alpha_i^\vee\rangle \alpha_i$ 
is also a weight of the same multiplicity.
\end{enumerate}
\end{corollary}

\begin{proof}

Suppose $\lambda$ is a weight. This means $0\in M$.

If $\lambda+\alpha_i$ is not a weight, then $1\notin M$. Since $M$ is the integer interval $[-p,q]$, the only way this happens is if $q=0$. Then
$p - q = p - 0 = p = \langle \lambda,\alpha_i^\vee\rangle \ge 0$.

The multiplicities are symmetric with respect to $t_0 = -\tfrac{1}{2}\langle \lambda,\alpha_i^\vee\rangle$. At $t=0$, the multiplicity is $\mathrm{mult}_V(\lambda)$. Symmetry says $m_t = m_{-\,\langle \lambda,\alpha_i^\vee\rangle - t}$. Plugging $t=0$ gives $\mathrm{mult}_V(\lambda) = m_0 = m_{-\langle \lambda,\alpha_i^\vee\rangle}$.
\end{proof}

\begin{remark}
Let $V$ be an integrable $\mathfrak{g}'(A)$-module. Then, clearly the proposition and corollary with $\mathfrak{h}$ replaced by $\mathfrak{h}'$, still hold. Furthermore, the local nilpotency of $e_i$ and $f_i$ on $V$ 
guarantees that $V$ is $\mathfrak{h}_i$-diagonalizable, and hence $\mathfrak{h}'$-diagonalizable provided that $n<\infty$. 
This follows from a general fact which will be proved later.
\end{remark}


\begin{remark}[Interpretation of the number $\langle \lambda,\alpha_i^\vee\rangle$]
    
There is a lattice $P$ of weights, called the \emph{weight lattice}, which contains all weights of integrable modules. It is defined by
\[    P = \{\lambda \in \mathfrak{h}^* \mid \langle \lambda, \alpha_i^\vee \rangle \in \mathbb{Z} \text{ for all } i=1,\dots,n\}.
\]
It is generated by the \emph{fundamental weights} $\{\omega_i\}_{i=1}^n$ defined by
\[    \langle \omega_i, \alpha_j^\vee \rangle = \delta_{ij}.
\] In particular, the fundamental weights are the dual basis to the simple coroots $\{\alpha_i^\vee\}_{i=1}^n$ under the evaluation pairing. Then the number $\langle \lambda, \alpha_i^\vee \rangle$ is the coefficient of $\omega_i$ in the expansion of $\lambda$ in terms of the fundamental weights.

One has to ask why the weights of integrable modules lie in $P$. This is because restricting to each $\mathfrak{sl}_2$-triple forces the eigenvalues of $\alpha_i^\vee$ to be integers, i.e. $\langle \lambda,\alpha_i^\vee\rangle \in \mathbb{Z}$, precisely because of the integrable condition.

The root lattice $Q$ is defined to be the integer span of the simple roots $\{\alpha_i\}_{i=1}^n$. It is a sublattice of $P$. In the finite dimensional case, $Q$ has finite index in $P$ of order equal to the determinant of the Cartan matrix. In the infinite dimensional case, $\det A = 0$, so $Q$ has infinite index in $P$.
\end{remark}

\begin{definition}[Weyl group]
    For each $i=1,\dots,n$ we define the 
\textbf{fundamental reflection} $r_i$ of the space $\mathfrak{h}^*$ by
\[
   r_i(\lambda) = \lambda - \langle \lambda,\alpha_i^\vee\rangle \alpha_i,
   \qquad \lambda \in \mathfrak{h}^*.
\]

It is clear that $r_i$ is a reflection since its fixed point set is  $T_i = \{ \lambda \in \mathfrak{h}^* \mid \langle \lambda,\alpha_i^\vee\rangle=0\}$,
and $r_i(\alpha_i) = -\alpha_i$.

The subgroup $W$ of $GL(\mathfrak{h}^*)$ generated by all fundamental reflections 
is called the \textbf{Weyl group} of $\mathfrak{g}(A)$. We will write $W(A)$ when 
necessary to emphasize the dependence on $A$.
\end{definition}

\begin{proposition}[Weyl group invariance of weights and roots]\label{prop:Weyl-invariance}
\leavevmode
\begin{enumerate}[label=(\alph*)]
\item Let $V$ be an integrable module for Kac--Moody algebra $\mathfrak{g}(A)$. 
Then $\mathrm{mult}_V \lambda = \mathrm{mult}_V w(\lambda)$ for every 
$\lambda \in \mathfrak{h}^*$ and $w\in W$. In particular, the set of weights of $V$ 
is $W$-invariant.

\item The root system $\Delta$ of $\mathfrak{g}(A)$ is $W$-invariant, and 
$\mathrm{mult}\,\alpha = \mathrm{mult}\,w(\alpha)$ for every 
$\alpha \in \Delta$, $w\in W$.
\end{enumerate}
\end{proposition}

\begin{proof}The multiplicities $m_t$ in a weight string along a simple root direction are symmetric and behave nicely under reflection. The fundamental reflection $r_i$ exactly encodes this symmetry. It sends
$\lambda \mapsto \lambda - \langle \lambda,\alpha_i^\vee \rangle \alpha_i$
\end{proof}

The following lemma tells us what happens to the positive roots under a fundamental reflection. In particular, it says that all of the positive roots stay positive except for the simple root corresponding to the reflection, which gets sent to its negative.
\begin{lemma}\label{lem:3.7}
If $\alpha \in \Delta_+$ and $r_i(\alpha) < 0$, then $\alpha = \alpha_i$. 
In other words, $\Delta_+ \setminus \{\alpha_i\}$ is $r_i$-invariant.
\end{lemma}

\begin{proof}
Follows from Lemma \ref{lem:rt-string}. In particular, the simple reflection $r_i$ reverses the $\alpha_i$-string through $\alpha$ and we know that root strings don't pass through zero unless $\alpha = \alpha_i$.
\end{proof}

\begin{proposition}[3.9]
The restriction of the bilinear form $(.|.)$ to $\mathfrak{h}^*$ is $W$-invariant.
\end{proposition}

\begin{proof}
    It is a standard fact from linear algebra that reflection with respect to a nonzero vector $\alpha_i$ preserves the bilinear form, provided the form makes $\alpha_i$ non-isotropic (i.e. $(\alpha_i|\alpha_i)\neq 0$). As $|r_i(\alpha_i)|^2 = |-\alpha_i|^2 = |\alpha_i|^2 \neq 0$, it suffices to check that $(\lambda|\alpha_i) = 0$ implies 
$(r_i(\lambda)|\alpha_i) = 0$. Compute:
\[
(r_i(\lambda)|\alpha_i) = (\lambda|\alpha_i) - \langle \lambda, \alpha_i^\vee \rangle (\alpha_i|\alpha_i)
\]

Now use the formula:
\[
\alpha_i^\vee = \frac{2}{(\alpha_i|\alpha_i)}\nu^{-1}(\alpha_i)
\]

So
\[
\langle \lambda, \alpha_i^\vee \rangle = \frac{2}{(\alpha_i|\alpha_i)} (\lambda|\alpha_i)
\]

Plugging back:
\[
(r_i(\lambda)|\alpha_i)
= (\lambda|\alpha_i) - \frac{2}{(\alpha_i|\alpha_i)} (\lambda|\alpha_i)(\alpha_i|\alpha_i)
= (\lambda|\alpha_i) - 2(\lambda|\alpha_i)
= -(\lambda|\alpha_i)
\]

Therefore if $(\lambda|\alpha_i) = 0$, then $(r_i(\lambda)|\alpha_i)=0$. 
\end{proof}



We have the following technical lemma about words in the Weyl group. It says that if applying a word $w = r_{i_1}\cdots r_{i_t}$ to a simple root makes it negative, then $wr_i$ is a shorter word than $w$.

\begin{lemma}[Exchange lemma]\label{lem:exchange}
If $\alpha_i$ is a simple root and 
\[
   r_{i_1}\cdots r_{i_t}(\alpha_i) < 0,
\]
then there exists $s$ $(1 \leq s \leq t)$ such that
\begin{equation}\label{eq:3.10.1}
   r_{i_1}\cdots r_{i_s}\cdots r_{i_t} r_i
   = r_{i_1}\cdots r_{i_{s-1}} r_{i_{s+1}} \cdots r_{i_t}.
\end{equation}
\end{lemma}

\begin{definition}
The expression $w = r_{i_1}\cdots r_{i_s} \in W$ is called \textbf{reduced} if $s$ is 
minimal possible among all representations of $w \in W$ as a product of the $r_i$. 
Then $s$ is called the \textbf{length} of $w$ and is denoted by $\ell(w)$. 
\end{definition}
Note that $\det_{\mathfrak{h}^*} r_i = -1$ and hence
\begin{equation}\label{eq:3.11.1}
   \det_{\mathfrak{h}^*} w = (-1)^{\ell(w)} \qquad \text{for } w \in W.
\end{equation}

The following lemma is an important corollary of Lemma \ref{lem:exchange}

\begin{lemma}
Let $w = r_{i_1}\cdots r_{i_t} \in W$ be a reduced expression and let $\alpha_i$ 
be a simple root. Then we have
\begin{enumerate}[label=(\alph*)]
\item $\ell(wr_i) < \ell(w)$ if and only if $w(\alpha_i) < 0$.
\item $w(\alpha_{i_t}) < 0$.
\item If $\ell(wr_i) < \ell(w)$, then there exists $s$, 
$1 \leq s \leq t$, such that
\[
   r_{i_1}\cdots r_{i_s}\cdots r_{i_t} = r_{i_1}\cdots r_{i_{s-1}}\, r_i\, r_{i_{s+1}}\cdots r_{i_t}.
\]
\end{enumerate}
\end{lemma}

\begin{proof}
By Lemma \ref{lem:exchange} (applied to $w$), $w(\alpha_i)<0$ implies that $\ell(wr_i)<\ell(w)$. 
If now $w(\alpha_i) > 0$, then $wr_i(\alpha_i)<0$ and hence $\ell(w) = \ell(wr_i^2) < \ell(wr_i)$, proving (a). 
Part (b) follows immediately from (a). 

Finally, if $\ell(wr_i)<\ell(w)$, then (a) implies $w(\alpha_i)<0$ and applying 
Lemma \ref{lem:exchange} to $w$ we deduce the exchange condition, multiplying it 
by $(r_{i_1}\cdots r_{i_{s-1}})^{-1}$ on the left and by $r_i$ on the right.
\end{proof}

Now we are in a position to study the geometric properties of the action of the Weyl group. 
Pick a real realization $(\mathfrak{h}_{\mathbb{R}},\Pi, \Pi^\vee)$ of $A$ so that $(\mathfrak{h}_{\mathbb{R}} \otimes_{\mathbb{R}} \mathbb{C}), \Pi, \Pi^\vee)$ is a realization of $A$ over $\C$. 
The set
\[
   C = \{ h \in \mathfrak{h}_{\mathbb{R}} \mid \langle \alpha_i,h \rangle \geq 0 
   \ \text{for } i=1,\dots,n \}
\]
is called the \textbf{fundamental chamber}. There is a contragradient action of $W$ on $\mf h$ defined by \begin{align*}
    r_i(h) &= h - \langle \alpha_i,h \rangle \alpha_i^\vee, \qquad h \in \mathfrak{h}
\end{align*} which respects the pairing $\langle \cdot, \cdot \rangle$. \begin{align*}
    \langle r_i(\alpha), h \rangle &= \langle \alpha, r_i(h) \rangle, \qquad \alpha \in \mathfrak{h}^*, h \in \mathfrak{h}.
\end{align*} Note that $\mathfrak{h}_{\mathbb{R}}$ is stable under $W$ since $Q^\vee \subset \mathfrak{h}_{\mathbb{R}}$. Therefore we can talk about the sets $w(C)$, $w\in W$, called \textbf{chambers}, and their union
\[
   X = \bigcup_{w\in W} w(C)
\]
is called the \textbf{Tits cone}. We clearly have the corresponding dual notions of $C^\vee$ and $X^\vee$ in $\mathfrak{h}_{\mathbb{R}}^*$.

\begin{proposition}[Geometry of $W$]
\leavevmode
\begin{enumerate}[label=(\alph*)]
\item For $h \in C$, the group $W_h = \{ w \in W \mid w(h) = h \}$ 
is generated by the fundamental reflections which it contains.

\item The fundamental chamber $C$ is a fundamental domain for the action of $W$ on $X$, 
i.e.\ any orbit $W\cdot h$ of $h \in X$ intersects $C$ in exactly one point. 
In particular, $W$ operates simply transitively on chambers.

\item $X = \{ h \in \mathfrak{h}_{\mathbb{R}} \mid \langle \alpha,h \rangle < 0 \ 
\text{only for a finite number of } \alpha \in \Delta_+ \}$. 
In particular, $X$ is a convex cone.

\item $C = \{ h \in \mathfrak{h}_{\mathbb{R}} \mid \text{for every } w \in W,\ 
h-w(h) = \sum_i c_i \alpha_i^\vee \ \text{where } c_i \geq 0 \}$.

\item The following conditions are equivalent:
\begin{enumerate}[label=(\roman*)]
\item $|W| < \infty$;
\item $X = \mathfrak{h}_{\mathbb{R}}$;
\item $|\Delta| < \infty$;
\item $|\Delta^\vee| < \infty$.
\end{enumerate}

\item If $h \in X$, then $|W_h| < \infty$ if and only if $h$ lies in the interior of $X$.
\end{enumerate}
\end{proposition}

\begin{proof}
Let $w \in W$ and let $w = r_{i_1}\cdots r_{i_s}$ be a reduced expression of $w$. 
Take $h \in C$ and suppose that $h' = w(h) \in C$. 
We have $\langle \alpha_i,h \rangle \geq 0$ and therefore 
$\langle w(\alpha_i),h' \rangle \geq 0$ by the $W$-invariance of the pairing. But by lemma above, $w(\alpha_i)<0$, and hence 
$\langle w(\alpha_i),h' \rangle \leq 0$ because $h' \in C$. So $\langle w(\alpha_i),h' \rangle = 0$ and $\langle \alpha_i,h \rangle = 0$. 
Hence $r_i(h) = h$ and both (a) and (b) follow by induction on $\ell(w)$.

Set \[X' = \{ h \in \mathfrak{h}_{\mathbb{R}} \mid \langle \alpha,h \rangle < 0 
\ \text{only for a finite number of } \alpha \in \Delta_+ \}\]
It is clear that $C \subset X'$ and that $X'$ is $r_i$-invariant. 
Hence $X' \supset X$. In order to prove the reverse inclusion, let $h \in X'$ 
and set $M_h = \{ \alpha \in \Delta_+ \mid \langle \alpha,h \rangle < 0 \}$. We need to show that $M_h$ is empty. By definition, $|M_h|$ is finite. If $M_h \neq \emptyset$, then $\alpha_i \in M_h$ 
for some $i$. But then it follows from Lemma \ref{lem:3.7} that $|M_{r_i(h)}| < |M_h|$. This process must eventually terminate with some $h^{(k)}$ such that $M_{h^{(k)}} = \emptyset$. But $M_{h^{(k)}} = \emptyset$ means $h^{(k)} \in C$. Thus $h^{(k)} \in C$ and $h = w^{-1}(h^{(k)}) \in w^{-1}(C)$, so $h \in W(C) = X$ as desired.

The inclusion $\supset$ of (d) follows from the fact that \begin{align*}
    h - r_i(h) &= \langle \alpha_i,h \rangle \alpha_i^\vee
\end{align*} so being in the RHS implies that $\langle \alpha_i,h \rangle \geq 0$ for all $i$, i.e. $h \in C$.


We prove the reverse inclusion by induction on $s=\ell(w)$. 
For $\ell(w)=1$, (d) is the definition of $C$. 
If $\ell(w)>1$, let $w=r_{i_1}\cdots r_{i_s}$. 
We have
\[
   h-w(h) = (h-r_{i_1}\cdots r_{i_{s-1}}(h)) + r_{i_1}\cdots r_{i_{s-1}}(h-r_{i_s}(h)),
\]
and we apply the inductive assumption to the first summand and part (b) of the lemma above for $\Delta^\vee$ to the second summand. In particular, the lemma says that $w(\alpha_{i_s})<0$.
But $w(\alpha_{i_s})<0$ is equivalent to
$r_{i_1}\cdots r_{i_{s-1}}(\alpha_{i_s}) > 0$ is a positive root. The second summand is \begin{align*}
    r_{i_1}\cdots r_{i_{s-1}}(h - r_{i_s}(h)) &= r_{i_1}\cdots r_{i_{s-1}}(\langle \alpha_{i_s}, h \rangle \alpha_{i_s}^\vee) \\
    &= \langle \alpha_{i_s}, h \rangle r_{i_1}\cdots r_{i_{s-1}}(\alpha_{i_s}^\vee)
\end{align*} is a nonnegative multiple of a positive coroot, as desired.

Now we prove (e). To show (i) $\Rightarrow$ (ii). Pick $\rho\in \mathfrak{h}_{\mathbb{R}}^*$ in the interior of $C^\vee$, e.g. $\rho=\Lambda_1+\cdots+\Lambda_n$. Then
$\langle \rho,\alpha_i^\vee\rangle>0$ for all $i$.
Consider the finite set $W\cdot h$ and choose $h'\in W\cdot h$ that maximizes $\langle \rho, h' \rangle$.
If $h'\notin C$, then for some $i$ we have $\langle \alpha_i,h'\rangle<0$. Reflect:
\[
\langle \rho, r_i(h')\rangle
= \langle \rho, h' - \langle \alpha_i,h'\rangle \alpha_i^\vee\rangle
= \langle \rho, h'\rangle - \langle \alpha_i,h'\rangle\,\langle \rho,\alpha_i^\vee\rangle
> \langle \rho,h'\rangle
\]
since $\langle \rho,\alpha_i^\vee\rangle>0$ and $\langle \alpha_i,h'\rangle<0$. This contradicts maximality. Hence $\langle \alpha_i,h'\rangle\ge 0$ for all $i$, i.e. $h'\in C$. So every orbit meets $C$; thus $X=\mathfrak{h}_{\mathbb{R}}$.

In order to show (ii) $\Rightarrow$ (iii) take $h$ in the interior of $C$, so $\langle \alpha_i,h\rangle>0$ for all simple $\alpha_i$. Then for every positive root $\alpha$ (a nonnegative combination of simple roots) we have $\langle \alpha, h\rangle>0$, hence
$\langle \alpha,-h\rangle<0$ $\quad$ for all $\alpha\in\Delta_+$ .
Since (ii) says $-h\in X$, apply part (c) of this proposition: $X=\{x : \langle \alpha,x\rangle<0$ for only finitely many $\alpha\in\Delta_+\}$. But for $x=-h$ every $\alpha\in\Delta_+$ pairs negatively, so $\Delta_+$ must be finite. Thus $|\Delta|<\infty$.

(iii) $\Rightarrow$ (i) because The Weyl group $W$ acts on $\Delta$ by permuting roots, so we get a homomorphism $\varphi: W \longrightarrow \mathrm{Sym}(\Delta)$. To conclude $|W|<\infty$, it suffices to show $\varphi$ is injective, because $\mathrm{Sym}(\Delta)$ is finite.

We invoke the fact that
\begin{equation}\label{eq:3.12.1}
   \{ w(\alpha) = \alpha \ \text{for } w\in W \text{ and all } \alpha \in \Delta_+ \}
   \Rightarrow w=1.
\end{equation}
To prove \eqref{eq:3.12.1}, note that if a reduced expression $w=r_{i_1}\cdots r_{i_s}$ 
is nontrivial, then part (b) of the above lemma implies that $w(\alpha_i)<0$, contradiction. 
The fact that (iv) is equivalent to (i) follows by using the dual root system.

Finally, to prove (f) we may assume that $h \in C$. Then (f) follows from (a) 
by applying the equivalence of (e)(i) and (e)(ii) to $W_h$ operating on $\mathfrak{h}/\C h$.
\end{proof}

\begin{exercise}
\end{exercise}

\section{Root systems of affine Lie algebras}
In order to develop the theory of root systems of Kac-Moody algebras we need to know some properties of generalized Cartan matrices. It is convenient to work in a slightly more general situation. Let $A = (a_{ij})$ which satisfies the following three properties:
\begin{itemize}
    \item[(m1)] $A$ is indecomposable;
    \item[(m2)] $a_{ij} \leq 0$ for $i \neq j$;
    \item[(m3)] $a_{ij} = 0$ implies $a_{ji} = 0$.
\end{itemize}
Kac proves the following classification theorem in \cite{kac}.
\begin{theorem}[Classification of matrices satisfying (m1), (m2), and (m3)]
    Let $A$ be a real $n \times n$ matrix satisfying (m1), (m2), and (m3). Then one and only one of the following three possibilities holds for both $A$ and ${}^t A$:
    \begin{itemize}
        \item[(Fin)] $\det A \neq 0$; there exists $u > 0$ such that $Au > 0$; $Av \geq 0$ implies $v > 0$ or $v = 0$;
        \item[(Aff)] $\operatorname{corank} A = 1$; there exists $u > 0$ such that $Au = 0$; $Av \geq 0$ implies $Av = 0$;
        \item[(Ind)] there exists $u > 0$ such that $Au < 0$; $Av \geq 0$, $v \geq 0$ imply $v = 0$.
    \end{itemize}
\end{theorem}

\begin{remark}
    The matrices of type (Fin) are precisely the Cartan matrices of finite-dimensional semisimple Lie algebras. The matrices of type (Aff) are precisely the Cartan matrices of affine Kac-Moody algebras. The matrices of type (Ind) are called indefinite type.

    Borcherds made the remark that nobody has really found a use for the indefinite type matrices.
\end{remark}

We proceed to classify all generalized Cartan matrices of finite and affine type. 
For this it is convenient to introduce the so-called \textbf{Dynkin diagrams}. 
\begin{definition}[Dynkin diagram]
    Let $A=(a_{ij})_{i,j=1}^n$ be a generalized Cartan matrix.  We associate with $A$ a graph $S(A)$, called the \emph{Dynkin diagram} of $A$ as follows. If $a_{ij}a_{ji}\leq 4$ and $|a_{ij}|\geq |a_{ji}|$, the vertices $i$ and $j$ are connected by $|a_{ij}|$ lines, and these lines are equipped with an arrow pointing toward $j$ if $|a_{ij}|>1$. 
If $a_{ij}a_{ji}>4$, the vertices $i$ and $j$ are connected by a bold-faced line 
equipped with an ordered pair of integers $|a_{ij}|,|a_{ji}|$. 
\end{definition}

It is clear that $A$ is indecomposable if and only if $S(A)$ is a connected graph. 
Note also that $A$ is determined by the Dynkin diagram $S(A)$ and an enumeration of its vertices. 
We say that $S(A)$ is of finite, affine, or indefinite type if $A$ is of that type.

\begin{proposition}[Properties of generalized Cartan matrices of finite and affine type]\label{prop:finite-affine}
Let $A$ be an indecomposable generalized Cartan matrix.
\begin{enumerate}[label=\alph*)]
    \item $A$ is of finite type if and only if all its principal minors are positive.
    \item $A$ is of affine type if and only if all its proper principal minors are positive and $\det A = 0$.
    \item If $A$ is of finite or affine type, then any proper subdiagram of $S(A)$ is a union of (connected) Dynkin diagrams of finite type.
    \item If $A$ is of finite type, then $S(A)$ contains no cycles. 
          If $A$ is of affine type and $S(A)$ contains a cycle, then $S(A)$ is the cycle $A_\ell^{(1)}$ from Table Aff 1.
    \item $A$ is of affine type if and only if there exists $\delta>0$ such that $A\delta=0$; 
          such a $\delta$ is unique up to a constant factor.
\end{enumerate}
\end{proposition}
In particular, an affine generalized Cartan matrix $A$ satisfies all proper principal minors are positive, $\det A = 0$, and is indecomposable. This means A is “just one step” beyond the finite type: it's rank deficient by one. That's why one gets affine (loop) extensions.

\begin{theorem}[Classification of generalized Cartan matrices of finite and affine type]
\leavevmode
\begin{enumerate}[label=\alph*)]
    \item The Dynkin diagrams of all generalized Cartan matrices of finite type 
    are listed in Table Fin.
    
    \item The Dynkin diagrams of all generalized Cartan matrices of affine type 
    are listed in Tables Aff~1--3 (all of them have $\ell+1$ vertices).
    
    \item The numerical labels in Tables Aff~1--3 are the coordinates of the unique 
    vector \[\delta = {}^t(a_0,a_1,\dots,a_\ell)\] such that $A\delta=0$ and the $a_i$ 
    are positive relatively prime integers.
\end{enumerate}
\end{theorem}

\begin{remark}
    Suppose $S(A)$ is of affine type. It turns out that if $S(A)$ has cycles, then it is the cycle $A_\ell^{(1)}$ from Table Aff 1. Otherwise, $S(A)$ has a vertex such that removing it produces a diagram of finite type. Now suppose that $S(A)$ has no cycles. Then by Proposition \ref{prop:finite-affine}(c), removing any vertex produces a diagram of finite type. Conversely, any diagram of affine type is obtained from a diagram of Table Fin by adding one vertex in such a way that any subdiagram is from Table Fin. Then one sees that the only the diagrams from Tables Aff~1--3 may be obtained in this way.
\end{remark}

\begin{proposition}[Characterizations of finite type]\label{prop:finite-char}
Let $A$ be an indecomposable generalized Cartan matrix. Then the following 
conditions are equivalent:
\begin{enumerate}[label=(\roman*)]
    \item $A$ is a generalized Cartan matrix of finite type;
    \item $A$ is symmetrizable and the bilinear form $(\,.\mid.\,)_{\mathfrak{h}_\mathbb{R}}$ 
          is positive-definite;
    \item $|W| < \infty$;
    \item $|\Delta| < \infty$;
    \item $\mathfrak{g}(A)$ is a simple finite-dimensional Lie algebra;
    \item there exists $\alpha \in \Delta_+$ such that $\alpha+\alpha_i \notin \Delta$ 
          for all $i=1,\dots,n$.
\end{enumerate}
\end{proposition}

\begin{figure}[H]
    \centering
\includegraphics[width=0.8\textwidth]{img/aff1.png}
    \caption{Dynkin diagrams of affine type - part 1}
\end{figure}

\begin{figure}[H]
    \centering \includegraphics[width=0.8\textwidth]{img/aff23.png} 
    \caption{Dynkin diagrams of affine type - parts 2 and 3}
\end{figure}


\begin{remark}
A root of a finite root system $\Delta$ which satisfies condition (vi) of 
Proposition \ref{prop:finite-char} is called a \textbf{highest root}. It is unique and given by 
the formula
\[
   \theta = \sum_{i=1}^\ell a_i \alpha_i,
\]
where $a_i$ are the labels of the extended Dynkin diagram from Table Aff~1.
\end{remark}



\begin{remark}
    Kac separates the affine types into three families:
\begin{enumerate}
    \item \textbf{Table Aff 1: Untwisted affine types}
These come from taking a finite-dimensional simple Lie algebra $\dot{\mathfrak{g}}$ and forming the loop algebra
$$\dot{\mathfrak{g}}\otimes \mathbb{C}[t,t^{-1}] \;\oplus\; \mathbb{C}c \;\oplus\; \mathbb{C}d.$$
The corresponding Cartan matrix is obtained by adding one extra node (the "affine node") to the Dynkin diagram of $\dot{\mathfrak{g}}$.
These are the "standard" affine types: $A_\ell^{(1)}$, $B_\ell^{(1)}$, $C_\ell^{(1)}$, ..., $G_2^{(1)}$, $F_4^{(1)}$, $E_6^{(1)}$, $E_7^{(1)}$, $E_8^{(1)}$.

    \item \textbf{Table Aff 2: Twisted affine types of order 2}
These come from folding untwisted diagrams by a diagram automorphism of order 2.
For example, folding $A_{2\ell-1}^{(1)}$ by an involution produces $C_\ell^{(1)}$, but folding $D_{\ell+1}^{(1)}$ or $E_6^{(1)}$ produces new "twisted" types $A_{2\ell-1}^{(2)}$, $D_{\ell+1}^{(2)}$, $E_6^{(2)}$.
These correspond to twisted loop algebras:
$$\dot{\mathfrak{g}}\otimes \mathbb{C}[t,t^{-1}]^\sigma$$
where $\sigma$ is a diagram automorphism of order 2.

    \item \textbf{Table Aff 3: Twisted affine types of order 3}
These arise similarly from folding by a diagram automorphism of order 3 (only possible for $D_4^{(1)}$), producing $D_4^{(3)}$.
\end{enumerate}
\end{remark}


\begin{example}
    [Affine $\sl_2$] We consider the affine Dynkin diagram $A_1^{(1)}$. It turns out that this corresponds to the affine Lie algebra $\widehat{\mathfrak{sl}}_2$. 
    \[\widehat{sl}_2 = (sl_2\otimes\mathbb C[t,t^{-1}]) \oplus \mathbb Cc \oplus \mathbb Cd\] which has bracket \begin{align*}
    [x\otimes t^m, y\otimes t^n] &= [x,y]\otimes t^{m+n} + m\delta_{m,-n}\kappa(x,y)c \\
    [d, x\otimes t^m] &= m x\otimes t^m \\
    [c, \widehat{sl}_2] &= 0
\end{align*} where $\kappa(x,y) = \operatorname{tr}(xy)$ is the Killing form on $\mathfrak{sl}_2$. The Cartan subalgebra is \begin{align*}
    \widehat{\mathfrak{h}} &= (\mathfrak{h}\otimes 1) \oplus \mathbb{C}c \oplus \mathbb{C}d \\
    &= \mathbb{C}h \oplus \mathbb{C}c \oplus \mathbb{C}d
\end{align*} where $\mathfrak{h} = \mathbb{C}h$ is the Cartan subalgebra of $\mathfrak{sl}_2$. If you study the weight spaces which appear in the adjoint representation of $\widehat{\mathfrak{sl}}_2$, you find that the roots are \begin{align*}
    \Delta &= \{\alpha + n\delta, -\alpha + n\delta, n\delta \mid n \in \mathbb{Z}, n \neq 0\} \\
    &= \{\pm \alpha + n\delta \mid n \in \mathbb{Z}\} \cup \{n\delta \mid n \in \mathbb{Z}, n \neq 0\}
\end{align*} where $\alpha \in \mathfrak{h}^*$ is the positive root of $\mathfrak{sl}_2$ and $\delta \in \widehat{\mathfrak{h}}^*$ is the \textbf{null root} defined by \begin{align*}
    \delta(h) &= 0 \\
    \delta(c) &= 0 \\
    \delta(d) &= 1.
\end{align*} The simple roots are $\alpha_0 = \delta - \alpha$ and $\alpha_1 = \alpha$. It seems to me that $\delta$ and $\alpha$ really should be the simple roots, but the issue is that $\delta$ is not a real root. It doesn't correspond to a reflection and it has no coroot, which is a consequence of the fact that $(\delta \mid \delta) = 0$. 

Then one computes the pairings \begin{align*}
    a_{11} &= \langle \alpha_1, \alpha_1^\vee \rangle & 2 \\
    a_{00} &= \frac{2(\alpha_0 \mid \alpha_0)}{(\alpha_0 \mid \alpha_0)} \texty{because there's a unique extension of $(\cdot \mid \cdot)$ to $\widehat{\sl_2}$} \\
    a_{10} &= \frac{2(\alpha_1 \mid \alpha_0)}{(\alpha_1 \mid \alpha_1)} = \frac{2(\alpha \mid \delta - \alpha)}{(\alpha \mid \alpha)} = -2 \\
\end{align*} because the under the extension of $(\cdot \mid \cdot)$ to $\widehat{\sl_2}$, $\delta$ is orthogonal to everything. By general uniqueness results, this is the only way to extend the Killing form so we didn't cheat. Hence the Cartan matrix is \[
    A = \begin{pmatrix}
        2 & -2 \\
        -2 & 2
    \end{pmatrix}
\] which is indeed the Cartan matrix of $A_1^{(1)}$
\end{example}

We elaborate upon this perspective of affine Lie algebras via central extensions of loop algebras and their derivations.

\subsection*{Central extensions of $\mathfrak{g}[z,z^{-1}]$}

Let $\mathfrak{g}$ be a simple Lie algebra over $\mathbb{C}$, e.g.\ $\mathfrak{sl}_n$, $\mathfrak{so}_n$ for $n \geq 5$, $\mathfrak{sp}_{2n}$ for $n \geq 4$, exceptional ones, etc. Fix an invariant inner product $\langle\ ,\ \rangle$ on $\mathfrak{g}$ satisfying
\[
    \langle x, [y,z]\rangle = \langle [x,y], z\rangle;
\]
since $\mathfrak{g}$ is simple, this inner product is unique up to scalar.

Now consider the \emph{loop algebra} $\mathfrak{g}[z,z^{-1}]$. It's called loop because it's “equal” to $T_e LG$, where
\[
    LG = \{\text{analytic maps } S^1 \to G\}
\]
(not precisely; we should really take some completion of $\mathfrak{g}[z,z^{-1}]$, but it's a dense subspace inside of $T_e LG$, which is enough for our purposes). This algebra $\mathfrak{g}[z,z^{-1}]$ is graded: the $n$th graded component is $\mathfrak{g} z^n$, hence the grading is given by operator $z\partial_z$. Hence
\[
    \mathfrak{g}[z,z^{-1}] = \bigoplus_{n \in \mathbb{Z}} \mathfrak{g}z^n
\]
is a graded Lie algebra, with
\[
    [xz^n, yz^m] = [x,y] z^{n+m}.
\]
It carries an invariant inner product defined by
\[
    \langle xz^n, yz^m \rangle = \delta_{n,-m} \langle x,y \rangle
\] where $\langle x,y \rangle$ is the invariant inner product on $\mathfrak{g}$. There is also the equivalent residue formulation of this inner product:
\[
    \langle x\otimes f(z), y\otimes g(z) \rangle = \langle x,y \rangle \operatorname{Res}_{z=0}\bigg(f(z)g(z)/z dz\bigg).
\] which is more suitable for generalizations. The choice of inner product on $\mathfrak{g}$ also gives rise to a 2-cocycle on $\mathfrak{g}[z,z^{-1}]$:
\[
    \omega(xz^n, yz^m) = n \delta_{n,-m} \langle x,y \rangle.
\]
This is a 2-cocycle because it is antisymmetric and satisfies the cocycle condition (Jacobi):
\[
    \omega([x,y],z) + \omega([y,z],x) + \omega([z,x],y) = 0.
\]

Recall that central extensions are classified by $H^2(\mathfrak{g}[z,z^{-1}])$, so we want to compute this cohomology group. It turns out that $H^2(\mathfrak{g}[z,z^{-1}]) \cong \mathbb{C}$, and the class of $\omega$ is a generator. In particular, any central extension of $\mathfrak{g}[z,z^{-1}]$ is a multiple of the one defined by $\omega$. We have the Chevalley complex
\[
    C^\bullet(\mathfrak{g}[z,z^{-1}]) = \Lambda^\bullet(\mathfrak{g}[z,z^{-1}]^*),
\]
and it carries an action of $\mathfrak{g}[z,z^{-1}]$ which acts trivially on cohomology. In particular, $\mathfrak{g}$ acts trivially on the cohomology.
To see this, observe that the subalgebra $\mathfrak g\subset L$ of constant loops acts on $C^\bullet(L)$ by the usual Lie derivative
$\mathcal L_\xi = \iota_\xi d + d\,\iota_\xi$ (Cartan's homotopy formula), which is chain-homotopic to 0 via the contraction $\iota_\xi$.
Hence $\mathfrak g$ acts trivially on cohomology so every class has a $\mathfrak g$-invariant representative (Lemma \ref{lem:invariant-representative}).

Thus, to classify $H^2(L)$, we may restrict to $\mathfrak g$-invariant 2-cocycles, modulo coboundaries.

\begin{proposition}[Second cohomology of loop algebra]
    Any cohomology class from $H^2(\mathfrak{g}[z,z^{-1}])$ is uniquely represented by a $\mf g$-invariant cocycle.

    Any $\mf g$-invariant 2-cocycle is of the form
    \[
        \sum_{m\neq n} \gamma_{m,n} \omega_{m,n}, \quad
        \gamma_{m,n} = -\gamma_{n,m} \in \mathbb{C}, \qquad
        \omega_{m,n}(xz^k, yz^\ell) = \delta_{m=k,n=\ell} \langle x,y\rangle,
    \]
\end{proposition}

\begin{proof}
    A 1-cochain is a linear functional $\varphi \in L^*$. A coboundary is
    \[
        (d\varphi)(x z^k, y z^\ell) = \varphi([x z^k, y z^\ell]) = \varphi([x,y]z^{k+\ell}).
    \]
    If $\varphi$ is $\mathfrak{g}$-invariant, then its restriction to each $\mathfrak{g}_{r} \simeq \mathfrak{g}$ is a $\mathfrak{g}$-invariant linear functional on the adjoint module $\mathfrak{g}$, which must be zero.

    This is because there are no nonzero invariants in $\mathfrak{g}^*$ for a simple $\mathfrak{g}$. To see this, recall that a linear functional $\lambda \in \mathfrak{g}^*$ is $\mathfrak{g}$-invariant iff
    \[\lambda([x,y]) = 0 \quad \forall x,y \in \mathfrak{g}.\]
    But $[\mathfrak{g},\mathfrak{g}] = \mathfrak{g}$ for simple $\mathfrak{g}$, so $\lambda = 0$.

    Therefore $L^{*\mathfrak{g}} = 0$ so invariant 2-cocycles have no nontrivial invariant coboundaries.

    Now suppose you have two invariant 2-cocycles $\omega,\omega'$ that represent the same cohomology class. That means $\omega - \omega' = d\varphi$ for some $\varphi \in C^1(L)$. Then their difference $d\varphi$ is also $\mathfrak g$-invariant, so $d\varphi$ is an invariant coboundary, which must be zero by the above argument. Hence $\omega = \omega'$, i.e. once we restrict to $\mathfrak{g}$-invariant representatives, the class determines the representative uniquely. This proves the first statement.

    Decompose $L$ as a $\mathfrak g$-module:
    \[
        L=\bigoplus_{k\in\mathbb Z}\mathfrak g_k,
        \qquad \mathfrak g_k\simeq \mathfrak g \ \text{(adjoint)}.
    \]
    Let $\omega\in C^2(L)$ be $\mathfrak g$-invariant. In particular, being a 2-cocycle it is alternating. Fix degrees $k,\ell$. The bilinear map
    \[
        \omega_{k,\ell}\colon \mathfrak g_k\times \mathfrak g_\ell\to\Bbb C,\qquad
        (x z^k, y z^\ell)\mapsto \omega(x z^k, y z^\ell)
    \]
    is $\mathfrak g$-invariant for the diagonal adjoint action, i.e.
    \[
        \omega_{k,\ell}([\xi,x]z^k,\,y z^\ell)+\omega_{k,\ell}(x z^k,\,[\xi,y]z^\ell)=0
        \quad \forall\,\xi\in\mathfrak g.
    \]
    Since $\mathfrak g$ is simple, the space of $\mathfrak g$-invariant bilinear forms on $\mathfrak g\times\mathfrak g$ is one-dimensional and spanned by the fixed invariant form $\langle\ ,\ \rangle$. Therefore there exist scalars $c_{k,\ell}\in\Bbb C$ with \[\omega(x z^k, y z^\ell)=c_{k,\ell}\,\langle x,y\rangle\qquad(k,\ell\in\mathbb{Z})\] Skew-symmetry of $\omega$ gives $c_{k,\ell}=-c_{\ell,k}$. So every $\mathfrak g$-invariant 2-cochain is encoded by an antisymmetric function
    $c\colon \mathbb Z^2\to\Bbb C$.
\end{proof}

This still consists of infinitely many parameters. We will whittle it down using the cocycle condition. 

\begin{proposition}
    We have
    \[
        \gamma_{n,m+p} + \gamma_{m,p+n} + \gamma_{p,n+m} = 0.
    \]
\end{proposition}

\begin{proof}
    We have
    \begin{align*}
        0 & = d\omega(x_1 z^n, x_2 z^m, x_3 z^p)            \\
          & = \omega([x_1,x_2]z^{n+m}, x_3 z^p)
        - \omega([x_1,x_3] z^{n+p}, x_2 z^m)
        + \omega([x_2,x_3] z^{m+p}, x_1 z^n)                \\
          & = \gamma_{m+p,n} \langle [x_1,x_2], x_3 \rangle
        - \gamma_{n+p,m} \langle [x_1,x_3], x_2 \rangle
        + \gamma_{n+m,p} \langle [x_2,x_3], x_1 \rangle.
    \end{align*}

    Since $\langle \ ,\ \rangle$ is totally antisymmetric, all of the $\langle \ ,\ \rangle$ equal the same constant up to sign (note that
    \[
        \langle [x_1,x_3], x_2\rangle = \langle x_1, [x_3,x_2]\rangle
        = -\langle x_1, [x_2,x_3]\rangle
        = -\langle [x_1,x_2], x_3\rangle),
    \]
    hence the above expression is equal to some constant times
    \[
        \gamma_{p,n+m} + \gamma_{m,p+n} + \gamma_{n,m+p},
    \]
    and choosing the constant to be nonzero we have that this is zero.
\end{proof}

\begin{corollary}
    $\gamma_{n,-n} = n\gamma_{1,-1} = n\gamma$ and $\gamma_{m,n} = 0$ otherwise. In particular,
    \[
        \dim H^2(\mathfrak{g}[z,z^{-1}]) = 1.
    \]
\end{corollary}

\begin{proof}
    We have $\gamma_{n,s-n} + \gamma_{m,s-m} = \gamma_{n+m,s-n-m} \implies \gamma_{0,s} = 0$ for all $s$. By induction,
    \[
        -(s-n)\gamma_{1,s-1} = \gamma_{n,s-n} = n \cdot \gamma_{1,s-1} \implies (s-n)\gamma_{1,s-1} = -n\gamma_{1,s-1}
        \implies s \cdot \gamma_{1,s-1} = 0,
    \]
    hence $\gamma_{1,s-1} = 0$ for all $s \neq 0$. This implies the result.
\end{proof}

\begin{definition}[Derived affine Kac-Moody Lie algebra]
    Any central extension of $\mathfrak{g}[z,z^{-1}]$ has the form
    \[
        0 \to \mathbb{C}c \to \widehat{\mathfrak{g}'} \to \mathfrak{g}[z,z^{-1}] \to 0,
        \qquad
    [xz^n, yz^m]_{\widehat{\mathfrak{g}'}} = [x,y]_{\mathfrak{g}} z^{n+m} + n \cdot \delta_{n+m=0} \cdot \langle x,y \rangle \cdot c.
    \]

    Equivalently (terrible abuse of notation since they mean apply the bracket coefficient-wise in the Laurent series),
    \[
        [x(z), y(z)]_{\widehat{\mathfrak{g}'}} = [x(z), y(z)]_{\mathfrak{g}} + \operatorname{Res}_{z=0}\langle dx(z), y(z) \rangle \cdot c.
    \]

    Therefore, we define $\widehat{\mathfrak{g}'}$ associated to a simple (finite-dimensional) Lie algebra $\mathfrak{g}$ to be the unique (up to rescaling) nontrivial central extension of the loop space $\mathfrak{g}[z,z^{-1}]$.
\end{definition}

\begin{remark}
    We will prove that this is the derived subalgebra of an affine Kac-Moody algebra $\widehat{\mathfrak{g}}$ which is obtained by adjoining a derivation $d$ to $\widehat{\mathfrak{g}'}$.
\end{remark}

\begin{example}[Affine $\widehat{\mathfrak{sl}}_2'$]
    Let $\mathfrak{g} = \mathfrak{sl}_2$. We may consider $\widehat{\mathfrak{sl}}_2'$ as a bigraded Lie algebra, with one grading coming from $z\partial_z$, and the other grading coming from $\operatorname{ad} h$.

    Then $\widehat{\mathfrak{sl}}_2'$ is generated by $h, c, e, f, fz, ez^{-1}$. To see this observe that \begin{align*}
        [fz,e] = hz, \quad [hz,e] = 2ez
    \end{align*} and so we have generated $ez$, even though it looks like $fz$ only moves down the $\mf h$-grading. In particular, I'm thinking of $z$ as a left translation and the coefficient $\in \mf g$ as vertical translation.

    In particular we have a triangular decomposition \begin{align*}
        \widehat{\mathfrak{sl}}_2' & = \widehat{\mathfrak{n}}_- \oplus \widehat{\mathfrak{h}} \oplus \widehat{\mathfrak{n}}_+
    \end{align*} where \begin{align*}
        \widehat{\mf h}          & = \mathbb{C}h \oplus \mathbb{C}c                    \\
        \widehat{\mathfrak{n}}_+ & = \mathbb{C}e \oplus z\mathfrak{sl}_2[z]            \\
        \widehat{\mathfrak{n}}_- & = \mathbb{C}f \oplus z^{-1}\mathfrak{sl}_2[z^{-1}].
    \end{align*}
\end{example}

\begin{definition}[Affine Lie algebra]
    The \textbf{affine Lie algebra} $\widehat{\mathfrak{g}}$ associated to $\mathfrak{g}$ is obtained by adjoining a derivation $d$, defined by
    \[
        [d,\, x \otimes z^n] = n\,x \otimes z^n,
        \qquad [d,c]=0.
    \]
    Thus,
    \[
        \widehat{\mathfrak{g}} = L\mathfrak{g} \oplus \mathbb{C}c \oplus \mathbb{C}d.
    \] and \begin{align*}
        0 \to \widehat{\mathfrak{g}} \to \widetilde{\mathfrak{g}} \to \mathbb{C}d \to 0.
    \end{align*}
\end{definition}

\section{Real and imaginary roots}
In this chapter we give an explicit description of the root system of a Kac-Moody algebra $\mf g(A)$. Our main instrument is the notion of an
imaginary root, which has no counterpart in the finite dimensional theory.

\begin{definition}[Real root]
    A root $\alpha \in \Delta$ is called \textbf{real} if there exists $w \in W$ such that $w(\alpha)$ is a simple root. Otherwise, $\alpha$ is called \textbf{imaginary}.
\end{definition}
Denote by $\Delta^{\mathrm{re}}$ and $\Delta^{\mathrm{re}}_+$ the sets of all real and positive real roots respectively.

Let $\alpha \in \Delta^{\mathrm{re}}$; then $\alpha = w(\alpha_i)$ for some $\alpha_i \in \Pi$, $w \in W$. Define the \emph{dual (real) root} $\alpha^\vee \in \Delta^{\vee\,\mathrm{re}}$ by $\alpha^\vee = w(\alpha_i^\vee)$. This is independent of the choice of the presentation $\alpha = w(\alpha_i)$. Indeed, we have to show that the equality $u(\alpha_i) = \alpha_j$ implies $u(\alpha_i^\vee) = \alpha_j^\vee$ for $u \in W$. But we showed this earlier. Thus we have a canonical $W$-equivariant bijection $\Delta^{\mathrm{re}} \to \Delta^{\vee\,\mathrm{re}}$. By an easy induction on $\mathrm{ht}\,\alpha$, one shows, using Proposition \ref{prop:real-roots}e below, that $\alpha > 0$ if and only if $\alpha^\vee > 0$.

We define a reflection $r_\alpha$ with respect to $\alpha \in \Delta^{\mathrm{re}}$ by
\[
    r_\alpha(\lambda) = \lambda - \langle \lambda, \alpha^\vee\rangle \alpha, 
    \qquad \lambda \in \mathfrak{h}^*.
\]
Since $\langle \alpha,\alpha^\vee\rangle = 2$, this is a reflection, and since 
$r_\alpha = w r_i w^{-1}$ if $\alpha = w(\alpha_i)$, it lies in $W$. Note that $r_\alpha$ is the fundamental reflection $r_i$.

\begin{proposition}[Properties of real roots]\label{prop:real-roots}
Let $\alpha$ be a real root of a Kac--Moody algebra $\mathfrak{g}(A)$. Then:
\begin{enumerate}[label=\alph*)]
    \item $\mathrm{mult}\,\alpha = 1$.
    \item $k\alpha$ is a root if and only if $k = \pm 1$.
    \item If $\beta \in \Delta$ then there exist nonnegative integers $p$ and $q$ related by the equation
    \[
        p - q = \langle \beta, \alpha^\vee \rangle,
    \]
    such that $\beta + k\alpha \in \Delta \cup \{0\}$ if and only if $-p \leq k \leq q$, $k \in \mathbb{Z}$.
    \item Suppose that $A$ is symmetrizable and let $(.|.)$ be a standard invariant bilinear form on $\mathfrak{g}(A)$. Then
    \begin{enumerate}[label=(\roman*)]
        \item $(\alpha|\alpha) > 0$.
        \item $\alpha^\vee = 2\nu^{-1}(\alpha)/(\alpha|\alpha)$.
        \item If $\alpha = \sum_i k_i \alpha_i$, then $k_i(\alpha_i|\alpha_i) \in (\alpha|\alpha)\mathbb{Z}$.
    \end{enumerate}
    \item Provided that $\pm \alpha \notin \Pi$, there exists $i$ such that
    \[
        |\mathrm{ht}\, r_i(\alpha)| < |\mathrm{ht}\,\alpha|.
    \]
\end{enumerate}
\end{proposition}

\begin{proof}
All the statements a)--d) are clear if $\alpha$ is a simple root. Now a), b), and c) follow from the fact that the action of the Weyl group preserves root system and root multiplicities \ref{prop:Weyl-invariance}.

Statements d)(i) and (ii) follow from the fact that the restriction of the bilinear form to $\mf h^*$ is $W$-invariant. Statement d)(iii) follows from the fact that $\alpha^\vee \in \sum \mathbb{Z}\alpha_i^\vee$ and the following formula:
\[
    \alpha^\vee = \sum_i \frac{(\alpha_i|\alpha_i)}{(\alpha|\alpha)}\, k_i \alpha_i^\vee. \tag{5.1.1}
\] This formula follows from the identities \begin{align*}
    \nu^{-1}(\alpha_i) &= \frac{(\alpha_i|\alpha_i)}{2} \, \alpha_i^\vee \\
    \nu^{-1}(\alpha) = \sum_i k_i \nu^{-1}(\alpha_i).
\end{align*} because $\alpha$ is a $\Z$-linear combination of simple roots $\alpha_i$ and apply $\nu^{-1}$ to both sides.

Finally, suppose the contrary to e); we may assume that $\alpha > 0$. But then $-\alpha \in C^\vee$ and, by Proposition 3.12d) for the dual root system, $-\alpha + w(\alpha) \geq 0$ for any $w \in W$. Taking $w$ such that $w(\alpha) \in \Pi$, we arrive at a contradiction.
\end{proof}

\section{Meetings}
\subsection*{Questions (Sept 17)}
\begin{enumerate}[A.]
    \item I've been thinking about the hierarchy of objects: $L\mathfrak g$ $\to$ central extensions $\to$ full affine Kac–Moody with $d$. Each step seems to be motivated by making the structure better behaved for representation theory.
        \begin{enumerate}[a)] 
            \item The loop algebra $L\mathfrak g$ is already a perfectly good infinite-dimensional Lie algebra, realized geometrically as a dense subalgebra of the Lie algebra of the loop group $LG$. Why isn't it sufficient to study its representation theory directly? In what sense are central extensions 'forced upon us' if we want a rich representation theory?
            \item I see that the affine Kac–Moody algebra arises as the universal central extension of the loop algebra. Is there a structural reason why the extension must be central and abelian, rather than by some nonabelian ideal? In other words, what uniquely characterizes the central extension as the 'correct' enlargement of $L\mathfrak g$ for representation theory?
            \item Once we accept the central extension, we typically adjoin the degree derivation $d$, defined by $[d, x\otimes z^n]=n\,x\otimes z^n$. Should I think of $d$ as a geometric object, namely the vector field $z\frac{d}{dz}$ on the circle? Is the introduction of $d$ mathematically necessary (e.g. for a nondegenerate invariant bilinear form, or for grading representations), or is it mainly motivated by geometric/physical considerations?
        \end{enumerate}
    \item I know that algebraically the invariant form defines the 2-cocycle, hence the central extension and the line bundle on $\mathrm{Gr}_G$. But is there a more geometric explanation of why the bilinear form is the key ingredient? For instance, can one see directly from the geometry of $G$-bundles or determinant line bundles on curves that the form $(\cdot,\cdot)$ is exactly what controls the Picard group of the affine Grassmannian? For example, what exactly does the level of a line bundle correspond to geometrically?
    \item I understand that the level (the eigenvalue of the central element) is a key invariant of representations of affine Kac-Moody algebras. But where exactly does it start to matter? The line bundle on $\mathrm{Gr}_G$ has a level, and this level determines which representation we get. But is there a more algebraic way to see why the level is important? For instance, if we just look at highest weight modules for $\widehat{\mathfrak g}$, how does the level affect their structure?
\end{enumerate}
\subsection*{Answers (Sept 17)}
Professor Borcherds tends to give short answers. First one sees from the combinatorics of Dynkin diagramms that the central extension of the loop algebra is forced upon us. 

He had a lot to say about modular forms and theta functions, in particular the denominator of affine Kac-Moody algebras associated to $\mf g$ is a theta function, which is something similar to a modular form. In fact, he said "theta functions are nothing but modular forms." I asked about theta divisors which can be thought of as square roots of the canonical bundle, and he made a funny joke. He said "Ah yes, God introduced square roots because there's a lot of sign errors floating around, $2^n$ many of them." Then he let me borrow his copy of a book and referred me to Mumford's three volumes on theta functions and abelian varieties.
\section{Appendix A: Lie algebra cohomology}

\begin{lemma}[Invariant representative via averaging]\label{lem:invariant-representative}
    Let $K$ be a compact Lie group acting on a cochain complex $(C^\bullet,d)$ by
    cochain maps (so $k\cdot d = d\,(k\cdot)$ for all $k\in K$). Suppose the induced
    $K$-action on cohomology $H^\bullet(C^\bullet)$ is trivial. Then for every
    cohomology class $[c]\in H^p(C^\bullet)$ there exists a $K$-invariant cocycle
    $c^{\mathrm{av}}\in C^p$ with $dc^{\mathrm{av}}=0$ and $[c^{\mathrm{av}}]=[c]$.
    Equivalently, the inclusion $i:(C^\bullet)^K\hookrightarrow C^\bullet$
    induces a surjection $H^\bullet((C^\bullet)^K)\twoheadrightarrow H^\bullet(C^\bullet)$.
\end{lemma}

\begin{proof}
    Let $\mu$ be the Haar probability measure on $K$. For $c\in C^p$ define the
    \emph{averaging operator}
    \[
        P(c)\;:=\;\int_{K} k\cdot c\; d\mu(k)\ \in C^p.
    \]
    This Bochner integral is well-defined because $C^p$ is a (Hausdorff) complex
    vector space and $k\mapsto k\cdot c$ is continuous.

    \smallskip

    \emph{(1) $P$ is a cochain map and projects onto invariants.}
    For any $c\in C^p$,
    \[
        d\,P(c)\;=\;\int_K d(k\cdot c)\,d\mu(k)\;=\;\int_K k\cdot (dc)\,d\mu(k)\;=\;P(dc),
    \]
    so $dP=Pd$. Moreover, for any $k_0\in K$,
    \[
        k_0\cdot P(c)\;=\;\int_K (k_0k)\cdot c\,d\mu(k)\;=\;\int_K k\cdot c\,d\mu(k)\;=\;P(c),
    \]
    using left-invariance of $\mu$. Hence $P(c)\in (C^p)^K$. If $c$ is already
    $K$-invariant then $P(c)=c$, so $P$ is a projection onto $(C^\bullet)^K$.

    \smallskip

    \emph{(2) $P$ preserves cocycles.}
    If $dc=0$ then $dP(c)=P(dc)=0$, so $c^{\mathrm{av}}:=P(c)$ is again a cocycle.

    \smallskip

    \emph{(3) $P$ preserves cohomology classes when the $K$-action on cohomology is trivial.}
    By assumption, $[k\cdot c]=[c]$ in $H^p(C^\bullet)$ for every $k\in K$. Using linearity
    of cohomology,
    \[
        [c^{\mathrm{av}}]\;=\;\Big[\int_K k\cdot c\,d\mu(k)\Big]
        \;=\;\int_K [\,k\cdot c\,]\,d\mu(k)
        \;=\;\int_K [c]\,d\mu(k)
        \;=\;[c].
    \]
    Thus $c^{\mathrm{av}}$ is a $K$-invariant cocycle representing the same class as $c$.

    Therefore every class in $H^p(C^\bullet)$ has a $K$-invariant representative,
    and the inclusion of invariants induces a surjection on cohomology.
\end{proof}


\section{References}
\begin{enumerate}
    \bibitem{bump} Bump, D. \textbf{Weyl Character Formula notes}, available at \url{http://sporadic.stanford.edu/Math210C/WCF.pdf}.

    \bibitem{cai} Cai, M. \textbf{Infinite-dimensional Lie algebras}, available at \url{https://merrickcai.com/pdfs_notes/747.pdf}.

    \bibitem{kac} Kac, V. G. \textbf{Infinite Dimensional Lie Algebras}. Cambridge University Press, 1990.
    \bibitem{pressley-segal} Pressley, A., and Segal, G. \textbf{Loop Groups}. Oxford University Press, 1986.
\end{enumerate}
\end{document}