\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{bookmark}
\usepackage{tikz}
\usepackage{/Users/songye03/Desktop/math_tex/style/quiver}
\usepackage{/Users/songye03/Desktop/math_tex/style/scribe}
\usepackage{fancyhdr}

\usepackage{parskip} % Automatically respects blank lines
\setlength{\parskip}{1em} % Adds more space between paragraphs
\setlength{\parindent}{0pt} % Removes paragraph indentation

\begin{document}


\lhead{Songyu Ye}
\rhead{\today}
\cfoot{\thepage}

\title{Interview Preparation}

\author{Songyu Ye}
\date{\today}
\maketitle


\begin{abstract}
    These are notes for my final round interview for the Jane Street Quantitative Trading Internship 2025.
\end{abstract}

\tableofcontents

\section{Practice Problems}
We start with a bankroll of 100 dollars. We want to maximize our winnings from each game while strongly minimizing the risk of ruin. In particular, one should think very carefully about bet sizing.

\begin{example}
    You and I are playing a game. We will draw four cards form a deck of standard playing cards at random. The payout of the game is equal to $5^n$ dollars, where $n$ is the number of red cards.

    Using a binomial distribution, one can roughly estimate the payout of the game to be 81 dollars. We decide to make a 10 wide market on the game. In particular, I will buy the game from you for 86 dollars and sell it to you for 76 dollars.

    Let's suppose that I buy one contract. So now you have 186 dollars. You are potentially on the hook to lose everything if I hit 4 red cards. But luckily there is a side bet. Before the first card and each subsequent card is flipped over, you are allowed to take a 1:1 side bet on whether the card is red or black. Find an optimal strategy to minimize your risk of ruin.

    At the beginning of the game, suppose the first card is red. Then 1/8 of the time, you will owe me 625 dollars at the end of the game. 3/8 of the time, you will owe me 125 dollars at the end of the game. 3/8 of the time, you will owe me 25 dollars at the end of the game. 1/8 of the time, you will owe me 5 dollars at the end of the game. Therefore, your expected loss is 135 dollars.

    Conversely, if the first card is black, then 1/8 of the time, you will owe me 125 dollars at the end of the game. 3/8 of the time, you will owe me 25 dollars at the end of the game. 3/8 of the time, you will owe me 5 dollars at the end of the game. 1/8 of the time, you will owe me 1 dollar at the end of the game. Therefore, your expected loss is 27 dollars.

    Therefore, you should take (135 - 27)/2 = 54 dollar sidebet on red for the first card to minimize your expected loss in the worst case scenario.

    Now let's suppose the first card comes red. You now have 186 + 54 = 240 dollars. Then let's reason about how to hedge moving forward. If you hit red next, then you owe 25 dollars 1/4 of the time, 125 dollars 1/2 of the time, and 625 dollars 1/4 of the time. Therefore, your expected loss is 225 dollars. If you hit black next, then you owe 5 dollars 1/4 of the time, 25 dollars 1/2 of the time, and 125 dollars 1/4 of the time. Therefore, your expected loss is 45 dollars. Therefore, you should take the side bet of (225 - 45)/2 = 90 dollars on red to minimize your expected loss in the worst case scenario.

    Say the next card comes black. You now have 240 - 90 = 150 dollars. Then let's reason about how to hedge moving forward. If I hit red next, then half the time you will owe me 25 dollars and the other half of the time you will owe me 125. If I hit black next, then half of the time I owe you 5 dollars and half the time you owe me 25 dollars. Therefore, if I hit red, your expected loss is 75 dollars, and if I hit black, your expected loss is 15 dollars. Therefore, you should take the side bet of 30 dollars on red to minimize your expected loss in the worst case scenario.

    Say the third card comes black. At this point, you have 150 - 30 = 120 dollars. If the last card comes red, you owe me 25 dollars, and if the last card comes black, you owe me 5 dollars. Therefore, you should take a side bet of (25 - 5)/2 = 10 dollars on red to minimize your expected loss in the worst case scenario. No matter what happens, you will lose 15 dollars. So you end up with 105 dollars no matter what happens. This is the 5 dollars that comes from the bid-ask spread. So you have hedged away all of the risk of ruin while still making a profit.
\end{example}

Key takeaways from the game:
\begin{itemize}
    \item Always think about worst case scenarios and try to minimize your expected loss in the worst case scenario.
    \item Don't just pick sizes blindly. Think about how your bet sizes affect your risk of ruin.
    \item Use side bets to hedge your risk.
    \item Rule of thumb: Don't risk more than 40\% of your bankroll on a single bet, unless the odds are heavily in your favor.
\end{itemize}

\begin{example}
    We are playing with a 40 card deck of cards numbered 1 to 10. A player will draw four cards from the deck. The value of the game is the sum of the values of the red cards.

    One can estimate the expected value of the game to be 11 dollars. We decide to make a 10 wide market on the game. In particular, I will buy the option to play the game from you for 16 dollars and sell it to you for 6 dollars. There is a 1:1 sidebet where you can bet on whether the next card drawn is $\geq 6$ or $< 6$. Find an optimal sidebet strategy. With this sidebet strategy, how many contracts should you sell?

    Before any cards are drawn, we can consider four situations. Since you lose more on larger numbers, your sidebet should be biased towards larger numbers. Suppose you bet $x$ dollars on the next card being $\geq 6$.
    \begin{itemize}
        \item If the next card is black and $\geq 6$, then you win $x$ dollars from the sidebet.
        \item If the next card is black and $< 6$, then you lose $x$ dollars from the sidebet.
        \item If the next card is red and $\geq 6$, then you win $x - 8$ dollars from the sidebet. Note that $8$ is the average value of the red cards $\geq 6$.
        \item If the next card is red and $< 6$, then you win $-x - 3$ dollars from the sidebet.
    \end{itemize}
    We want to use the sidebet to minimize our expected loss in the worst case scenario. Therefore, we want to solve the following equation:
    \[x - 8 = -x - 3.\]
    Solving this equation gives $x = 2.5$. Therefore, we should bet 2.5 dollars on the next card being $\geq 6$. Then the two worst case scenarios for us are losing 5.5 dollars. Since each sidebet is independent (the removed cards are negligible), we do not have to dynamically adjust our sidebet sizes.

    Now we calculate how many contracts we should sell. The worst case scenario is that I pick four red 10s or four red 5s. In this case, you will owe me 40 dollars from the main bet, win 10 dollars from the sidebets, and make 16 dollars from selling the contract. Therefore, your total loss is 14 dollars. Since you start with 100 dollars, you can sell at most 7 contracts. However you should only sell 3 contracts to be safe, abide by the rule of not risking more than 40\% of your bankroll on a single bet.
\end{example}
Key takeaways from the game:
\begin{itemize}
    \item Use expected values (like 8, 3) within branches to summarize future payoffs.
    \item Equalize losses between branches to flatten your worst-case profile.
    \item “Minimizing expected loss in the worst case on average.”
    \item Reason about the game before you decide how many contracts to sell.
\end{itemize}

\begin{example}

    \textbf{2. Sequential coin trade}

    A hidden sequence of three fair coin flips will determine the payout $2^{\text{\#H}}$. Player A sees the first coin; Player B sees the second; both know the rules. A posts a bid/ask for the whole contract; B decides to hit or pass before seeing the third coin. Find sub-game-perfect equilibrium pricing. (Tests backward induction with asymmetric information.)

    \textbf{3. Dynamic price quoting}

    You must quote a price each round for an asset that moves $\pm1$ with equal probability for 4 rounds. At each round, an adversary can buy or sell 1 unit against your quote. Payout equals final price. Find the quoting rule that minimizes worst-case loss given you can't refuse trades. (Tests game-theoretic minimax / option-market-making intuition.)

    \textbf{4. Variance market}

    Underlying $X\in\{-1,1\}$ each day, independent. Over 3 days, payoff $= (\sum X_i)^2$. Player A sells variance for price $p$. Player B can dynamically trade 1:1 in $X_i$ each day. Find the no-arbitrage value of the variance contract. (Tests replication of quadratic payoffs; the answer is 3.)

    \textbf{5. Information auction}

    A random variable $R \in \{0, 1, 2\}$ is drawn uniformly. A knows $R$, B doesn't. A offers to sell B a contract paying $R^2$ for a price $x$. B can accept or reject. Compute A's optimal ask price and B's rational cutoff. (Tests signaling / adverse selection equilibrium.)

    \textbf{6. Permuted cards}

    Deck of four cards $\{1, 2, 3, 4\}$. Player A draws two at random and keeps their sum $S$. Player B sees only that $S \in \{3,\ldots,7\}$. A offers to sell B the contract paying $S^2$ for price $x$. B must guess whether to buy or short. Find equilibrium price (both mixed). (Tests expectation under constrained information.)

    \textbf{7. Correlated assets}

    Two correlated binary assets A,B with
    \[\Pr(A=1,B=1)=0.4, \Pr(A=1,B=0)=0.1, \Pr(A=0,B=1)=0.1, \Pr(A=0,B=0)=0.4.\]
    Payout = $A \times B \times 100$. One player sees A before quoting a price; the other sees B. Find equilibrium price when both can post bid/ask sequentially. (Tests correlation intuition \& conditioning.)

    \textbf{8. Inventory penalty game}

    You quote two-sided prices around an asset that moves $\pm1$ each round (like a random walk). Opponent trades 1 share per round to maximize your expected inventory cost (quadratic in position). Find the optimal spread as a function of volatility and penalty coefficient $\lambda$. (Tests dynamic programming and inventory control.)

    \textbf{9. Forecasting competition}

    A hidden number $N\in\{0,1,2,3,4\}$ drawn uniform. Players simultaneously post forecasts $f_A,f_B$. Whoever's closer ($|f-N|$ smaller) wins $\$1$; if tied, split. Each can see a private noisy signal $N+\varepsilon_i$ ($\varepsilon_i \in \{-1,0,1\}$). Find Bayesian best responses. (Tests signal weighting and equilibrium strategy.)

        \textbf{10. Exploding payoff}

        Payout = $5^n$ dollars where $n$ = number of reds in four cards, but after any card you may cancel the game by paying $\$50$. Two players alternate control: A decides to continue or cancel on odd turns, B on even turns. Find equilibrium stopping thresholds. (Tests dynamic stopping games \& expected value reasoning.)


\end{example}
\begin{example}

    You roll a fair six-sided die once. The payout is $2^{X}$ dollars, where $X$ is the number on the die. Before the roll, you may trade a 1:1 side bet on whether the result will be even or odd.

    The fair value is $\frac{2^1 + 2^2 + 2^3 + 2^4 + 2^5 + 2^6}{6} \approx 21.33$ dollars. Suppose the contract is trading at 15 25 dollars.

    The optimal hedge is to bet $\frac{(2^6 + 2^4 + 2^2) - (2^5 + 2^3 + 2^1)}{2} = 21$ dollars on even. Then if the opponent rolls even, on average you lose them $\frac{2^6 + 2^4 + 2^2}{3} - 21 = 7$ dollars; if they roll odd, on average you lose them $\frac{2^5 + 2^3 + 2^1}{3} + 21 = 35$ dollars. The expected loss is $\frac{7 + 35}{2} = 21$ dollars.

    In the worst case scenario, if you sell the contract for 25 dollars, you might lose 64 dollars, but make 25 + 21 = 46 dollars from selling and hedging, for a net loss of 18 dollars. This means you can sell 2 contracts with this hedge before risking more than 40\% of your bankroll.

\end{example}
\begin{example}
    \textbf{2. Coin chain with quadratic payoff}

    You flip 3 fair coins. Payout equals $10(\text{\# heads})^2$. Before each flip, you can bet 1:1 on the next coin's outcome.

    The fair price is $10(0^2\cdot\frac{1}{8} + 1^2\cdot\frac{3}{8} + 2^2\cdot\frac{3}{8} + 3^2\cdot\frac{1}{8}) = 30$ dollars. However, if you sell a contract for 30 dollars, you can lose up to 90 dollars if all coins are heads. You can hedge dynamically as follows:

    Before the first flip, if the first coin is heads, then you have a 1/4 chance of owing 90 dollars, a 1/2 chance of owing 40 dollars, and a 1/4 chance of owing 10 dollars. Expected loss is 45 dollars. If the first coin is tails, then you have a 1/4 chance of owing 40 dollars, a 1/2 chance of owing 10 dollars, and a 1/4 chance of owing 0 dollars. Expected loss is 15 dollars. Therefore, you should bet (45 - 15)/2 = 15 dollars on heads for the first flip.

    So say the first coin is heads. You now have 115 dollars. Before the second flip, if the second coin is heads, then you have a 1/2 chance of owing 90 dollars and a 1/2 chance of owing 40 dollars. Expected loss is 65 dollars. If the second coin is tails, then you have a 1/2 chance of owing 40 dollars and a 1/2 chance of owing 10 dollars. Expected loss is 25 dollars. Therefore, you should bet (65 - 25)/2 = 20 dollars on heads for the second flip.

    So say the second coin is tails. You now have 115 - 20 = 95 dollars. Before the third flip, if the third coin is heads, then you owe 40 dollars; if the third coin is tails, then you owe 10 dollars. Therefore, you should bet (40 - 10)/2 = 15 dollars on heads for the third flip.

    Finally, say the third coin is tails. You now have 95 - 15 = 80 dollars. No matter what happens, you owe 10 dollars. So you end up with 70 dollars no matter what happens. However, you sold the contract for 30 dollars, so you broke exactly even.

    Proceeding this way, you see that with optimal hedging, even if you get all heads, you will always break even. Therefore, if you sell at a spread, you can sell as many contracts as you want without risking ruin.
\end{example}
\begin{example}


    \begin{example}
    There are two independent fair coins, A,B. Payout is $100$ if both are heads, $0$ otherwise. You can make 1:1 bets on each coin's outcome.

    Say the first coin is heads. Then you owe 100 dollars with probability 1/2 and 0 dollars with probability 1/2. Expected loss is 50 dollars. If the first coin is tails, then you owe 0 dollars no matter what. Therefore, you should bet (50 - 0)/2 = 25 dollars on heads for the first coin. 

    So we flip the first coin. Say it comes up heads. You now have 125 dollars. Then for the second coin, if it is heads, you owe 100 dollars; if it is tails, you owe 0 dollars. Therefore, you should bet (100 - 0)/2 = 50 dollars on heads for the second coin.

    Say the second coin comes up tails. You now have 125 - 50 = 75 dollars. So you end up with 75 dollars no matter what happens. If the second coin came up heads, you would make 50 dollars from the sidebet and pay 100 dollars from the main bet, for a net loss of 25 dollars. So you would end up with 75 dollars no matter what happens. So we see that hedging allows us to realize a sure final amount of 75 dollars. Therefore, if you sell the contract for more than 25 dollars, you can make a guaranteed profit.
    \end{example}
    \textbf{4. Sequential urn draw}

    Urn starts with 4 red and 4 black balls. We draw 3 balls without replacement. Payout equals $50$ times the number of reds. Before each draw, you can place a 1:1 bet on "next ball = red".

    The fair price is $150$ dollars. For hedging, bet $\frac{50r(8-r)}{2(r+b)}$ dollars on red before each draw, where $r,b$ are remaining red and black balls. Selling for $160$ guarantees a $10$ dollar profit with proper hedging.
\end{example}
\begin{example}

    Biased coin with $P(\text{heads})=0.6$. We flip twice with payout $5^{\text{\# heads}}$. You can only hedge on the first flip. The payout of the hedge is $(1-.6)/.6 = 2/3$ dollars heads, $-1$ dollar tails.

    The fair price is $0.6(0.6\cdot25 + 0.4\cdot5) + 0.4(0.6\cdot5 + 0.4\cdot1) = 11.56$ dollars.
    
    If you flip heads first, you owe $25$ with probability $0.6$ and $5$ with probability $0.4$. Expected loss is $17$ dollars. If you flip tails first, you owe $5$ with probability $0.6$ and $1$ with probability $0.4$. Expected loss is $3.4$ dollars. But since heads is going to happen $60\%$ of the time, you should weight the two scenarios accordingly. Explicitly, you should solve the following equation: 
    \[ 17 - \frac{2}{3}x = 3.4 + x\]
because we want the expected loss to be equal in both scenarios. Solving this equation gives $x = 8.16$. 
\end{example}

\begin{example}
    Consider a game with 2 dice, where you cannot see the opponent's die. You can take your own die value, your opponent's die value, or defer by paying 20 cents. What is your optimal strategy?

    Assume this model: each round you roll your die and see $x\in\{1,\dots,6\}$; the opponent rolls a hidden die. You may (1) take $x$, (2) take the opponent's hidden value (mean 3.5), or (3) pay $c=0.2$ to defer and reroll both dice, with the same choices again. Repeat allowed.

    Let $V$ be the optimal expected value before seeing a roll. After paying to defer you return to the same state, so the "continue" value is $V-c$. Given a seen $x$, the stop value is $S(x)=\max(x,3.5)$. Optimal choice at $x$ is $\max\{S(x),V-c\}$. Therefore:
    \[V=\mathbb E_x\big[\max\{S(x),V-c\}\big]=\frac{1}{6}\sum_{x=1}^6 \max\{\max(x,3.5),T\}\]
    where $T:=V-c$.

    For any optimal solution we must have $T\geq 3.5$ (otherwise you would never defer). Work piecewise on $T$:

    If $3.5\leq T<4$:
    \[V=\frac{1}{6}(3T+4+5+6)=0.5T+2.5\]
    Set $T=V-0.2$ gives $V=4.8\Rightarrow T=4.6$, which contradicts $T<4$. Discard.

    If $4\leq T<5$:
    \[V=\frac{1}{6}(3T+T+5+6)=\frac{2}{3}T+\frac{11}{6}\]
    Set $T=V-0.2$:
    \[V=\frac{2}{3}(V-0.2)+\frac{11}{6}\Rightarrow V=5.1,\quad T=V-0.2=4.9\]
    which is consistent with $4\leq T<5$.

    Hence the fixed point is $V=5.1$ and $T=4.9$.

    Decision rule: Defer (pay 0.2) if $S(x)=\max(x,3.5)<4.9$, i.e., for $x\in\{1,2,3,4\}$. Stop if $x\in\{5,6\}$. When stopping, take your own die (5 or 6 beats the hidden die's 3.5).

    So the optimal strategy is: pay to defer when your die is 1-4; take your die when it is 5 or 6; never choose the hidden die directly. The optimal expected value from the start is $V=5.1$ (in the same units as the die; if die are dollars, that's $5.10$).

    In particular, deferral is always worth 4.9 regardless of how many times you've deferred before, since the state resets. So it is never optimal to tkae the hidden die directly. 
\end{example}

\begin{example}
    Flip 4 fair coins and note the parity of each. You are allowed to reroll any instance of TT. You cannot flip one coin at a time; you must reroll both coins together. What is the expected number of Hs at the end?

    Let $t$ be the number of tails and $a_t$ be the probability of ending with $0$ tails starting from $t$ tails. The base cases are $a_0=1$ (already all heads). Then $a_1=0$ and $a_2 = \frac{1}{4} + \frac{1}{4}a_2$ so $a_2 = 1/3$. Also $a_3 = 1/4 a_1 + 1/4 a_3 + 1/2 a_2 = 1/4 a_3 + 1/6$ so $a_3 = 2/9$. Finally, $a_4 = 1/4 a_2 + 1/4 a_4 + 1/2 a_3 = 1/4 a_4 + 1/12 + 1/9$ so $a_4 = 7/27$.
    
The starting number of tails is distributed binomially with $n=4$ and $p=1/2$. Therefore, the expected number of heads at the end is
\[
a_0/16 + 4a_1/16 + 6a_2/16 + 4a_3/16 + a_4/16 = 7/27\]
The expected number of heads is $4 * 7/27 + 3 * (1 - 7/27) = 88/27 \approx 3.26$.
\end{example}

\begin{example}
Flip 4 fair coins and notes each outcome. After seeing them, he may flip any pair of tails again (i.e., reroll those two coins). He continues as long as he has fewer than 3 heads—once he has 3 or 4 heads, there's no TT pair, and the game stops. Each coin flip counts individually (so each pair reroll adds 2 to the flip count). Find the expected total number of flips, including the first 4.

Let $E[h]$ = expected number of individual flips remaining (not counting the initial 4) starting with $h$ heads.

If you have $h \geq 3$, stop: $E[3] = E[4] = 0$.

Otherwise:
\[
E[h] = 2 + \frac{1}{4} E[h] + \frac{1}{2} E[h+1] + \frac{1}{4} E[h+2]
\] Now we can solve the system:
\begin{align*}
E[2] & = 2 + \frac{1}{4} E[2] + \frac{1}{2} \cdot 0 + \frac{1}{4} \cdot 0 \Rightarrow E[2] = \frac{8}{3} \\
E[1] & = 2 + \frac{1}{4} E[1] + \frac{1}{2} \cdot \frac{8}{3} + \frac{1}{4} \cdot 0 \Rightarrow E[1] = \frac{40}{9} \\
E[0] & = 2 + \frac{1}{4} E[0] + \frac{1}{2} \cdot \frac{40}{9} + \frac{1}{4} \cdot \frac{8}{3} \Rightarrow E[0] = \frac{176}{27}
\end{align*}
and so the total expected number of flips is
\[
\frac{1}{16} E[0] + \frac{4}{16} E[1] + \frac{6}{16} E[2] = 68/27\] and the final answer is $4 + 68/27 = 176/27 \approx 6.52$ flips.
\end{example}

\begin{example}
    [Coupon collector with 4 coupons] 
    We start with no coupons. Let $k$ be the number of distinct coupons we have collected so far. The probability of getting a new coupon is $\frac{4-k}{4}$. Therefore, the expected number of trials to get a new coupon is $\frac{4}{4-k}$. Therefore, the expected number of trials to collect all 4 coupons is
    \[
        \frac{4}{4} + \frac{4}{3} + \frac{4}{2} + \frac{4}{1} = \frac{25}{3}\]
        The same method can be used to solve the general coupon collector problem with $n$ coupons. The expected number of trials to collect all $n$ coupons is $nH_n$, where $H_n$ is the $n$-th harmonic number.
\end{example}

\begin{example}
    Take a deck of cards numbered 1 to 10, with 4 copies of each card (40 cards total). Three cards are dealt face-down on the table each round. The game presents a market for the sum of the cards face values (each card is 1-10 with equal probability). Say the market is 15 for 100, 100 at 25. How much would you pay to look at one card before deciding to buy/sell, assuming that you have to trade?

    Contract is linear on the sum $S$ of the three cards. Bid/ask is 15-25, so with no info you'd sell at 15. Baseline P/L: $15-\mathbb{E}[S]=15-16.5=-3/2$.

    If we peek one card $X=x$, the two unseen cards are drawn from the remaining 39 cards whose mean is $(220-x)/39$. Therefore:
    \[
    \mu_x:=\mathbb{E}[S|X=x]=x+2\frac{220-x}{39}=\frac{440+37x}{39}
    \]

    For optimal action given $x$: buy if $\mu_x>20$ (mid), else sell. Solving $(440+37x)/39>20$ gives $x>340/37$. So only $x=10$ triggers a buy; for $x\in\{1,\dots,9\}$ you sell.

    The expected P/L with the signal is:
    \[
    \frac{1}{10}\Bigg[\sum_{x=1}^{9}\big(15-\mu_x\big)+\big(\mu_{10}-25\big)\Bigg]=-\frac{35}{26}
    \]

    Therefore the value of the peek, which is the improvement over baseline, is:
    \[
    -\frac{35}{26}-\Big(-\frac{3}{2}\Big)=\frac{2}{13}\approx0.1538
    \] For 100 contracts, the value is 15.38 units.
\end{example}

\begin{example}
    Suppose you have a bankroll of 100 dollars. Take a deck of cards numbered 1 to 10, with 4 copies of each card (40 cards total). Three cards are dealt face-down on the table each round. The game presents a market for the sum of the cards face values (each card is 1-10 with equal probability). Say the market is 11.5 for 5, 5 at 21.5. How much would you pay to look at one card before deciding to buy/sell, assuming that you have to trade?

    Per unit, the peek is worth $\frac{185}{78} \approx 2.372$ dollars.

    Let $S$ be the sum of the three cards. With one peek revealing $X=x$, we have
    \[
    \mu_x := \mathbb{E}[S|X=x] = x + 2\frac{220-x}{39} = \frac{440+37x}{39}
    \]

    The market is 11.5-21.5 (center 16.5, half-width 5). If you must trade one unit, you buy if $\mu_x > 16.5$ and sell if $\mu_x < 16.5$. Your loss per unit after seeing $x$ is the distance from $\mu_x$ to the nearer edge:
    \[
    \text{loss}(x) = \min(\mu_x-11.5,\ 21.5-\mu_x) = 5-|\mu_x-16.5|
    \]

    Without a peek the loss is always 5 per unit, so the value of the peek is
    \[
    \mathbb{E}[|\mu_X-16.5|] = \frac{1}{10}\sum_{x=1}^{10}\left|\frac{440+37x}{39}-\frac{33}{2}\right| = \frac{185}{78}
    \]

    If you can trade multiple units subject to your $100 $bankroll and the posted sizes (5 at each side), the optimal sizing is: sell up to 5 units when $x \leq 5$; buy up to 4 units when $x \geq 6$ (cannot afford $5 \times 21.5$). Then the peek's total value (vs. best no-peek choice, which is buying 4 to lose $20$) is $\frac{555}{52} \approx 10.67$. If instead you must trade exactly one unit, use the per-unit value $\frac{185}{78}$.

    Suppose the peek is mispriced and u have to pay 2 dollars per contract to peek. Suppose that the volume can be whatever you want. How many contracts would you trade subject to the condition you can't go bankrupt?

    4 contracts.

The worst-case loss per contract equals market loss of $18.5$ (since $S\in[3,30]$ and you must trade at $11.5/21.5$) plus peek fee of $2$, totaling $20.5$ per contract.

With a bankroll of $100$, we must satisfy $Q\cdot 20.5 \leq 100$ where $Q$ is the number of contracts, which implies $Q \leq 4.87$. Since we need an integer number of contracts, this means $Q=4$.

While the peek reduces expected loss per unit by $\frac{2}{13}\approx0.154$ compared to no peek, the net expected value per unit becomes negative after the $2$ fee. Therefore, we don't scale beyond what the no-bankruptcy constraint allows.
\end{example}

\section{ML and Programming Questions}
The package we are most interested in is pandas. It provides data structures and functions needed to manipulate structured data seamlessly. Recall some basic operations that one can perform with pandas:
\begin{itemize}
    \item Reading data: \texttt{pd.read\_csv('file.csv')}
    \item Basic operations:
          \begin{itemize}
              \item Select columns: \texttt{df['column\_name']} or \texttt{df.column\_name}
              \item Filter rows: \texttt{df[df.column > value]}
              \item Sort: \texttt{df.sort\_values('column')}
              \item Group by: \texttt{df.groupby('column').mean()}
          \end{itemize}
    \item Data manipulation:
          \begin{itemize}
              \item Add column: \texttt{df['new\_column'] = values}
              \item Drop missing values: \texttt{df.dropna()}
              \item Fill missing values: \texttt{df.fillna(value)}
              \item Merge dataframes: \texttt{pd.merge(df1, df2, on='key')}
          \end{itemize}
    \item Aggregations:
          \begin{itemize}
              \item Summary statistics: \texttt{df.describe()}
              \item Mean/median: \texttt{df.mean()}, \texttt{df.median()}
              \item Count unique values: \texttt{df['column'].value\_counts()}
          \end{itemize}
\end{itemize}

\subsection{Resume items}
Cornell Quant Club — Machine Learning Subteam

\begin{itemize}
    \item Deep learning trading algorithms: You designed and tuned models that make buy/sell decisions automatically. Implemented data pipelines and training logic in Python; optimized latency-critical components in C++ for execution speed.
    \item Neural networks for financial time series: Built RNNs (LSTMs, GRUs) and CNNs that learn temporal and local patterns in stock-price sequences.
    \item Frameworks: Used TensorFlow and PyTorch for model definition, training loops, and GPU acceleration.
    \item Latency optimization: Used C++ for order-book simulation and low-level numerical routines to minimize compute and communication delays in backtesting or live trading.
    \item Backtesting and evaluation: Ran simulations to compare strategy performance under different market scenarios.
\end{itemize}

\begin{example}[Processing financial data with pandas and computing RSI]
    \leavevmode
    \begin{lstlisting}[language=Python]
    import pandas as pd, numpy as np, yfinance as yf

    data = yf.download("AAPL", period="60d", interval="1m")
    data["rsi"] = compute_rsi(data["Close"], 14)
    data["target"] = np.sign(data["Close"].shift(-5) - data["Close"])  # 5-min horizon
    \end{lstlisting}
\end{example}

\begin{example}[RNN]
    For sequences $x_1, x_2, \ldots, x_T$, we often want to predict
    \[y_t = f(x_1, x_2, \ldots, x_t)\]
    where each new input depends on previous ones. For example, in language modeling, predicting the next word depends on all prior words, so the $x_i$ are word embeddings and $y_t$ is the next word prediction.

    A feedforward network can't easily handle varying sequence lengths or reuse past information. Recall that a feedforward network is just:
    \[y_t = W_2 \sigma(W_1 x_t + b_1) + b_2\]
    where $\sigma$ is a nonlinearity like ReLU (Rectified Linear Unit, meaning $\sigma(x) = \max(0, x)$). This is used because the negative part does not make too much sense (in the general context of neural networks). This processes each $x_t$ independently, losing context.

    A single neuron computes a weighted linear combination followed by a nonlinearity:
    \[
        y = \sigma(w^\top x + b),
    \]
    where \(x \in \mathbb{R}^d\), \(w \in \mathbb{R}^d\), and \(\sigma\) is a nonlinear activation such as ReLU or \(\tanh\).

    To approximate more complex functions, we can stack several neurons together in a \emph{layer}.
    Each neuron learns a different direction in input space, producing intermediate features:
    \[
        z_i = \operatorname{ReLU}(x^\top w_i), \qquad i=1,\dots,3,
    \]
    and the layer output is a linear combination of them:
    \[
        y = \sum_{i=1}^3 a_i\,\operatorname{ReLU}(x^\top w_i) + b.
    \]
    Each hidden neuron defines a half-space in \(\mathbb{R}^d\).
    The sum of several ReLU activations yields a piecewise-linear approximation to any target function. A single wide layer can approximate any continuous function (universal approximation theorem), but may require exponentially many neurons.
    Stacking layers—adding \emph{depth}—greatly reduces width requirements: deeper networks reuse intermediate features to represent complex functions more efficiently.

    An RNN introduces a hidden state $h_t$ that summarizes what has happened so far. At each time step:
    \[h_t = \tanh(W_x x_t + W_h h_{t-1} + b)\]
    \[y_t = W_y h_t + c\]

    Here $x_t$ is the input vector at time $t$, $h_t$ is the hidden state (memory of the past), $y_t$ is the output, $W_x$, $W_h$, $W_y$ are learnable weight matrices, and $\tanh$ is a nonlinearity ensuring stability.

    So $h_t$ is the "memory" passed forward; each new step updates that memory using the new input. To train the network, compute losses at each step (e.g. difference between predicted and true $y_t$), sum them, and backpropagate through all unrolled steps. This multiplies many Jacobians:
    \[\frac{\partial L}{\partial W_h} = \sum_t \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial h_{t-1}} \cdots\]
    The repeated multiplication can make gradients vanish (go to zero) or explode (blow up) — the core limitation of vanilla RNNs.
\end{example}

\begin{example}
    [Stochastic gradient descent]
    You have a model $h_\theta(x)$ with parameters $\theta$ (these are the weights).
    You have data $(x_i, y_i)$. You want to make $h_\theta(x_i)$ close to $y_i$.

    So you define a loss function
    \[L(\theta) = \frac{1}{n}\sum_{i=1}^n \ell(h_\theta(x_i), y_i)\]
    (e.g. squared error $\ell = (\hat y - y)^2$).

    The goal is to find
    \[\theta^\star = \arg\min_\theta L(\theta).\]

    The gradient $\nabla_\theta L(\theta)$ is a vector showing how the loss changes if you change each parameter slightly. If one coordinate of the gradient is positive, increasing that parameter increases the loss, so to reduce the loss, you move in the opposite direction.

    Gradient descent means repeatedly applying:
    \[\theta \leftarrow \theta - \eta\, \nabla_\theta L(\theta)\]
    where $\eta > 0$ is the learning rate (step size). The minus sign moves you "downhill." This is a discrete version of moving along the negative gradient of the loss surface.

    If $L(\theta)$ is smooth, then moving a small distance opposite the gradient direction always decreases $L$ (to first order):
    \[L(\theta - \eta\nabla L) \approx L(\theta) - \eta \|\nabla L\|^2 < L(\theta).\]
    So each update reduces the loss until you reach a local minimum.

    Computing $\nabla L$ over all data can be expensive. So you estimate it from a random subset (a mini-batch):
    \[g = \frac{1}{B}\sum_{(x,y)\in \mathcal{B}} \nabla_\theta \ell(h_\theta(x), y)\]
    and update
    \[\theta \leftarrow \theta - \eta g.\]
    The gradient estimate is noisy, but in expectation points the same way. Noise helps escape bad local minima.
\end{example}

\begin{example}
    [C++ optimizations for low-latency trading]
    \begin{itemize}
        \item C++ lets you decide when and where memory is allocated.
        \item Function calls require saving registers, jumping, returning.
              If the function body is small, that overhead dominates.
              Marking it inline lets the compiler paste it directly into the caller, removing the jump.
        \item constexpr tells the compiler to compute something at compile time rather than runtime.
    \end{itemize}
    C++ executes anything that must be fast, predictable, or close to hardware. For example, Feature Extraction for Streaming Market Data:
    \begin{lstlisting}[language=C++]
struct Tick { double bid, ask; int bid_sz, ask_sz; };
struct State { double mid, spread, imb; };

inline void update_features(const Tick& t, State& s) {
    s.mid    = 0.5 * (t.bid + t.ask);
    s.spread = t.ask - t.bid;
    s.imb    = double(t.bid_sz) / (t.bid_sz + t.ask_sz + 1e-9);
}
    \end{lstlisting}
Python can then import it via pybind11 for high-level logic. Another example couple be to compute signal kernels, such as order book imbalance over time:
    \begin{lstlisting}[language=C++]
    inline double imbalance(double bid_sz, double ask_sz) {
    return (bid_sz - ask_sz) / (bid_sz + ask_sz + 1e-9);
}
    \end{lstlisting}
Other examples of signals could be mid-price changes, spread changes, volume-weighted average price (VWAP), etc.
\end{example}

\begin{example}
    [Interpositioning] Function interpositioning is a Unix/Linux technique that lets you override or wrap existing library functions (like malloc, free, open, read, etc.) without changing the program's source code. Widely used for profiling, debugging, or performance instrumentation.

    Interpositioning allows you to insert measurement or debugging code transparently.

    For example, you can:
    \begin{itemize}
        \item Measure total bytes allocated.
        \item Time how long each call takes.
        \item Detect memory leaks (by logging unfreed pointers).
        \item Replace slow system calls with mocks during testing.
    \end{itemize}

    This works for any dynamically linked program — even proprietary binaries you can't recompile.
\end{example}

\end{document}